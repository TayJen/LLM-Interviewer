[Document(metadata={'source': '/Users/eugenekillevsky/PycharmProjects/LLM-Interviewer/data/RAG/ml-handbook-master/chapters/model_evaluation/intro.md'}, page_content='title: Оценка качества моделей author: maria_burkina, alexey_gorchakov, pavel_gubko\n\nЭтот список будет заменен оглавлением, за вычетом заголовка "Contents", к которому добавлен класс no_toc. {:toc}\n\n<<Гораздо легче что-то измерить, чем понять, что именно вы измеряете>> – Джон Уильям Салливан\n\nЗадачи машинного обучения с учителем как правило состоят в восстановлении зависимости между парами (признаковое описание, целевая переменная) по данным, доступным нам для анализа. Алгоритмы машинного обучения (learning algorithm), со многими из которых вы уже успели познакомиться, позволяют построить модель, аппроксимирующую эту зависимость. Но как понять, насколько качественной получилась аппроксимация?\n\nПочти наверняка наша модель будет ошибаться на некоторых объектах: будь она даже идеальной, шум или выбросы в тестовых данных всё испортят. При этом разные модели будут ошибаться на разных объектах и в разной степени. Задача специалиста по машинному обучению – подобрать подходящий критерий, который позволит сравнивать различные модели.\n\nПеред чтением этой главы мы хотели бы ещё раз напомнить, что качество модели нельзя оценивать на обучающей выборке. Как минимум, это стоит делать на отложенной (тестовой) выборке, но, если вам это позволяют время и вычислительные ресурсы, стоит прибегнуть и к более надёжным способам проверки – например, кросс-валидации (о ней вы узнаете в отдельной главе).\n\nВыбор метрик в реальных задачах\n\nВозможно, вы уже участвовали в соревнованиях по анализу данных. На таких соревнованиях метрику (критерий качества модели) организатор выбирает за вас, и она, как правило, довольно понятным образом связана с результатами предсказаний. Но на практике всё бывает намного сложнее.\n\nНапример, мы хотим: - решить, сколько коробок с бананами нужно завтра привезти в конкретный магазин, чтобы минимизировать количество товара, который не будет выкуплен и минимизировать ситуацию, когда покупатель к концу дня не находит желаемый продукт на полке; - увеличить счастье пользователя от работы с нашим сервисом, чтобы он стал лояльным и обеспечивал тем самым стабильный прогнозируемый доход; - решить, нужно ли направить человека на дополнительное обследование.\n\nВ каждом конкретном случае может возникать целая иерархия метрик. Представим, например, что речь идёт о стриминговом музыкальном сервисе, пользователей которого мы решили порадовать сгенерированными самодельной нейросетью треками – не защищёнными авторским правом, а потому совершенно бесплатными. Иерархия метрик могла бы иметь такой вид: 1. Самый верхний уровень: будущий доход сервиса – невозможно измерить в моменте, сложным образом зависит от совокупности всех наших усилий; 2. Медианная длина сессии, возможно, служащая оценкой радости пользователей, которая, как мы надеемся, повлияет на их желание продолжать платить за подписку – её нам придётся измерять в продакшене, ведь нас интересует реакция настоящих пользователей на новшество; 3. Доля удовлетворённых качеством сгенерированной музыки асессоров, на которых мы потестируем её до того, как выставить на суд пользователей; 4. Функция потерь, на которую мы будем обучать генеративную сеть.\n\nНа этом примере мы можем заметить сразу несколько общих закономерностей. Во-первых, метрики бывают offline и online (оффлайновыми и онлайновыми). Online метрики вычисляются по данным, собираемым с работающей системы (например, медианная длина сессии). Offline метрики могут быть измерены до введения модели в эксплуатацию, например, по историческим данным или с привлечением специальных людей, асессоров. Последнее часто применяется, когда метрикой является реакция живого человека: скажем, так поступают поисковые компании, которые предлагают людям оценить качество ранжирования экспериментальной системы еще до того, как рядовые пользователи увидят эти результаты в обычном порядке. На самом же нижнем этаже иерархии лежат оптимизируемые в ходе обучения функции потерь.\n\nВ данном разделе нас будут интересовать offline метрики, которые могут быть измерены без привлечения людей.\n\nФункция потерь $\\neq$ метрика качества\n\nКак мы узнали ранее, методы обучения реализуют разные подходы к обучению: - обучение на основе прироста информации (как в деревьях решений) - обучение на основе сходства (как в методах ближайших соседей) - обучение на основе вероятностной модели данных (например, максимизацией правдоподобия) - обучение на основе ошибок (минимизация эмпирического риска)\n\nИ в рамках обучения на основе минимизации ошибок мы уже отвечали на вопрос: как можно штрафовать модель за предсказание на обучающем объекте.\n\nВо время сведения задачи о построении решающего правила к задаче численной оптимизации, мы вводили понятие функции потерь и, обычно, объявляли целевой функцией сумму потерь от предсказаний на всех объектах обучающей выборке.\n\nВажно понимать разницу между функцией потерь и метрикой качества. Её можно сформулировать следующим образом:\n\nФункция потерь возникает в тот момент, когда мы сводим задачу построения модели к задаче оптимизации. Обычно требуется, чтобы она обладала хорошими свойствами (например, дифференцируемостью).\n\nМетрика – внешний, объективный критерий качества, обычно зависящий не от параметров модели, а только от предсказанных меток.\n\nВ некоторых случаях метрика может совпадать с функцией потерь. Например, в задаче регрессии MSE играет роль как функции потерь, так и метрики. Но, скажем, в задаче бинарной классификации они почти всегда различаются: в качестве функции потерь может выступать кросс-энтропия, а в качестве метрики – число верно угаданных меток (accuracy). Отметим, что в последнем примере у них различные аргументы: на вход кросс-энтропии нужно подавать логиты, а на вход accuracy – предсказанные метки (то есть по сути argmax логитов).\n\nБинарная классификация: метки классов\n\nПерейдём к обзору метрик и начнём с самой простой разновидности классификации – бинарной, а затем постепенно будем наращивать сложность.\n\nНапомним постановку задачи бинарной классификации: нам нужно по обучающей выборке $\\{(x_i, y_i)\\}_{i=1}^N$, где $y_i\\in\\{0, 1\\}$ построить модель, которая по объекту $x$ предсказывает метку класса $f(x)\\in\\{0, 1\\}$.\n\nПервым критерием качества, который приходит в голову, является accuracy – доля объектов, для которых мы правильно предсказали класс:\n\n$$ \\color{#348FEA}{\\text{Accuracy}(y, y^{pred}) = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}[y_i = f(x_i)]} $$\n\nИли же сопряженная ей метрика – доля ошибочных классификаций (error rate):\n\n$$\\text{Error rate} = 1 - \\text{Accuracy}$$\n\nПознакомившись чуть внимательнее с этой метрикой, можно заметить, что у неё есть несколько недостатков: - она не учитывает дисбаланс классов. Например, в задаче диагностики редких заболеваний классификатор, предсказывающий всем пациентам отсутствие болезни будет иметь достаточно высокую accuracy просто потому, что больных людей в выборке намного меньше; - она также не учитывает цену ошибки на объектах разных классов. Для примера снова можно привести задачу медицинской диагностики: если ошибочный положительный диагноз для здорового больного обернётся лишь ещё одним обследованием, то ошибочно отрицательный вердикт может повлечь роковые последствия.\n\nConfusion matrix (матрица ошибок)\n\nИсторически задача бинарной классификации – это задача об обнаружении чего-то редкого в большом потоке объектов, например, поиск человека, больного туберкулёзом, по флюорографии. Или задача признания пятна на экране приёмника радиолакационной станции бомбардировщиком, представляющем угрозу охраняемому объекту (в противовес стае гусей).\n\nПоэтому класс, который представляет для нас интерес, называется «положительным», а оставшийся – «отрицательным».\n\nЗаметим, что для каждого объекта в выборке возможно 4 ситуации: - мы предсказали положительную метку и угадали. Будет относить такие объекты к true positive (TP) группе (true – потому что предсказали мы правильно, а positive – потому что предсказали положительную метку); - мы предсказали положительную метку, но ошиблись в своём предсказании – false positive (FP) (false, потому что предсказание было неправильным); - мы предсказали отрицательную метку и угадали – true negative (TN); - и наконец, мы предсказали отрицательную метку, но ошиблись – false negative (FN). Для удобства все эти 4 числа изображают в виде таблицы, которую называют confusion matrix (матрицей ошибок):\n\n{: center}\n\nНе волнуйтесь, если первое время эти обозначения будут сводить вас с ума (будем откровенны, даже профи со стажем в них порой путаются), однако логика за ними достаточно простая: первая часть названия группы показывает угадали ли мы с классом, а вторая – какой класс мы предсказали.\n\n{: center}\n\nПример\n\nПопробуем воспользоваться введёнными метриками в боевом примере: сравним работу нескольких моделей классификации на Breast cancer wisconsin (diagnostic) dataset.\n\nОбъектами выборки являются фотографии биопсии грудных опухолей. С их помощью было сформировано признаковое описание, которое заключается в характеристиках ядер клеток (таких как радиус ядра, его текстура, симметричность). Положительным классом в такой постановке будут злокачественные опухоли, а отрицательным – доброкачественные.\n\nМодель 1. Константное предсказание.\n\nРешение задачи начнём с самого простого классификатора, который выдаёт на каждом объекте константное предсказание – самый часто встречающийся класс. {% include details.html summary="Зачем вообще замерять качество на такой модели?" details="При разработке модели машинного обучения для проекта всегда желательно иметь некоторую baseline модель. Так нам будет легче проконтролировать, что наша более сложная модель действительно дает нам прирост качества." %}\n\n```python from sklearn.datasets import load_breast_cancer the_data = load_breast_cancer()\n\n0 – "доброкачественный"\n\n1 – "злокачественный"\n\nrelabeled_target = 1 - the_data["target"]\n\nfrom sklearn.model_selection import train_test_split X = the_data["data"] y = relabeled_target X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nfrom sklearn.dummy import DummyClassifier dc_mf = DummyClassifier(strategy="most_frequent") dc_mf.fit(X_train, y_train)\n\nfrom sklearn.metrics import confusion_matrix y_true = y_test y_pred = dc_mf.predict(X_test) dc_mf_tn, dc_mf_fp, dc_mf_fn, dc_mf_tp = confusion_matrix(y_true, y_pred, labels = [0, 1]).ravel() ```\n\nПрогнозируемый    класс + Прогнозируемый класс - Истинный класс + TP = 0 FN = 53 Истинный класс - FP = 0 TN = 90\n\nОбучающие данные таковы, что наш dummy-классификатор все объекты записывает в отрицательный класс, то есть признаёт все опухоли доброкачественными. Такой наивный подход позволяет нам получить минимальный штраф за FP (действительно, нельзя ошибиться в предсказании, если положительный класс вообще не предсказывается), но и максимальный штраф за FN (в эту группу попадут все злокачественные опухоли).\n\nМодель 2. Случайный лес.\n\nНастало время воспользоваться всем арсеналом моделей машинного обучения, и начнём мы со случайного леса.\n\npython from sklearn.ensemble import RandomForestClassifier rfc = RandomForestClassifier() rfc.fit(X_train, y_train) y_true = y_test y_pred = rfc.predict(X_test) rfc_tn, rfc_fp, rfc_fn, rfc_tp = confusion_matrix(y_true, y_pred, labels = [0, 1]).ravel()\n\nПрогнозируемый класс + Прогнозируемый класс - Истинный класс + TP = 52 FN = 1 Истинный класс - FP = 4 TN = 86\n\nМожно сказать, что этот классификатор чему-то научился, т.к. главная диагональ матрицы стала содержать все объекты из отложенной выборки, за исключением 4 + 1 = 5 объектов (сравните с 0 + 53 объектами dummy-классификатора, все опухоли объявляющего доброкачественными).\n\nОтметим, что вычисляя долю недиагональных элементов, мы приходим к метрике error rate, о которой мы говорили в самом начале:\n\n$$\\text{Error rate} = \\frac{FP + FN}{ TP + TN + FP + FN}$$\n\nтогда как доля объектов, попавших на главную диагональ – это как раз таки accuracy:\n\n$$\\text{Accuracy} = \\frac{TP + TN}{ TP + TN + FP + FN}$$\n\nМодель 3. Метод опорных векторов.\n\nДавайте построим еще один классификатор на основе линейного метода опорных векторов.\n\nНе забудьте привести признаки к единому масштабу, иначе численный алгоритм не сойдется к решению и мы получим гораздо более плохо работающее решающее правило. Попробуйте проделать это упражнение.\n\npython from sklearn.svm import LinearSVC from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(X_train) scaled_linsvc = LinearSVC(C=0.01,random_state=42) scaled_linsvc.fit(ss.transform(X_train), y_train) y_true = y_test y_pred = scaled_linsvc.predict(ss.transform(X_test)) tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels = [0, 1]).ravel()\n\nПрогнозируемый класс + Прогнозируемый класс - Истинный класс + TP = 50 FN = 3 Истинный класс - FP = 1 TN = 89\n\nСравним результаты\n\nЛегко заметить, что каждая из двух моделей лучше классификатора-пустышки, однако давайте попробуем сравнить их между собой. С точки зрения error rate модели практически одинаковы: 5/143 для леса против 4/143 для SVM.\n\nПосмотрим на структуру ошибок чуть более внимательно: лес – (FP = 4, FN = 1), SVM – (FP = 1, FN = 3). Какая из моделей предпочтительнее?\n\nЗамечание: Мы сравниваем несколько классификаторов на основании их предсказаний на отложенной выборке. Насколько ошибки данных классификаторов зависят от разбиения исходного набора данных? Иногда в процессе оценки качества мы будем получать модели, чьи показатели эффективности будут статистически неразличимыми.\n\nПусть мы учли предыдущее замечание и эти модели действительно статистически значимо ошибаются в разную сторону. Мы встретились с очевидной вещью: на матрицах нет отношения порядка. Когда мы сравнивали dummy-классификатор и случайный лес с помощью Accuracy, мы всю сложную структуру ошибок свели к одному числу, т.к. на вещественных числах отношение порядка есть. Сводить оценку модели к одному числу очень удобно, однако не стоит забывать, что у вашей модели есть много аспектов качества.\n\nЧто же всё-таки важнее уменьшить: FP или FN? Вернёмся к задаче: FP – доля доброкачественных опухолей, которым ошибочно присваивается метка злокачественной, а FN – доля злокачественных опухолей, которые классификатор пропускает. В такой постановке становится понятно, что при сравнении выиграет модель с меньшим FN (то есть лес в нашем примере), ведь каждая не обнаруженная опухоль может стоить человеческой жизни.\n\nРассмотрим теперь другую задачу: по данным о погоде предсказать, будет ли успешным запуск спутника. FN в такой постановке – это ошибочное предсказание неуспеха, то есть не более, чем упущенный шанс (если вас, конечно не уволят за срыв сроков). С FP всё серьёзней: если вы предскажете удачный запуск спутника, а на деле он потерпит крушение из-за погодных условий, то ваши потери будут в разы существеннее.\n\nИтак, из примеров мы видим, что в текущем виде введенная нами доля ошибочных классификаций не даст нам возможности учесть неравную важность FP и FN. Поэтому введем две новые метрики: точность и полноту.\n\nТочность и полнота\n\nAccuracy - это метрика, которая характеризует качество модели, агрегированное по всем классам. Это полезно, когда классы для нас имеют одинаковое значение. В случае, если это не так, accuracy может быть обманчивой.\n\nРассмотрим ситуацию, когда положительный класс это событие редкое. Возьмем в качестве примера поисковую систему - в нашем хранилище хранятся миллиарды документов, а релевантных к конкретному поисковому запросу на несколько порядков меньше.\n\nПусть мы хотим решить задачу бинарной классификации «документ d релевантен по запросу q». Благодаря большому дисбалансу, Accuracy dummy-классификатора, объявляющего все документы нерелевантными, будет близка к единице. Напомним, что $\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$, и в нашем случае высокое значение метрики будет обеспечено членом TN, в то время для пользователей более важен высокий TP.\n\nПоэтому в случае ассиметрии классов, можно использовать метрики, которые не учитывают TN и ориентируются на TP.\n\nЕсли мы рассмотрим долю правильно предсказанных положительных объектов среди всех объектов, предсказанных положительным классом, то мы получим метрику, которая называется точностью (precision)\n\n$$\\color{#348FEA}{\\text{Precision} = \\frac{TP}{TP + FP}}$$\n\nИнтуитивно метрика показывает долю релевантных документов среди всех найденных классификатором. Чем меньше ложноположительных срабатываний будет допускать модель, тем больше будет её Precision.\n\nЕсли же мы рассмотрим долю правильно найденных положительных объектов среди всех объектов положительного класса, то мы получим метрику, которая называется полнотой (recall)\n\n$$\\color{#348FEA}{\\text{Recall} = \\frac{TP}{TP + FN}}$$\n\nИнтуитивно метрика показывает долю найденных документов из всех релевантных. Чем меньше ложно отрицательных срабатываний, тем выше recall модели.\n\nНапример, в задаче предсказания злокачественности опухоли точность показывает, сколько из определённых нами как злокачественные опухолей действительно являются злокачественными, а полнота – какую долю злокачественных опухолей нам удалось выявить.\n\nХорошее понимание происходящего даёт следующая картинка: {: .center} (источник картинки)\n\nRecall@k, Precision@k\n\nМетрики Recall и Precision хорошо подходят для задачи поиска «документ d релевантен запросу q», когда из списка рекомендованных алгоритмом документов нас интересует только первый. Но не всегда алгоритм машинного обучения вынужден работать в таких жестких условиях. Может быть такое, что вполне достаточно, что релевантный документ попал в первые k рекомендованных. Например, в интерфейсе выдачи первые три подсказки видны всегда одновременно и вообще не очень понятно, какой у них порядок. Тогда более честной оценкой качества алгоритма будет «в выдаче D размера k по запросу q нашлись релевантные документы». Для расчёта метрики по всей выборке объединим все выдачи и рассчитаем precision, recall как обычно подокументно.\n\nF1-мера\n\nКак мы уже отмечали ранее, модели очень удобно сравнивать, когда их качество выражено одним числом. В случае пары Precision-Recall существует популярный способ скомпоновать их в одну метрику - взять их среднее гармоническое. Данный показатель эффективности исторически носит название F1-меры (F1-measure).\n\n$$ \\color{#348FEA}{F_1 = \\frac{2}{\\frac{1}{Recall} + \\frac{1}{Precision}}} = $$\n\n$$ = 2 \\frac{Recall \\cdot Precision }{Recall + Precision} = \\frac {TP} {TP + \\frac{FP + FN}{2}} $$\n\nСтоит иметь в виду, что F1-мера предполагает одинаковую важность Precision и Recall, если одна из этих метрик для вас приоритетнее, то можно воспользоваться $F_{\\beta}$ мерой:\n\n$$ F_{\\beta} = (\\beta^2 + 1) \\frac{Recall \\cdot Precision }{Recall + \\beta^2Precision} $$\n\nБинарная классификация: вероятности классов\n\nМногие модели бинарной классификации устроены так, что класс объекта получается бинаризацией выхода классификатора по некоторому фиксированному порогу:\n\n$$f\\left(x ; w, w_{0}\\right)=\\mathbb{I}\\left[g(x, w) > w_{0}\\right].$$\n\nНапример, модель логистической регрессии возвращает оценку вероятности принадлежности примера к положительному классу. Другие модели бинарной классификации обычно возвращают произвольные вещественные значения, но существуют техники, называемые калибровкой классификатора, которые позволяют преобразовать предсказания в более или менее корректную оценку вероятности принадлежности к положительному классу.\n\nКак оценить качество предсказываемых вероятностей, если именно они являются нашей конечной целью? Общепринятой мерой является логистическая функция потерь, которую мы изучали раньше, когда говорили об устройстве некоторых методов классификации (например уже упоминавшейся логистической регрессии).\n\nЕсли же нашей целью является построение прогноза в терминах метки класса, то нам нужно учесть, что в зависимости от порога мы будем получать разные предсказания и разное качество на отложенной выборке. Так, чем ниже порог отсечения, тем больше объектов модель будет относить к положительному классу. Как в этом случае оценить качество модели?\n\nAUC\n\nПусть мы хотим учитывать ошибки на объектах обоих классов. При уменьшении порога отсечения мы будем находить (правильно предсказывать) всё большее число положительных объектов, но также и неправильно предсказывать положительную метку на всё большем числе отрицательных объектов. Естественным кажется ввести две метрики TPR и FPR:\n\nTPR (true positive rate) – это полнота, доля положительных объектов, правильно предсказанных положительными:\n\n$$ TPR = \\frac{TP}{P} = \\frac{TP}{TP + FN} $$\n\nFPR (false positive rate) – это доля отрицательных объектов, неправильно предсказанных положительными:\n\n$$FPR = \\frac{FP}{N} = \\frac{FP}{FP + TN}$$\n\nОбе эти величины растут при уменьшении порога. Кривая в осях TPR/FPR, которая получается при варьировании порога, исторически называется ROC-кривой (receiver operating characteristics curve, сокращённо ROC curve). Следующий график поможет вам понять поведение ROC-кривой:\n\n{% include_relative roc_auc/roc_auc.html %}\n\nЖелтая и синяя кривые показывают распределение предсказаний классификатора на объектах положительного и отрицательного классов соответственно. То есть значения на оси X (на графике с двумя гауссианами) мы получаем из классификатора. Если классификатор идеальный (две кривые разделимы по оси X), то на правом графике мы получаем ROC-кривую (0,0)->(0,1)->(1,1) (убедитесь сами!), площадь под которой равна 1. Если классификатор случайный (предсказывает одинаковые метки положительным и отрицательным объектам), то мы получаем ROC-кривую (0,0)->(1,1), площадь под которой равна 0.5. Поэкспериментируйте с разными вариантами распределения предсказаний по классам и посмотрите, как меняется ROC-кривая.\n\nЧем лучше классификатор разделяет два класса, тем больше площадь (area under curve) под ROC-кривой – и мы можем использовать её в качестве метрики. Эта метрика называется AUC и она работает благодаря следующему свойству ROC-кривой:\n\nAUC равен доле пар объектов вида (объект класса 1, объект класса 0), которые алгоритм верно упорядочил, т.е. предсказание классификатора на первом объекте больше:\n\n$$ \\color{#348FEA}{\\operatorname{AUC} = \\frac{\\sum\\limits_{i = 1}^{N} \\sum\\limits_{j = 1}^{N}\\mathbb{I}[y_i < y_j] I^{\\prime}[f(x_{i}) < f(x_{j})]}{\\sum\\limits_{i = 1}^{N} \\sum\\limits_{j = 1}^{N}\\mathbb{I}[y_i < y_j]}} $$\n\n$$ I^{\\prime}\\left[f(x_{i}) < f(x_{j})\\right]= \\left{ \\begin{array}{ll} 0, & f(x_{i}) > f(x_{j}) \\ 0.5 & f(x_{i}) = f(x_{j}) \\ 1, & f(x_{i}) < f(x_{j}) \\end{array} \\right. $$\n\n$$ I\\left[y_{i}< y_{j}\\right]= \\left{ \\begin{array}{ll} 0, & y_{i} \\geq y_{j} \\ 1, & y_{i} < y_{j} \\end{array} \\right. $$\n\nЧтобы детальнее разобраться, почему это так, советуем вам обратиться к материалам А.Г.Дьяконова.\n\nВ каких случаях лучше отдать предпочтение этой метрике? Рассмотрим следующую задачу: некоторый сотовый оператор хочет научиться предсказывать, будет ли клиент пользоваться его услугами через месяц. На первый взгляд кажется, что задача сводится к бинарной классификации с метками 1, если клиент останется с компанией и $0$ – иначе.\n\nОднако если копнуть глубже в процессы компании, то окажется, что такие метки практически бесполезны. Компании скорее интересно упорядочить клиентов по вероятности прекращения обслуживания и в зависимости от этого применять разные варианты удержания: кому-то прислать скидочный купон от партнёра, кому-то предложить скидку на следующий месяц, а кому-то и новый тариф на особых условиях.\n\nТаким образом, в любой задаче, где нам важна не метка сама по себе, а правильный порядок на объектах, имеет смысл применять AUC.\n\n{% include details.html summary="Утверждение выше может вызывать у вас желание использовать AUC в качестве метрики в задачах ранжирования, но мы призываем вас быть аккуратными." details="Продемонстрируем это на следующем примере: пусть наша выборка состоит из $9100$ объектов класса $0$ и $10$ объектов класса $1$, и модель расположила их следующим образом:\n\n$$\\underbrace{0 \\dots 0}{9000} ~ \\underbrace{1 \\dots 1}{10} ~ \\underbrace{0 \\dots 0}_{100}$$\n\nТогда AUC будет близка к единице: количество пар правильно расположенных объектов будет порядка $90000$, в то время как общее количество пар порядка $91000$.\n\nОднако самыми высокими по вероятности положительного класса будут совсем не те объекты, которые мы ожидаем." %}\n\nAverage Precision\n\nБудем постепенно уменьшать порог бинаризации. При этом полнота будет расти от $0$ до $1$, так как будет увеличиваться количество объектов, которым мы приписываем положительный класс (а количество объектов, на самом деле относящихся к положительному классу, очевидно, меняться не будет). Про точность же нельзя сказать ничего определённого, но мы понимаем, что скорее всего она будет выше при более высоком пороге отсечения (мы оставим только объекты, в которых модель <<уверена>> больше всего). Варьируя порог и пересчитывая значения Precision и Recall на каждом пороге, мы получим некоторую кривую примерно следующего вида:\n\n{:.center} (источник картинки)\n\nРассмотрим среднее значение точности (оно равно площади под кривой точность-полнота):\n\n$$ \\text { AP }=\\int_{0}^{1} p(r) d r$$\n\nПолучим показатель эффективности, который называется average precision. Как в случае матрицы ошибок мы переходили к скалярным показателям эффективности, так и в случае с кривой точность-полнота мы охарактеризовали ее в виде числа.\n\nМногоклассовая классификация\n\nЕсли классов становится больше двух, расчёт метрик усложняется. Если задача классификации на $K$ классов ставится как $K$ задач об отделении класса $i$ от остальных ($i=1,\\ldots,K$), то для каждой из них можно посчитать свою матрицу ошибок. Затем есть два варианта получения итогового значения метрики из $K$ матриц ошибок: 1. Усредняем элементы матрицы ошибок (TP, FP, TN, FN) между бинарными классификаторами, например $TP = \\frac{1}{K}\\sum_{i=1}^{K}TP_i$. Затем по одной усреднённой матрице ошибок считаем Precision, Recall, F-меру. Это называют микроусреднением. 2. Считаем Precision, Recall для каждого классификатора отдельно, а потом усредняем. Это называют макроусреднением.\n\nПорядок усреднения влияет на результат в случае дисбаланса классов. Показатели TP, FP, FN — это счётчики объектов. Пусть некоторый класс обладает маленькой мощностью (обозначим её $M$). Тогда значения TP и FN при классификации этого класса против остальных будут не больше $M$, то есть тоже маленькие. Про FP мы ничего уверенно сказать не можем, но скорее всего при дисбалансе классов классификатор не будет предсказывать редкий класс слишком часто, потому что есть большая вероятность ошибиться. Так что FP тоже мало. Поэтому усреднение первым способом сделает вклад маленького класса в общую метрику незаметным. А при усреднении вторым способом среднее считается уже для нормированных величин, так что вклад каждого класса будет одинаковым.\n\nРассмотрим пример. Пусть есть датасет из объектов трёх цветов: желтого, зелёного и синего. Желтого и зелёного цветов почти поровну — 21 и 20 объектов соответственно, а синих объектов всего 4. {:.center } Модель по очереди для каждого цвета пытается отделить объекты этого цвета от объектов оставшихся двух цветов. Результаты классификации проиллюстрированы матрицей ошибок. Модель «покрасила» в жёлтый 25 объектов, 20 из которых были действительно жёлтыми (левый столбец матрицы). В синий был «покрашен» только один объект, который на самом деле жёлтый (средний столбец матрицы). В зелёный — 19 объектов, все на самом деле зелёные (правый столбец матрицы). {:.center} Посчитаем Precision классификации двумя способами: 1. С помощью микроусреднения получаем $$ \\text{Precision} = \\frac{\\dfrac{1}{3}\\left(20 + 0 + 19\\right)}{\\dfrac{1}{3}\\left(20 + 0 + 19\\right) + \\dfrac{1}{3}\\left(5 + 1 + 0\\right)} = 0.87 $$ 2. С помощью макроусреднения получаем $$ \\text{Precision} = \\dfrac{1}{3}\\left( \\frac{20}{20 + 5} + \\frac{0}{0 + 1} + \\frac{19}{19 + 0}\\right) = 0.6 $$\n\nВидим, что макроусреднение лучше отражает тот факт, что синий цвет, которого в датасете было совсем мало, модель практически игнорирует.\n\nКак оптимизировать метрики классификации?\n\nПусть мы выбрали, что метрика качества алгоритма будет $F(a(X), Y)$. Тогда мы хотим обучить модель так, чтобы $F$ на валидационной выборке была минимальная/максимальная. Лучший способ добиться минимизации метрики $F$ — оптимизировать её напрямую, то есть выбрать в качестве функции потерь ту же $F(a(X), Y)$. К сожалению, это не всегда возможно. Рассмотрим, как оптимизировать метрики иначе.\n\nМетрики precision и recall невозможно оптимизировать напрямую, потому что эти метрики нельзя рассчитать на одном объекте, а затем усреднить. Они зависят от того, какими были правильная метка класса и ответ алгоритма на всех объектах. Чтобы понять, как оптимизировать precision, recall, рассмотрим, как расчитать эти метрики на отложенной выборке. Пусть модель обучена на стандартную для классификации функцию потерь (LogLoss). Для получения меток класса специалист по машинному обучению сначала применяет на объектах модель и получает вещественные предсказания модели ($$p_i \\in \\left(0, 1\\right)$$). Затем предсказания бинаризуются по порогу, выбранному специалистом: если предсказание на объекте больше порога, то метка класса 1 (или «положительная»), если меньше — 0 (или «отрицательная»). Рассмотрим, что будет с метриками precision, recall в крайних положениях порога. 1. Пусть порог равен нулю. Тогда всем объектам будет присвоена положительная метка. Следовательно, все объекты будут либо TP, либо FP, потому что отрицательных предсказаний нет, $$TP + FP = N$$, где $$N$$ — размер выборки. Также все объекты, у которых метка на самом деле 1, попадут в TP. По формуле точность $$\\text{Precision} = \\frac{TP}{TP + FP} = \\frac1N \\sum_{i = 1}^N \\mathbb{I} \\left[ y_i = 1 \\right]$$ равна среднему таргету в выборке. А полнота $$\\text{Recall} = \\frac{TP}{TP + FN} = \\frac{TP}{TP + 0} = 1$$ равна единице. 2. Пусть теперь порог равен единице. Тогда ни один объект не будет назван положительным, $$TP = FP = 0$$. Все объекты с меткой класса 1 попадут в FN. Если есть хотя бы один такой объект, то есть $$FN \\ne 0$$, будет верна формула $$\\text{Recall} = \\frac{TP}{TP + FN} = \\frac{0}{0+ FN} = 0$$. То есть при пороге единица, полнота равна нулю. Теперь посмотрим на точность. Формула для Precision состоит только из счётчиков положительных ответов модели (TP, FP). При единичном пороге они оба равны нулю, $$\\text{Precision} = \\frac{TP}{TP + FP} = \\frac{0}{0 + 0}$$то есть при единичном пороге точность неопределена. Пусть мы отступили чуть-чуть назад по порогу, чтобы хотя бы несколько объектов были названы моделью положительными. Скорее всего это будут самые «простые» объекты, которые модель распознает хорошо, потому что её предсказание близко к единице. В этом предположении $$FP \\approx 0$$. Тогда точность $$\\text{Precision} = \\frac{TP}{TP + FP} \\approx \\frac{TP}{TP + 0} \\approx 1$$ будет близка к единице.\n\nИзменяя порог, между крайними положениями, получим графики Precision и Recall, которые выглядят как-то так:\n\n{:.center }\n\nRecall меняется от единицы до нуля, а Precision от среднего тагрета до какого-то другого значения (нет гарантий, что график монотонный).\n\nИтого оптимизация precision и recall происходит так: 1. Модель обучается на стандартную функцию потерь (например, LogLoss). 2. Используя вещественные предсказания на валидационной выборке, перебирая разные пороги от 0 до 1, получаем графики метрик в зависимости от порога. 3. Выбираем нужное сочетание точности и полноты.\n\nПусть теперь мы хотим максимизировать метрику AUC. Стандартный метод оптимизации, градиентный спуск, предполагает, что функция потерь дифференцируема. AUC этим качеством не обладает, то есть мы не можем оптимизировать её напрямую. Поэтому для метрики AUC приходится изменять оптимизационную задачу. Метрика AUC считает долю верно упорядоченных пар. Значит от исходной выборки можно перейти к выборке упорядоченных пар объектов. На этой выборке ставится задача классификации: метка класса 1 соответствует правильно упорядоченной паре, 0 — неправильно. Новой метрикой становится accuracy — доля правильно классифицированных объектов, то есть доля правильно упорядоченных пар. Оптимизировать accuracy можно по той же схеме, что и precision, recall: обучаем модель на LogLoss и предсказываем вероятности положительной метки у объекта выборки, считаем accuracy для разных порогов по вероятности и выбираем понравившийся.\n\nРегрессия\n\nВ задачах регрессии целевая метка у нас имеет потенциально бесконечное число значений. И природа этих значений, обычно, связана с каким-то процессом измерений: - величина температуры в определенный момент времени на метеостанции - количество прочтений статьи на сайте - количество проданных бананов в конкретном магазине, сети магазинов или стране - дебит добывающей скважины на нефтегазовом месторождении за месяц и т.п.\n\nМы видим, что иногда метка это целое число, а иногда произвольное вещественное число. Обычно случаи целочисленных меток моделируют так, словно это просто обычное вещественное число. При таком подходе может оказаться так, что модель A лучше модели B по некоторой метрике, но при этом предсказания у модели A могут быть не целыми. Если в бизнес-задаче ожидается именно целочисленный ответ, то и оценивать нужно огрубление.\n\nОбщая рекомендация такова: оценивайте весь каскад решающих правил: и те <<внутренние>>, которые вы получаете в результате обучения, и те <<итоговые>>, которые вы отдаёте бизнес-заказчику.\n\nНапример, вы можете быть удовлетворены, что стали ошибаться не во втором, а только в третьем знаке после запятой при предсказании погоды. Но сами погодные данные измеряются с точностью до десятых долей градуса, а пользователь и вовсе может интересоваться лишь целым числом градусов.\n\nИтак, напомним постановку задачи регрессии: нам нужно по обучающей выборке ${(x_i, y_i)}_{i=1}^N$, где $y_i \\in \\mathbb{R}$ построить модель f(x).\n\nВеличину $$ e_i = f(x_i) - y_i $$ называют ошибкой на объекте i или регрессионным остатком.\n\nВесь набор ошибок на отложенной выборке может служить аналогом матрицы ошибок из задачи классификации. А именно, когда мы рассматриваем две разные модели, то, глядя на то, как и на каких объектах они ошиблись, мы можем прийти к выводу, что для решения бизнес-задачи нам выгоднее взять ту или иную модель. И, аналогично со случаем бинарной классификации, мы можем начать строить агрегаты от вектора ошибок, получая тем самым разные метрики.\n\nMSE, RMSE, $R^2$\n\nMSE – одна из самых популярных метрик в задаче регрессии. Она уже знакома вам, т.к. применяется в качестве функции потерь (или входит в ее состав) во многих ранее рассмотренных методах.\n\n$$ MSE(y^{true}, y^{pred}) = \\frac1N\\sum_{i=1}^{N} (y_i - f(x_i))^2 $$\n\nИногда для того, чтобы показатель эффективности MSE имел размерность исходных данных, из него извлекают квадратный корень и получают показатель эффективности RMSE.\n\nMSE неограничен сверху, и может быть нелегко понять, насколько "хорошим" или "плохим" является то или иное его значение. Чтобы появились какие-то ориентиры, делают следующее:\n\nБерут наилучшее константное предсказание с точки зрения MSE — среднее арифметическое меток $\\bar{y}$. При этом чтобы не было подглядывания в test, среднее нужно вычислять по обучающей выборке\n\nРассматривают в качестве показателя ошибки:\n\n$$ R^2 = 1 - \\frac{\\sum_{i=1}^{N} (y_i - f(x_i))^2}{\\sum_{i=1}^{N} (y_i - \\bar{y})^2}.$$\n\nУ идеального решающего правила $R^2$ равен $1$, у наилучшего константного предсказания он равен $0$ на обучающей выборке. Можно заметить, что $R^2$ показывает, какая доля дисперсии таргетов (знаменатель) объяснена моделью.\n\nMSE квадратично штрафует за большие ошибки на объектах. Мы уже видели проявление этого при обучении моделей методом минимизации квадратичных ошибок – там это проявлялось в том, что модель старалась хорошо подстроиться под выбросы.\n\nПусть теперь мы хотим использовать MSE для оценки наших регрессионных моделей. Если большие ошибки для нас действительно неприемлемы, то квадратичный штраф за них - очень полезное свойство (и его даже можно усиливать, повышая степень, в которую мы возводим ошибку на объекте). Однако если в наших тестовых данных присутствуют выбросы, то нам будет сложно объективно сравнить модели между собой: ошибки на выбросах будет маскировать различия в ошибках на основном множестве объектов.\n\nТаким образом, если мы будем сравнивать две модели при помощи MSE, у нас будет выигрывать та модель, у которой меньше ошибка на объектах-выбросах, а это, скорее всего, не то, чего требует от нас наша бизнес-задача.\n\n{% include details.html summary="История из жизни про бананы и квадратичный штраф за ошибку" details=" Из-за неверно введенных данных метка одного из объектов оказалась в 100 раз больше реального значения. Моделировалась величина при помощи градиентного бустинга над деревьями решений. Функция потерь была MSE.\n\nОднажды уже во время эксплуатации случилось ч.п.: у нас появились предсказания, в 100 раз превышающие допустимые из соображений физического смысла значения. Представьте себе, например, что вместо обычных 4 ящиков бананов система предлагала поставить в магазин 400. Были распечатаны все деревья из ансамбля, и мы увидели, что постепенно число ящиков действительно увеличивалось до прогнозных 400.\n\nБыло решено проверить гипотезу, что был выброс в данных для обучения. Так оно и оказалось: всего одна точка давала такую потерю на объекте, что алгоритм обучения решил, что лучше переобучиться под этот выброс, чем смириться с большим штрафом на этом объекте. А в эксплуатации у нас возникли точки, которые плюс-минус попадали в такие же листья ансамбля, что и объект-выброс.\n\nИзбежать такого рода проблем можно двумя способами: внимательнее контролируя качество данных или адаптировав функцию потерь.\n\nАналогично, можно поступать и в случае, когда мы разрабатываем метрику качества: менее жёстко штрафовать за большие отклонения от истинного таргета. " %}\n\nMAE\n\nИспользовать RMSE для сравнения моделей на выборках с большим количеством выбросов может быть неудобно. В таких случаях прибегают к также знакомой вам в качестве функции потери метрике MAE (mean absolute error):\n\n$$ MAE(y^{true}, y^{pred}) = \\frac{1}{N}\\sum_{i=1}^{N} \\left|y_i - f(x_i)\\right| $$\n\nМетрики, учитывающие относительные ошибки\n\nИ MSE и MAE считаются как сумма абсолютных ошибок на объектах.\n\nРассмотрим следующую задачу: мы хотим спрогнозировать спрос товаров на следующий месяц. Пусть у нас есть два продукта: продукт A продаётся в количестве 100 штук, а продукт В в количестве 10 штук. И пусть базовая модель предсказывает количество продаж продукта A как 98 штук, а продукта B как 8 штук. Ошибки на этих объектах добавляют 4 штрафных единицы в MAE.\n\nИ есть 2 модели-кандидата на улучшение. Первая предсказывает товар А 99 штук, а товар B 8 штук. Вторая предсказывает товар А 98 штук, а товар B 9 штук.\n\nОбе модели улучшают MAE базовой модели на 1 единицу. Однако, с точки зрения бизнес-заказчика вторая модель может оказаться предпочтительнее, т.к. предсказание продажи редких товаров может быть приоритетнее. Один из способов учесть такое требование – рассматривать не абсолютную, а относительную ошибку на объектах.\n\nMAPE, SMAPE\n\nКогда речь заходит об относительных ошибках, сразу возникает вопрос: что мы будем ставить в знаменатель?\n\nВ метрике MAPE (mean absolute percentage error) в знаменатель помещают целевое значение:\n\n$$ MAPE(y^{true}, y^{pred}) = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{ \\left|y_i - f(x_i)\\right|}{\\left|y_i\\right|} $$\n\nС особым случаем, когда в знаменателе оказывается $0$, обычно поступают "инженерным" способом: или выдают за непредсказание $0$ на таком объекте большой, но фиксированный штраф, или пытаются застраховаться от подобного на уровне формулы и переходят к метрике SMAPE (symmetric mean absolute percentage error):\n\n$$ SMAPE(y^{true}, y^{pred}) = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{ 2 \\left|y_i - f(x_i)\\right|}{y_i + f(x_i)} $$\n\nЕсли же предсказывается ноль, штраф считаем нулевым.\n\nТаким переходом от абсолютных ошибок на объекте к относительным мы сделали объекты в тестовой выборке равнозначными: даже если мы делаем абсурдно большое предсказание, на фоне которого истинная метка теряется, мы получаем штраф за этот объект порядка 1 в случае MAPE и 2 в случае SMAPE.\n\nWAPE\n\nКак и любая другая метрика, MAPE имеет свои границы применимости: например, она плохо справляется с прогнозом спроса на товары с прерывистыми продажами. Рассмотрим такой пример:\n\nПонедельник Вторник Среда Прогноз 55 2 50 Продажи 50 1 50 MAPE 10% 100% 0%\n\nСреднее MAPE – 36.7%, что не очень отражает реальную ситуацию, ведь два дня мы предсказывали с хорошей точностью. В таких ситуациях помогает WAPE (weighted average percentage error):\n\n$$ WAPE(y^{true}, y^{pred}) = \\frac{\\sum_{i=1}^{N} \\left|y_i - f(x_i)\\right|}{\\sum_{i=1}^{N} \\left|y_i\\right|} $$\n\nЕсли мы предсказываем идеально, то WAPE = 0, если все предсказания отдаём нулевыми, то WAPE = 1.\n\nВ нашем примере получим WAPE = 5.9%\n\nRMSLE\n\nАльтернативный способ уйти от абсолютных ошибок к относительным предлагает метрика RMSLE (root mean squared logarithmic error):\n\n$$ RMSLE(y^{true}, y^{pred}| c) = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N \\left(\\vphantom{\\frac12}\\log{\\left(y_i + c \\right)} - \\log{\\left(f(x_i) + c \\right)}\\right)^2 } $$\n\nгде нормировочная константа $$c$$ вводится искусственно, чтобы не брать логарифм от нуля. Также по построению видно, что метрика пригодна лишь для неотрицательных меток.\n\nВеса в метриках\n\nВсе вышеописанные метрики легко допускают введение весов для объектов. Если мы из каких-то соображений можем определить стоимость ошибки на объекте, можно брать эту величину в качестве веса. Например, в задаче предсказания спроса в качестве веса можно использовать стоимость объекта.\n\nДоля предсказаний с абсолютными ошибками больше, чем d\n\nЕще одним способом охарактеризовать качество модели в задаче регрессии является доля предсказаний с абсолютными ошибками больше заданного порога $d$:\n\n$$\\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}\\left[ \\left| y_i - f(x_i) \\right| > d \\right] $$\n\nНапример, можно считать, что прогноз погоды сбылся, если ошибка предсказания составила меньше 1/2/3 градусов. Тогда рассматриваемая метрика покажет, в какой доле случаев прогноз не сбылся.\n\nКак оптимизировать метрики регрессии?\n\nПусть мы выбрали, что метрика качества алгоритма будет $F(a(X), Y)$. Тогда мы хотим обучить модель так, чтобы F на валидационной выборке была минимальная/максимальная. Аналогично задачам классификации лучший способ добиться минимизации метрики $F$ — выбрать в качестве функции потерь ту же $F(a(X), Y)$. К счастью, основные метрики для регрессии: MSE, RMSE, MAE можно оптимизировать напрямую. С формальной точки зрения MAE не дифференцируема, так как там присутствует модуль, чья производная не определена в нуле. На практике для этого выколотого случая в коде можно возвращать ноль.\n\nДля оптимизации MAPE придётся изменять оптимизационную задачу. Оптимизацию MAPE можно представить как оптимизацию MAE, где объектам выборки присвоен вес $$\\frac{1}{\\vert y_i\\vert}$$.'), Document(metadata={'source': '/Users/eugenekillevsky/PycharmProjects/LLM-Interviewer/data/RAG/ml-handbook-master/chapters/prob_calibration/intro.md'}, page_content='title: Как оценивать вероятности author: stanislav_fedotov toc: true\n\nЭтот список будет заменен оглавлением, за вычетом заголовка "Contents", к которому добавлен класс no_toc. {:toc}\n\nМы уже упоминали, что оценивать вероятности классов как $$softmax(f_w(x_i))$$ для какой-то произвольной функции $$f_w$$ – это дело подозрительное. В этом разделе мы поговорим о том, как это делать хорошо и правильно.\n\nЧто же такое вероятность класса, если объект либо принадлежит этому классу, либо нет?\n\nОграничимся пока случаем двуклассовой классификации с классами 0 и 1. Пожалуй, если утверждается, что мы предсказываем корректную вероятность класса 1 (обозначим её $$q(x_i)$$), то прогноз <<объект $$x_i$$ принадлежит классу 1 с вероятностью $$\\frac23$$>> должен сбываться в $$\\frac23$$ случаев. То есть, условно говоря, если мы возьмём все объекты, которым мы предсказали вероятностью $$\\frac23$$, то среди них что-то около двух третей действительно имеет класс 1. На математическом языке это можно сформулировать так: Если $$\\widehat{p}$$ – предсказанная вероятность класса 1, то $$P(y_i = 1 \\vert q(x_i) = \\widehat{p}) = \\widehat{p}$$.\n\nК сожалению, в реальной жизни $$\\widehat{p}$$ – это скорее всего вещественные числа, которые будут различными для различных $$y_i$$, и никаких вероятностей мы не посчитаем, но мы можем разбить отрезок $$[0,1]$$ на бины, внутри каждого из которых уже вычислить, каковая там доля объектов класса 1, и сравнить эту долю со средним значением вероятности в бине:\n\nУ модели, которая идеально предсказывает вероятности (как обычно говорят, у идеально калиброванной модели) красные точки на диаграме калибровки должны совпадать с синими.\n\nА вот на картинке выше это не так: красные точки всегда ниже синих. Давайте поймём, что это значит. Получается, что наша модель систематически завышает предсказанную вероятность (синие точки), и порог отсечения нам, выходит, тоже надо было бы сдвинуть вправо:\n\nНо такая картинка, пожалуй, говорит о какой-то серьёзной патологии классификатора; гораздо чаще встречаются следующие две ситуации:\n\nСлишком уверенный (overconfident) классификатор: Такое случается с сильными классификаторыми (например, нейросетями), которые учились на метки классов, а не на вероятности: тем самым процесс обучения стимулировал их всегда давать как можно более близкий к 0 или 1 ответ.\n\nНеуверенный (underconfident) классификатор:\n\nТакое может случиться, например, если мы слишком много обращаем внимания на трудные для классификации объекты на границе классов (как, скажем, в SVM), в каком-то смысле в ущерб более однозначно определяемым точкам. Этим же могут и грешить модели на основе бэггинга (например, случайный лес). Грубо говоря, среднее нескольких моделей предскажет что-то близкое к единице только если все слагаемые предскажут что-то, близкое к единице – но из-за дисперсии моделей это будет случаться реже, чем могло бы. См. статью.\n\nВам скажут: логистическая регрессия корректно действительно предсказывает вероятности\n\nВам даже будут приводить какие-то обоснования. Важно понимать, что происходит на самом деле, и не дать ввести себя в заблуждение. В качестве противоядия от иллюзий предлагаем рассмотреть два примера.\n\nРассмотрим датасет c двумя классами (ниже на картинке обучающая выборка)\n\nОбучим на нём логистическую регрессию из sklearn безо всяких параметров (то есть $$L^2$$-регуляризованную, но это не так важно). Классы не так-то просто разделить, вот и логистическая регрессия так себе справляется. Ниже изображена часть тестовой выборки вместе с предсказанными вероятностями классов для всех точек области\n\nВидим, что модель не больно-то уверена в себе, и ясно почему: признаковое описание достаточно бедное и не позволяет нам хорошо разделить классы, хотя, казалось бы, это можно довольно неплохо сделать.\n\nПопробуем поправить дело, добавив полиномиальные фичи, то есть все $$x^jy^k$$ для $$0\\leqslant j,k\\leqslant 5$$ в качестве признаков, и обучив поверх этих данных логистическую регрессию. Снова нарисуем некоторые точки тестовой выборки и предсказания вероятностей для всех точек области:\n\nВидим, что имеет место сочетание двух проблем: неуверенности посередине и очень уверенных ошибок по краям.\n\nНарисуем теперь калибровочные кривые для обеих моделей:\n\nКалибровочные кривые весьма примечательны; в любом случае ясно, что с предсказанием вероятностей всё довольно плохо. Посмотрим ещё, какие вероятности наши классификаторы чаще приписывают объектам:\n\nКак и следовало ожидать, предсказания слабого классификатора тяготеют к серединке (та самая неуверенность), а среди предсказаний переобученного очень много крайне уверенных (и совсем не всегда правильных).\n\nНо почему же все твердят, что логистическая регрессия хорошо калибрована?!\n\nПопробуем понять и простить её.\n\nКак мы помним, логистическая регрессия учится путём минимизации функционала\n\n$$l(X, y) = -\\sum_{i=1}^N(y_i\\log(\\sigma(\\langle w, x_i\\rangle)) + (1 - y_i)\\log(1 - \\sigma(\\langle w, x_i\\rangle)))$$\n\nОтметим между делом, что каждое слагаемое – это кроссэнтропия распределения $$P$$, заданного вероятностями $$P(0) = 1 - \\sigma(\\langle w, x_i\\rangle)$$ и $$P(1) = \\sigma(\\langle w, x_i\\rangle)$$, и тривиального распределения, которое равно $$y_i$$ с вероятностью $$1$$.\n\nДопустим, что мы обучили по всему универсуму данных $$\\mathbb{X}$$ идеальную логистическую регрессию с идеальными весами $$w^{\\ast}$$. Пусть, далее, оказалось, что у нас есть $$n$$ объектов $$x_1,\\ldots,x_n$$ с одинаковым признаковым описанием (то есть по сути представленных одинаковыми векторами $$x_i$$), но, возможно, разными истинными метками классов $$y_1,\\ldots,y_n$$. Тогда соответствующий им кусок функции потерь имеет вид\n\n$$-\\left(\\sum_{i=1}^ny_i\\right)\\log(\\sigma(\\langle w, x_1\\rangle)) -\\left(\\sum_{i=1}^n (1 - y_i)\\right)\\log(1 - \\sigma(\\langle w, x_1\\rangle)) =$$\n\n$$=-n\\left(\\vphantom{\\frac12}p_0\\log(\\sigma(\\langle w, x_1\\rangle)) + p_1\\log(1 - \\sigma(\\langle w, x_1\\rangle))\\right)$$\n\nгде $$p_j$$ – частота $$j$$-го класса среди истинных меток. В скобках также стоит кросс-энтропия распределения, задаваемого частотой меток истинных классов, и распределения, предсказываемого логистической регрессией. Минимальное значение кросс-энтропии (и минимум функции потерь) достигается, когда\n\n$$\\sigma(\\langle w, x_1\\rangle) = p_0,\\quad 1 - \\sigma(\\langle w, x_1\\rangle) = p_1$$\n\nТеперь, если признаковое описание данных достаточно хорошее (то есть классы не перемешаны как попало и всё-таки близки к разделимым) и в то же время модель не переобученная (то есть, в частности, предсказания вероятностей не скачут очень уж резко – вспомните второй пример), то результат, полученный для $$n$$ совпадающих точек будет приблизительно верным и для $$n$$ достаточно близких точек: на всех них модель будет выдавать примерно долю положительных, то есть тоже хорошую оценку вероятности.\n\nКак же всё-таки предсказать вероятности: методы калибровки\n\nПусть наша модель (бинарной классификации) для каждого объекта $$x_i$$ выдаёт некоторое число $$q(x_i)\\in[0,1]$$. Как же эти числа превратить в корректные вероятности?\n\nГистограммная калибровка. Мы разбиваем отрезок $$[0,1]$$ на бины $$\\mathbb{B}_1,\\ldots,\\mathbb{B}_k$$ (одинаковой ширины или равномощные) и хотим на каждом из них предсказывать всегда одну и ту же вероятность: $$\\theta_j$$, если $$q(x_i)\\in \\mathbb{B}_j$$. Вероятности $$\\theta_i$$ подбираются так, чтобы они как можно лучше приближали средние метки классов на соответствующих бинах; иными словами, мы решаем задачу\n\n$$\\sum_{j=1}^k\\left|\\frac{\\sum_{i=1}^N\\mathbb{I}{q(x_i)\\in\\mathbb{B}j}y_i}{ \\vert \\mathbb{B}_j \\vert } - \\theta_j\\right|\\longrightarrow\\min\\limits{(\\theta_1,\\ldots,\\theta_k)}$$\n\nВместо разности модулей можно рассматривать и разность квадратов.\n\nМетод довольно простой и понятный, но требует подбора числа бинов и предсказывает лишь дискретное множество вероятностей.\n\nИзотоническая регрессия. Этот метод похож на предыдущий, только мы будем, во-первых, настраивать и границы $$0=b_0,b_1,\\ldots,b_k = 1$$ бинов $$\\mathbb{B}j = {t \\vert b{j-1}\\leqslant b_j}$$, а кроме того, накладываем условие $$\\theta_1\\leqslant\\ldots\\leqslant\\theta_k$$. Искать $$b_j$$ и $$\\theta_j$$ мы будем, приближая $$y_i$$ кусочно постоянной функцией $$g$$ от $$q(x_i)$$:\n\n$$\\sum_{i=1}^N(y_i - g(q(x_i)))^2\\longrightarrow\\min_{g}$$\n\nМинимизация осуществляется при помощи pool adjacent violators algorithm, и эти страницы слишком хрупки, чтобы выдержать его формулировку.\n\nКалибровка Платта представляет собой по сути применение сигмоиды поверх другой модели (то есть самый наивный способ получения \'\'вероятностей\'\'). Более точно, если $$q(x_i)$$ – предсказанная вероятность, то мы полагаем\n\n$$P(y_i = 1\\mid x_i) = \\sigma(aq(x_i) + b) = \\frac1{1 + e^{-aq(x_i) - b}}$$\n\nгде $$a$$ и $$b$$ подбираются методом максимального правдоподобия на отложенной выборке:\n\n$$-\\sum_{i=1}^N(\\vphantom{\\frac12}y_i\\log(\\sigma(q(x_i)) + (1 - y_i)\\log(1 - \\sigma(q(x_i))))\\longrightarrow\\min\\limits_{a,b}$$\n\nДля избежания переобучения Платт предлагал также заменить метки $$y_i$$ и $$(1 - y_i)$$ на регуляризованные вероятности таргетов:\n\n$$t_0 = \\frac1{#{i \\vert y_i = 0} + 2},\\quad t_0 = \\frac{#{i \\vert y_i = 1}}{#{i \\vert y_i = 0} + 2}$$\n\nКалибровка Платта неплохо справляется с выколачиванием вероятностей из SVM, но для более хитрых классификаторов может спасовать. В целом, можно показать, что этот метод хорошо работает, если для каждого из истинных классов предсказанные вероятности $$q(x_i)$$ распределы нормально с одинаковыми дисперсиями. Подробнее об этом вы можете почитать в этой статье. Там же описано обобщение данного подхода – бета-калибровка.\n\nС большим количеством других методов калибровки вы можете познакомиться в этой статье\n\nКак измерить качество калибровки\n\nКалибровочные кривые хорошо показывают, что есть проблемы, но как оценить наши потуги по улучшению предсказания вероятностей? Хочется иметь какую-то численную метрику. Мы упомянем две разновидности, которые по сути являются прямым воплощением описанных выше идей.\n\nExpected/Maximum calibration error. Самый простой способ, впрочем, является наследником идеи с калибровочной кривой. А именно, разобьём отрезок $$[0,1]$$ на бины $$\\mathbb{B}_1,\\ldots,\\mathbb{B}_k$$ по предсказанным вероятностям и вычислим\n\n$$\\sum_{j=1}^k\\frac{#\\mathbb{B}_j}{N}\\left|\\overline{y}(\\mathbb{B}_j) - \\overline{q}(\\mathbb{B}_j)\\right|$$\n\nили\n\n$$\\max\\limits_{j=1,\\ldots,k}\\left|\\overline{y}(\\mathbb{B}_j) - \\overline{q}(\\mathbb{B}_j)\\right|$$\n\nгде $$\\overline{y}(\\mathbb{B}_j)$$ – среднее значение $$y_i$$, а $$\\overline{q}(\\mathbb{B}_j)$$ – среднее значение $$q(x_i)$$ для $$x_i$$, таких что $$q(x_i)\\in\\mathbb{B}_j$$. Проблема этого способа в том, что мы можем очень по-разному предсказывать в каждом из бинов вероятности (в том числе константой) без ущерба для метрики.\n\nОдна из популярных метрик – это Brier score, которая попросту измеряет разницу между предсказанными вероятностями и $$ y_i $$:\n\n$$\\sum_{i=1}^N(y_i - q(x_i))^2$$\n\nКазалось бы, в чём смысл? Немного подрастить мотивацию помогает следующий пример. Допустим, наши таргеты совершенно случайны, то есть $$P(y_i = 1 \\vert x_i) = P(y_i)$$. Тогда хорошо калиброванный классификатор должен для каждого $$x_i$$ предсказывать вероятность $$\\frac12$$; соответственно, его brier score равен $$\\frac14$$. Если же классификатор хоть в одной точке выдаёт вероятность $$p>\\frac12$$, то в маленькой окрестности он должен выдавать примерно такие же вероятности; поскольку же таргет случаен, локальный кусочек суммы из brier score будет иметь вид $$\\frac{N\'}{2}p^2 + \\frac{N\'}{2}(1-p)^2 < \\frac{N\'}2$$, что хуже, чем получил бы всегда выдающий $$\\frac12$$ классификатор.\n\nНе обязательно брать квадратичную ошибку; сгодится и наш любимый log-loss:\n\n$$\\sum_{i=1}^N\\left(\\vphantom{\\frac12}y_i\\log{q(x_i)} + (1 - y_i)\\log(1 - q(x_i))\\right)$$\n\nЭто же и помогает высветить ограничения подхода, если вспомнить рассуждения о калиброванности логистической регрессии. Для достаточно гладких классификатора и датасета briar score и log-loss будут адекватными средствами оценки, но если нет – возможно всякое.\n\nВопрос на засыпку: а как быть, если у нас классификация не бинарная, а многоклассовая? Что такое хорошо калиброванный классификатор? Как это определить численно? Как заставить произвольный классификатор предсказывать вероятности?\n\nМы не будем про это рассказывать, но призываем читателя подумать над этим самостоятельно или, например, посмотреть туториал с ECML KDD 2020.'), Document(metadata={'source': '/Users/eugenekillevsky/PycharmProjects/LLM-Interviewer/data/RAG/ml-handbook-master/chapters/grad_boost/intro.md'}, page_content='title: Градиентный бустинг author: kirill_lunev, evgenia_elistarova\n\nЭтот список будет заменен оглавлением, за вычетом заголовка "Contents", к которому добавлен класс no_toc. {:toc}\n\nВ прошлых разделах мы научились соединять базовые алгоритмы в ансамбль с помощью бэггинга (и, в частности, строить из решающих деревьев случайные леса). Теперь мы рассмотрим другой способ объединять базовые алгоритмы в композицию – градиентный бустинг.\n\nВ ходе обучения случайного леса каждый базовый алгоритм строится независимо от остальных. Бустинг, в свою очередь, воплощает идею последовательного построения линейной комбинации алгоритмов. Каждый следующий алгоритм старается уменьшить ошибку текущего ансамбля.\n\nБустинг, использующий деревья решений в качестве базовых алгоритмов, называется градиентным бустингом над решающими деревьями, Gradient Boosting on Decision Trees, GBDT. Он отлично работает на выборках с <<табличными>>, неоднородными данными. Примером таких данных может служить описание пользователя Яндекса через его возраст, пол, среднее число поисковых запросов в день, число заказов такси и так далее. Такой бустинг способен эффективно находить нелинейные зависимости в данных различной природы. Этим свойством обладают все алгоритмы, использующие деревья решений, однако именно GBDT обычно выигрывает в подавляющем большинстве задач. Благодаря этому он широко применяется во многих конкурсах по машинному обучению и задачах из индустрии (поисковом ранжировании, рекомендательных системах, таргетировании рекламы, предсказании погоды, пункта назначения такси и многих других).\n\nНе так хорошо бустинг проявляет себя на однородных данных: текстах, изображениях, звуке, видео. В таких задачах нейросетевые подходы почти всегда демонстрируют лучшее качество.\n\nХотя деревья решений и являются традиционным выбором для объединения в ансамбли, никто не запрещает использовать и другие алгоритмы, например, линейные модели в качестве базовых (эта возможность реализована в пакете XGBoost). Стоит только понимать, что построенная композиция по сути окажется линейной комбинацией линейных моделей, то есть опять-таки линейной моделью (или нейросетью с одним полносвязным слоем). Это уменьшает возможности ансамбля эффективно определять нелинейные зависимости в данных. В рамках данной главы мы будем рассматривать только бустинг над решающими деревьями.\n\nИнтуиция\n\nРассмотрим задачу регрессии с квадратичной функцией потерь:\n\n$$\\mathcal{L}(y, x) = \\frac{1}{2}\\sum^{N}_{i=1}\\left(y_i - a(x_i)\\right)^{2} \\rightarrow \\min$$\n\nДля решения будем строить композицию из $K$ базовых алгоритмов\n\n$$a(x) = a_K(x) = b_1(x) + b_2(x) + \\dots +b_K(x)$$\n\nЕсли мы обучим единственное решающее дерево, то качество такой модели, скорее всего, будет низким. Однако о построенном дереве мы знаем, на каких объектах оно давало точные предсказания, а на каких ошибалось.\n\nПопробуем использовать эту информацию и обучим еще одну модель. Допустим, что предсказание первой модели на объекте $x_l$ на 10 больше, чем необходимо (т.е. $b_1(x_l) = y_l + 10$). Если бы мы могли обучить новую модель, которая на $x_l$ будет выдавать ответ $-10$, то сумма ответов этих двух моделей на объекте $x_l$ в точности совпала бы с истинным значением:\n\n$$ b_1(x_l) + b_2(x_l) = (y_l + 10) + (-10) = y_l $$\n\nДругими словами, если вторая модель научится предсказывать разницу между реальным значением и ответом первой, то это позволит уменьшить ошибку композиции.\n\nВ реальности вторая модель тоже не сможет обучиться идеально, поэтому обучим третью, которая будет <<компенсировать>> неточности первых двух. Будем продолжать так, пока не построим композицию из $K$ алгоритмов.\n\nДля объяснения метода градиентного бустинга полезно воспользоваться следующей аналогией. Бустинг можно представить как гольфиста, цель которого – загнать мяч в лунку с координатой $y_{\\text{ball}}$. Положение мяча здесь – ответ композиции $a(x_{\\text{ball}})$. Гольфист мог бы один раз ударить по мячу, не попасть в лунку и пойти домой, но настырность заставляет его продолжить.\n\n(источник картинки)\n\nПо счастью, ему не нужно начинать каждый раз с начальной позиции. Следующий удар гольфиста переводит мяч из текущего положения $a_k(x_{\\text{ball}})$ в положение $a_{k+1}(x_{\\text{ball}})$. Каждый следующий удар – это та поправка, которую вносит очередной базовый алгоритм в композицию. Если гольфист все делает правильно, то функция потерь будет уменьшаться: $$\\mathcal{L}(y, a_{k+1}(x)) < \\mathcal{L}(y, a_{k}(x)),$$ то есть мяч постепенно будет приближаться к лунке.\n\nУдары при этом делаются не хаотично. Гольфист оценивает текущее положение мяча относительно лунки и следующим ударом старается нивелировать те проблемы, которые он создал себе всеми предыдущими. Подбираясь к лунке, он будет бить всё аккуратнее и, возможно, даже возьмет другую клюшку, но точно не будет лупить так же, как из первоначальной позиции. В итоге комбинация всех ударов рано или поздно перенесет мяч в лунку.\n\nПодобно тому, как гольфист постепенно подводит мяч к цели, бустинг с каждым новым базовым алгоритмом всё больше приближает предсказание к истинному значению метки объекта.\n\nРассмотрим теперь другую аналогию – разложение функции в ряд Тейлора. Из курса математического анализа известно, что (достаточно хорошую) бесконечно дифференцируемую функцию $f(x)$ на интервале $x \\in \\left(a - R, a + R\\right)$ можно представить в виде бесконечной суммы степенных функций:\n\n$$f(x) = \\sum\\limits_{n = 1}^{\\infty}\\frac{f^{(n)}\\left(a\\right)}{n!}\\left(x - a\\right)^{n}.$$\n\nОдна, самая первая степенная функция в разложении, очень грубо приближает $f(x)$. Прибавляя к ней следующую, мы получим более точное приближение. Каждая следующая элементарная функция увеличивает точность приближения, но менее заметна в общей сумме. Если нам не требуется абсолютно точное разложение, вместо бесконечного ряда Тейлора мы можем ограничиться суммой его первых $k$ элементов. Таким образом, интересующую нас функцию мы с некоторой точностью представили в виде суммы <<простых>> функций.\n\nПеренесём эту идею на задачи машинного обучения. В машинном обучении мы пытаемся по выборке $(x_i, y_i)$ восстановить неизвестную истинную зависимость. Прежде всего, мы выбираем подходящий алгоритм. Мы можем выбрать <<сложный>> алгоритм, который сразу хорошо выучит истинную зависимость. А можем обучить <<простой>>, который выучит истинную зависимость посредственно. Затем мы добавим к нему ещё один такой простой алгоритм, чтобы уточнить предсказание первого алгоритма. Продолжая этот процесс, мы получим сумму простых алгоритмов, где первый алгоритм грубо приближает истинную зависимость, а каждый следующий делает приближение всё точнее.\n\nПример с задачей регрессии: формальное описание\n\nРассмотрим тот же пример с задачей регрессии и квадратичной функцией потерь:\n\n$$\\mathcal{L}(y, x) = \\frac{1}{2}\\sum\\limits^{N}_{i=1}\\left(y_i - a(x_i)\\right)^{2} \\rightarrow \\min$$\n\nДля решения также будем строить композицию из $K$ базовых алгоритмов семейства $\\mathcal{B}$:\n\n$$a(x) = a_K(x) = b_1(x) + b_2(x) + \\dots + b_K(x)$$\n\nВ качестве базовых алгоритмов выберем, как и условились в начале главы, семейство $\\mathcal{B}$ решающих деревьев некоторой фиксированной глубины.\n\nИспользуя известные нам методы построения решающих деревьев, обучим алгоритм $b_1(x) \\in \\mathcal{B}$, который наилучшим образом приближает целевую переменную:\n\n$$ b_1(x) = \\underset{b\\in \\mathcal{B}}{\\mathrm{argmin}} \\, \\mathcal{L}(y, b(x))$$\n\nПостроенный алгоритм $b_1(x)$, скорее всего, работает не идеально. Более того, если базовый алгоритм работает слишком хорошо на обучающей выборке, то высока вероятность переобучения (низкий уровень смещения, но высокий уровень разброса). Далее вычислим, насколько сильно отличаются предсказания этого дерева от истинных значений:\n\n$$s_i^{1} = y_i - b_1(x_i)$$\n\nТеперь мы хотим скорректировать $b_1(x)$ с помощью $b_2(x)$; в идеале так, чтобы $b_2(x)$ идеально предсказывал величины $s_i^{1}$, ведь в этом случае\n\n$$ a_2(x_i) = b_1(x_i) + b_2(x_i) = \\ = b_1(x_i) + s_i^1 = b_1(x_i) + (y_i - b_1(x_i)) = y_i $$\n\nНайти совершенный алгоритм, скорее всего, не получится, но по крайней мере мы можем выбрать из семейства наилучшего представителя для такой задачи. Итак, второе решающее дерево будет обучаться предсказывать разности $s_i^1$:\n\n$$ b_2(x) = \\underset{b\\in \\mathcal{B}}{\\mathrm{argmin}} \\, \\mathcal{L}(s^1, b(x))$$\n\nОжидается, что композиция из двух таких моделей $a_2(x) = b_1(x) + b_2(x)$ станет более качественно предсказывать целевую переменную $y$.\n\nДалее рассуждения повторяются до построения всей композиции. На $k$-ом шаге вычисляется разность между правильным ответом и текущим предсказанием композиции из $k - 1$ алгоритмов:\n\n$$ s_i^{k - 1} = y_i - \\sum_{i=1}^{k - 1} b_{k - 1}(x_i) = y_i - a_{k - 1}(x_i)$$\n\nЗатем $k$-й алгоритм учится предсказывать эту разность:\n\n$$ b_k(x) = \\underset{b\\in \\mathcal{B}}{\\mathrm{argmin}} \\, \\mathcal{L}(s^{k - 1}, b(x)), $$\n\nа композиция в целом обновляется по формуле\n\n$$a_k(x) = a_{k - 1}(x) + b_k(x)$$\n\nОбучение $K$ базовых алгоритмов завершает построение композиции.\n\nОбобщение на другие функции потерь\n\nИнтуиция\n\nОтметим теперь важное свойство функции потерь в рассмотренном выше примере с регрессией. Для этого посчитаем производную функции потерь по предсказанию $z = a_k(x_i)$ модели для $i$-го объекта:\n\n$$ \\frac{\\partial{\\mathcal{L}(y_i,z)}}{\\partial{z}}\\bigg|{z=a_k(x_i)} = \\frac{\\partial}{\\partial{z}}\\frac{1}{2}\\left(y_i - z\\right)^{2}\\bigg|{z=a_k(x_i)} = a_k(x_i) - y_i $$\n\nВидим, что разность, на которую обучается $k$-й алгоритм, выражается через производную:\n\n$$s_i^{k} = y_i -a_k(x_i) = -\\frac{\\partial{\\mathcal{L}(y_i,z)}}{\\partial{z}}\\bigg|_{z=a_k(x_i)}$$\n\nТаким образом, для каждого объекта $x_i$ очередной алгоритм в бустинге обучается предсказывать антиградиент функции потерь по предсказанию модели $-\\frac{\\partial{\\mathcal{L}(y_i,z)}}{\\partial{z}}$ в точке $a_k(x_i)$ предсказания текущей части композиции на объекте $x_i$.\n\nПочему же это важно? Дело в том, что это наблюдение позволяет обобщить подход построения бустинга на произвольную дифференцируемую функцию потерь. Для этого мы заменяем обучение на разность $s_i^k$ обучением на антиградиент функции потерь $(-g_i^k)$, где\n\n$$g_i^k =\\frac{\\partial{\\mathcal{L}(y_i,z)}}{\\partial{z}}\\bigg|_{z=a_k(x_i)}$$\n\nОбучение композиции можно представить (вспомните аналогию с гольфистом) как перемещение предсказания из точки $(a_k(x_1), a_k(x_2), \\dots, a_k(x_N))$ в точку $(a_{k+1}(x_1), a_{k+1}(x_2), \\dots, a_{k+1}(x_N))$. В конечном итоге мы ожидаем, что точка $(a_K(x_1), a_K(x_2), \\dots, a_K(x_N))$ будет располагаться как можно ближе к точке с истинными значениями $(y_1, y_2, \\dots, y_N)$.\n\nВ случае квадратичной функции потерь интуиция вполне подкрепляется математикой. Изменится ли что-либо в наших действиях, если мы поменяем квадратичную функцию потерь на любую другую? С одной стороны, мы, как и прежде, можем двигаться в направлении уменьшения разности предсказания и истинного значения: любая функция потерь поощряет такие шаги для каждого отдельного объекта, ведь для любой адекватной функции потерь выполнено $\\mathcal{L}(y, y) = 0$.\n\nНо мы можем посмотреть на задачу и с другой стороны: не с точки зрения уменьшения расстояния между вектором предсказаний и вектором истинных значений, а с точки зрения уменьшения значения функции потерь. Для наискорейшего уменьшения функции потерь нам необходимо шагнуть в сторону её антиградиента по вектору предсказаний текущей композиции, то есть как раз таки в сторону вектора $(-g_1^k,\\dots,-g_N^k)$. Это направление не обязано совпадать с шагом по направлению уменьшения разности предсказания и истинного значения. Например, может возникнуть гипотетическая ситуация, как на рисунке ниже:\n\nВ изображённом примере рассматриваются два объекта $x_1$ и $x_2$. Текущее предсказание для них – $(a_k(x_1), a_k(x_2))$, а окружность определяет варианты следующего шага: первый вариант – пойти в направлении $(s_1^k, s_2^k)$, как делалось ранее; второй – пойти в направлении антиградиента. Также показаны линии уровня значений функции потерь. Функция потерь в этом примере устроена таким образом, что $L_2 < L_1$, из-за чего шаг по антиградиенту оказывается более выгодным.\n\nМатематическое обоснование\n\nПопробуем записать наши интуитивные соображения более формально. Пусть $\\mathcal{L}$ – дифференцируемая функция потерь, а наш алгоритм $a(x)$ представляет собой композицию базовых алгоритмов:\n\n$$ a(x) = a_k(x) = b_1(x) + \\ldots + b_k(x) $$\n\nМы строим нашу композицию <<жадно>>:\n\n$$ a_k(x) = a_{k - 1}(x) + b_k(x), $$\n\nгде вновь добавляемый базовый алгоритм $b_k$ обучается так, чтобы улучшить предсказания текущей композиции:\n\n$$ b_k = \\underset{b\\in \\mathcal{B}}{\\mathrm{argmin}} \\sum_{i = 1}^N \\mathcal{L}(y_i, a_{k - 1}(x_i) + b(x_i)) $$\n\nМодель $b_0$ выбирается так, чтобы минимизировать потери на обучающей выборке:\n\n$$ b_0 = \\underset{b\\in \\mathcal{B}}{\\mathrm{argmin}} \\sum_{i = 1}^N \\mathcal{L}(y_i, b(x_i)) $$\n\nДля построения базовых алгоритмов на следующих шагах рассмотрим разложение Тейлора функции потерь $\\mathcal L$ до первого члена в окрестности точки $(y_i, a_{k - 1}(x_i))$:\n\n$$ \\mathcal{L}(y_i, a_{k - 1}(x_i) + b(x_i)) \\approx \\mathcal{L}(y_i, a_{k - 1}(x_i)) + b(x_i) \\frac{\\partial \\mathcal{L}(y_i, z)}{\\partial z} \\bigg|{z = a{k - 1}(x_i)} = \\mathcal{L}(y_i, a_{k - 1}(x_i)) + b(x_i) g_i^{k - 1} $$\n\nИзбавившись от постоянных членов, мы получим следующую оптимизационную задачу:\n\n$$ b_k \\approx \\underset{b\\in \\mathcal{B}}{\\mathrm{argmin}} \\sum_{i = 1}^N b(x_i) g_i^{k - 1} $$\n\nПоскольку суммируемое выражение – это скалярное произведение двух векторов, его значение минимизируют $b(x_i)$, пропорциональные значениям $-g_i^{k - 1}$. Поэтому на каждой итерации базовые алгоритмы $b_k$ обучаются предсказывать значения антиградиента функции потерь по текущим предсказаниям композиции.\n\nИтак, использованная нами интуиция шага в сторону <<уменьшения остатка>> удивительным образом привела к оптимальным смещениям в случае квадратичной функции потерь, но для других функций потерь это не так: для них смещение происходит в сторону антиградиента.\n\nОбучение базового алгоритма\n\nПри построении очередного базового алгоритма $b_{k+1}$ мы решаем задачу регрессии с таргетом, равным антиградиенту функции потерь исходной задачи на предсказании $a_k = b_1 + \\ldots + b_k$.\n\nТеоретически можно воспользоваться любым методом построения регрессионного дерева. Важно выбрать оценочную функцию $S$, которая будет показывать, насколько текущая структура дерева хорошо приближает антиградиент. Её нужно будет использовать для построения критерия ветвления:\n\n$$ |R| \\cdot S(R) - |R_{right}| \\cdot S(R_{right}) - |R_{left}| \\cdot S(R_{left}) \\rightarrow \\max,$$\n\nгде $S(R)$ – значение функции $S$ в вершине $R$, $S(R_{left}), S(R_{right})$ – значения в левом и правом сыновьях $R$ после добавления предиката, $\\mid \\, \\cdot \\, \\mid$ – количество элементов, пришедших в вершину.\n\nНапример, можно использовать следующие оценочные функции:\n\n$$ L_2(g, p) = \\sum\\limits_{i=1}^N\\left(p_i - g_i\\right)^2,\\ Cosine(g, p) = -\\frac{\\sum\\limits_{i=1}^N(p_i \\cdot g_i)}{\\sqrt{\\sum\\limits_{i=1}^Np_i^2} \\cdot \\sqrt{\\sum\\limits_{i=1}^Ng_i^2}}, $$\n\nгде $p_i$ – предсказание дерева на объекте $x_i$, $g_i$ – антиградиент, на который учится дерево, $$ p = { p_i }{i=1}^N $$, $$ g = { g_i }{i=1}^N $$. Функция $L_2$ представляет собой среднеквадратичную ошибку, а функция $Cosine$ определяет близость через косинусное расстояние между векторами предсказаний и антиградиентов.\n\nВ итоге обучение базового алгоритма проходит в два шага: - по функции потерь вычисляется целевая переменная для обучения следующего базового алгоритма:\n\n$$g_i^k =\\frac{\\partial{\\mathcal{L}(y_i,z)}}{\\partial{z}}\\bigg|_{z=a_k(x_i)}$$\n\nстроится регрессионное дерево на обучающей выборке $(x_i, -g_i^k)$, минимизирующее выбранную оценочную функцию.\n\nНа практике\n\nПоскольку для построения градиентного бустинга достаточно уметь считать градиент функции потерь по предсказаниям, с его помощью можно решать широкий спектр задач. В библиотеках градиентного бустинга даже реализована возможность создавать свои функции потерь: для этого достаточно уметь вычислять ее градиент, зная истинные значения и текущие предсказания для элементов обучающей выборки.\n\nТипичный градиентный бустинг имеет в составе несколько тысяч деревьев решений, которые необходимо строить последовательно. Построение решающего дерева на выборках типичного размера и современном железе, даже с учетом всех оптимизаций, требует небольшого, но всё-таки заметного времени (0.1-1c), которое для всего ансамбля превратится в десятки минут. Это не так быстро, как обучение линейных моделей, но всё-таки значительно быстрее, чем обучение типичных нейросетей.\n\nТемп обучения (learning rate)\n\nОбучение композиции с помощью градиентного бустинга может привести к переобучению, если базовые алгоритмы слишком сложные. Например, если сделать решающие деревья слишком глубокими (более 10 уровней), то при обучении бустинга ошибка на обучающей выборке даже при довольно скромном $K$ может приблизиться к нулю, то есть предсказание будет почти идеальным, но на тестовой выборке всё будет плохо.\n\nСуществует два решения этой проблемы. Во-первых, необходимо упростить базовую модель, уменьшив глубину дерева (либо примерив какие-либо ещё техники регуляризации). Во-вторых, мы можем ввести параметр, называемый темпом обучения (learning rate) $\\eta \\in (0, 1]$:\n\n$$ a_{k+1}(x) = a_{k}(x) + \\eta b_{k+1}(x) $$\n\nПрисутствие этого параметра означает, что каждый базовый алгоритм вносит относительно небольшой вклад во всю композицию: если расписать сумму целиком, она будет иметь вид\n\n$$a_{k+1}(x) = b_1(x) + \\eta b_2(x) + \\eta b_3(x) + \\ldots + \\eta b_{k+1}(x)$$\n\nЗначение параметра обычно определяется эмпирически по входным данным. В библиотеке CatBoost темп обучения может быть выбран автоматически по набору данных. Для этого используется заранее обученная линейная модель, предсказывающая темп обучения по мета-параметрам выборки данных: числу объектов, числу признаков и другим.\n\nFeature importance\n\nОтдельные деревья решений можно легко интерпретировать, просто визуализируя их структуру. Но в модели градиентного бустинга содержатся сотни деревьев, и поэтому её нелегко интерпретировать путем визуализации входящих в неё деревьев. При этом хотелось бы, как минимум, понимать, какие именно признаки в данных оказывают наибольшее влияние на предсказание композиции.\n\nМожно сделать следующее наблюдение: признаки, используемые в верхней части дерева, влияют на окончательное предсказание для большей доли обучающих объектов, чем признаки, попавшие на более глубокие уровни. Таким образом, ожидаемая доля обучающих объектов, для которых происходило ветвление по данному признаку, может быть использована в качестве оценки его относительной важности для итогового предсказания. Усредняя полученные оценки важности признаков по всем решающим деревьям из ансамбля, можно уменьшить дисперсию такой оценки и использовать ее для отбора признаков. Этот метод известен как MDI (mean decrease in impurity). Существуют и другие методы оценки важности признаков для ансамблей: например, Permutation feature importance (см. описание в sklearn) и множество разных подходов, предлагаемых в библиотеке CatBoost. Все эти техники отбора признаков применимы также и для случайных лесов.\n\nРеализации\n\nДля общего развития имеет смысл посмотреть реализацию в sklearn, но на практике она весьма медленная и не такая уж умная.\n\nХороших реализаций GBDT есть, как минимум, три: LightGBM, XGBoost и CatBoost. Исторически они отличались довольно сильно, но за последние годы успели скопировать друг у друга все хорошие идеи.\n\nГде используется\n\nВезде :) На сегодня градиентный бустинг – это, фактически, один из двух подходов, которые используются на практике (второй – это нейронные сети, конечно). Он формально слабее и менее гибок, чем сети, но выигрывает в простоте настройки темпа обучения и применения, размере и интерпретируемости модели.\n\nВо многих компаниях, так или иначе связанных с ML, он используется для всех задач, которые не связаны с однородными данными (картинками, текстами, etc). Типичный поисковый запрос в Яндексе, выбор отеля на Booking.com или сериала на вечер в Netflix задействует несколько десятков моделей GBDT.\n\nВпрочем, в будущем можно ожидать плавного исчезновения этого подхода, так как улучшение архитектур глубинного обучения и дальнейшее развитие железа нивелирует его преимущество по сравнению с нейросетями.\n\nСписок литературы\n\nСерия блог-постов о градиентном бустинге от Terence Parr and Jeremy Howard\n\nРаздел документации sklearn с теоретическими выкладками для градиентного бустинга'), Document(metadata={'source': '/Users/eugenekillevsky/PycharmProjects/LLM-Interviewer/data/RAG/ml-handbook-master/chapters/linear_models/intro.md'}, page_content='title: Линейные модели author: filipp_sinicin, evgenii_sokolov\n\nМы начнем с самых простых и понятных моделей машинного обучения: линейных. В этой главе мы разберёмся, что это такое, почему они работают и в каких случаях их стоит использовать. Так как это первый класс моделей, с которым вы столкнётесь, мы постараемся подробно проговорить все важные моменты. Заодно объясним, как работает машинное обучение, на сравнительно простых примерах.\n\nПочему модели линейные?\n\nПредставьте, что у вас есть множество объектов $\\mathbb{X}$, а вы хотели бы каждому объекту сопоставить какое-то значение. К примеру, у вас есть набор операций по банковской карте, а вы бы хотели, понять, какие из этих операций сделали мошенники. Если вы разделите все операции на два класса и нулём обозначите законные действия, а единицей мошеннические, то у вас получится простейшая задача классификации. Представьте другую ситуацию: у вас есть данные геологоразведки, по которым вы хотели бы оценить перспективы разных месторождений. В данном случае по набору геологических данных ваша модель будет, к примеру, оценивать потенциальную годовую доходность шахты. Это пример задачи регрессии. Числа, которым мы хотим сопоставить объекты из нашего множества иногда называют таргетами (от английского target).\n\nТаким образом, задачи классификации и регрессии можно сформулировать как поиск отображения из множества объектов $\\mathbb{X}$ в множество возможных таргетов.\n\nМатематически задачи можно описать так: - классификация: $$\\mathbb{X} \\to {0, 1, \\ldots, K}$$, где $0, \\ldots, K$ – номера классов, - регрессия: $\\mathbb{X} \\to \\mathbb{R}$.\n\nОчевидно, что просто сопоставить какие-то объекты каким-то числам — дело довольно бессмысленное. Мы же хотим быстро обнаруживать мошенников или принимать решение, где строить шахту. Значит нам нужен какой-то критерий качества. Мы бы хотели найти такое отображение, которое лучше всего приближает истинное соответствие между объектами и таргетами. Что значит <<лучше всего>> – вопрос сложный. Мы к нему будем много раз возвращаться. Однако, есть более простой вопрос: среди каких отображений мы будем искать самое лучшее? Возможных отображений может быть много, но мы можем упростить себе задачу и договориться, что хотим искать решение только в каком-то заранее заданном параметризированном семействе функций. Вся эта глава будет посвящена самому простому такому семейству — линейным функциям вида\n\n$$ y = w_1 x_1 + \\ldots + w_D x_D + w_0, $$\n\nгде $y$ – целевая переменная (таргет), $(x_1, \\ldots, x_D)$ – вектор, соответствующий объекту выборки (вектор признаков), а $w_1, \\ldots, w_D, w_0$ – параметры модели. Признаки ещё называют фичами (от английского features). Вектор $w = (w_1,\\ldots,w_D)$ часто называют вектором весов, так как на предсказание модели можно смотреть как на взвешенную сумму признаков объекта, а число $w_0$ – свободным коэффициентом, или сдвигом (bias). Более компактно линейную модель можно записать в виде\n\n$$y = \\langle x, w\\rangle + w_0$$\n\nТеперь, когда мы выбрали семейство функций, в котором будем искать решение, задача стала существенно проще. Мы теперь ищем не какое-то абстрактное отображение, а конкретный вектор $(w_0,w_1,\\ldots,w_D)\\in\\mathbb{R}^{D+1}$.\n\nЗамечание. Чтобы применять линейную модель, нужно, чтобы каждый объект уже был представлен вектором численных признаков $x_1,\\ldots,x_D$. Конечно, просто текст или граф в линейную модель не положить, придётся сначала придумать для него численные фичи. Модель называют линейной, если она является линейной по этим численным признакам.\n\nРазберёмся, как будет работать такая модель в случае, если $$D = 1$$. То есть у наших объектов есть ровно один численный признак, по которому они отличаются. Теперь наша линейная модель будет выглядеть совсем просто: $$y = w_1 x_1 + w_0$$. Для задачи регрессии мы теперь пытаемся приблизить значение игрек какой-то линейной функцией от переменной икс. А что будет значить линейность для задачи классификации? Давайте вспомним про пример с поиском мошеннических транзакций по каратам. Допустим, нам известна ровно одна численная переменная — объём транзакции. Для бинарной классификации транзакций на законные и потенциально мошеннические мы будем искать так называемое разделяющее правило: там, где значение функции положительно, мы будем предсказывать один класс, где отрицательно – другой. В нашем примере простейшим правилом будет какое-то пороговое значение объёма транзакций, после которого есть смысл пометить транзакцию как подозрительную.\n\n{: .left}\n\nВ случае более высоких размерностей вместо прямой будет гиперплоскость с аналогичным смыслом.\n\nВопрос на подумать. Если вы посмотрите содержание учебника, то не найдёте в нём ни <<полиномиальных>> моделей, ни каких-нибудь <<логарифмических>>, хотя, казалось бы, зависимости бывают довольно сложными. Почему так?\n\n{% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details="Линейные зависимости не так просты, как кажется. Пусть мы решаем задачу регрессии. Если мы подозреваем, что целевая переменная $$y$$ не выражается через $$x_1, x_2$$ как линейная функция, а зависит ещё от логарифма $$x_1$$ и ещё как-нибудь от того, разные ли знаки у признаков, то мы можем ввести дополнительные слагаемые в нашу линейную зависимость, просто объявим эти слагаемые новыми переменными, и добавив перед ними соответствующие регрессионные коэффициенты\n\n$$y \\approx w_1 x_1 + w_2 x_2 + w_3\\log{x_1} + w_4\\text{sgn}(x_1x_2) + w_0,$$\n\nи в итоге из двумерной нелинейной задачи мы получили четырёхмерную линейную регрессию." %}\n\nВопрос на подумать. А как быть, если одна из фичей является категориальной, то есть принимает значения из (обычно конечного числа) значений, не являющихся числами? Например, это может быть время года, уровень образования, марка машины и так далее. Как правило, с такими значениями невозможно производить арифметические операции или же результаты их применения не имеют смысла.\n\n{% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details=" В линейную модель можно подать только численные признаки, так что категориальную фичу придётся как-то закодировать. Рассмотрим для примера вот такой датасет\n\n{: .left}\n\nЗдесь два категориальных признака – pet_type и color. Первый принимает четыре различных значения, второй – пять.\n\nСамый простой способ – использовать one-hot кодирование (one-hot encoding). Пусть исходный признак мог принимать $M$ значений $c_1,\\ldots, c_M$. Давайте заменим категориальный признак на $M$ признаков, которые принимают значения $0$ и $1$: $i$-й будет отвечать на вопрос <<принимает ли признак значение $c_i$?>>. Иными словами, вместо ячейки со значением $c_i$ у объекта появляется строка нулей и единиц, в которой единица стоит только на $i$-м месте.\n\nВ нашем примере получится вот такая табличка:\n\n{: .left}\n\nМожно было бы на этом остановится, но добавленные признаки обладают одним неприятным свойством: в каждом из них ровно одна единица, так что сумма соответствующих столбцов равна столбцу из единиц. А это уже плохо. Представьте, что у нас есть линейная модель\n\n$$y \\sim w_1x_1 + \\ldots + w_{D-1}x_{d-1} + w_{c_1}x_{c_1} + \\ldots + w_{c_M}x_{c_M} + w_0$$\n\nПреобразуем немного правую часть:\n\n$$y\\sim w_1x_1 + \\ldots + w_{D-1}x_{d-1} + \\underbrace{(w_{c_1} - w_{c_N})}{=:w\'{c_1}}x_{c_1} + \\ldots + \\underbrace{(w_{c_{M-1}} - w_{c_N})}{=:w\'{C_{N-1}}}x_{c_{M-1}} + w_{c_N}\\underbrace{(x_{c_1} + \\ldots + x_{c_M})}_{=1} + w_0 = $$\n\n$$ = w_1x_1 + \\ldots + w_{D-1}x_{d-1} + w\'{c_1}x{c_1} + \\ldots + w\'{c{M-1}}x_{c_{M-1}} + (w_{c_N} + w_0){=w\'{0}}$$\n\nКак видим, от одного из новых признаков можно избавиться, не меняя модель. Больше того, это стоит сделать, потому что наличие <<лишних>> признаков ведёт к переобучению или вовсе ломает модель – подробнее об этом мы поговорим в разделе про регуляризацию. Поэтому при использовании one-hot-encoding обычно выкидывают признак, соответствующий одному из значений. Например, в нашем примере итоговая матрица объекты-признаки будет иметь вид:\n\n{: .left}\n\nКонечно, one-hot кодирование – это самый наивный способ работы с категориальными признаками, и для более сложных фичей или фичей с большим количеством значений оно плохо подходит. С рядом более продвинутых техник вы познакомитесь в разделе про обучение представлений. " %}\n\nПомимо простоты, у линейных моделей есть несколько других достоинств. К примеру, мы можем достаточно легко судить, как влияют на результат те или иные признаки. Скажем, если вес $w_i$ положителен, то с ростом $i$-го признака таргет в случае регрессии будет увеличиваться, а в случае классификации наш выбор будет сдвигаться в пользу одного из классов. Значение весов тоже имеет прозрачную интерпретацию: чем вес $w_i$ больше, тем <<важнее>> $i$-й признак для итогового предсказания. То есть, если вы построили линейную модель, вы неплохо можете объяснить заказчику те или иные её результаты. Это качество моделей называют интерпретируемостью. Оно особенно ценится в индустриальных задачах, цена ошибки в которых высока. Если от работы вашей модели может зависеть жизнь человека, то очень важно понимать, как модель принимает те или иные решения и какими принципами руководствуется. При этом, не все методы машинного обучения хорошо интерпретируемы, к примеру, поведение искусственных нейронных сетей или градиентного бустинга интерпретировать довольно сложно.\n\nВ то же время слепо доверять весам линейных моделей тоже не стоит по целому ряду причин:\n\nЛинейные модели всё-таки довольно узкий класс функций, они неплохо работают для небольших датасетов и простых задач. Однако, если вы решаете линейной моделью более сложную задачу, то вам, скорее всего, придётся выдумывать дополнительные признаки, являющиеся сложными функциями от исходных. Поиск таких дополнительных признаков называется feature engineering, технически он устроен примерно так, как мы описали в вопросе про "полиномиальные модели". Вот только поиском таких искусственных фичей можно сильно увлечься, так что осмысленность интерпретации будет сильно зависеть от здравого смысла эксперта, строившего модель.\n\nЕсли между признаками есть приближённая линейная зависимость, коэффициенты в линейной модели могут совершенно потерять физический смысл (об этой проблеме и о том, как с ней бороться, мы поговорим дальше, когда будем обсуждать регуляризацию).\n\nОсобенно осторожно стоит верить в утверждения вида <<этот коэффициент маленький, значит, этот признак не важен>>. Во-первых, всё зависит от масштаба признака: вдруг коэффициент мал, чтобы скомпенсировать его. Во-вторых, зависимость действительно может быть слабой, но кто знает, в какой ситуации она окажется важна. Такие решения принимаются на основе данных, например, путём проверки статистического критерия (об этом мы коротко упомянем в разделе по вероятностные модели).\n\nКонкретные значения весов могут меняться в зависимости от обучающей выборки, хотя с ростом её размера они будут потихоньку сходиться к весам <<наилучшей>> линейной модели, которую можно было бы построить по всем-всем-всем данным на свете.\n\nОбсудив немного общие свойства линейных моделей, перейдём к тому, как их всё-таки обучать. Сначала разберёмся с регрессией, а затем настанет черёд классификации.\n\nЛинейная регрессия и метод наименьших квадратов (МНК)\n\nМы начнём с использования линейных моделей для решения задачи регрессии. Простейшим примером постановки задачи линейной регрессии является метод наименьших квадратов (Ordinary least squares).\n\nПусть у нас задан датасет $(X, y)$, где $$y=(y_i){i=1}^N \\in \\mathbb{R}^N$$ – вектор значений целевой переменной, а $$X=(x_i){i = 1}^N \\in \\mathbb{R}^{N \\times D}, x_i \\in \\mathbb{R}^D$$ – матрица объекты-признаки, в которой $i$-я строка – это вектор признаков $i$-го объекта выборки. Мы хотим моделировать зависимость $$y_i$$ от $$x_i$$ как линейную функцию со свободным членом. Общий вид такой функции из $$\\mathbb{R}^D$$ в $$\\mathbb{R}$$ выглядит следующим образом:\n\n$$\\color{#348FEA}{f_w(x_i) = \\langle w, x_i \\rangle + w_0}$$\n\nСвободный член $$w_0$$ часто опускают, потому что такого же результата можно добиться, добавив ко всем $$x_i$$ признак, тождественно равный единице; тогда роль свободного члена будет играть соответствующий ему вес:\n\n$$\\begin{pmatrix}x_{i1} & \\ldots & x_{iD} \\end{pmatrix}\\cdot\\begin{pmatrix}w_1\\ \\vdots \\ w_D\\end{pmatrix} + w_0 = \\begin{pmatrix}1 & x_{i1} & \\ldots & x_{iD} \\end{pmatrix}\\cdot\\begin{pmatrix}w_0 \\ w_1\\ \\vdots \\ w_D \\end{pmatrix}$$\n\nПоскольку это сильно упрощает запись, в дальнейшем мы будем считать, что это уже сделано и зависимость имеет вид просто $$f_w(x_i) = \\langle w, x_i \\rangle$$.\n\nСведение к задаче оптимизации\n\nМы хотим, чтобы на нашем датасете (то есть на парах $$(x_i, y_i)$$ из обучающей выборки) функция $$f_w$$ как можно лучше приближала нашу зависимость.\n\n{: .left}\n\nДля того, чтобы чётко сформулировать задачу, нам осталось только одно: на математическом языке выразить желание <<приблизить $$f_w(x)$$ к $$y$$>>. Говоря простым языком, мы должны научиться измерять качество модели и минимизировать её ошибку, как-то меняя обучаемые параметры. В нашем примере обучаемые параметры — это веса $$w$$. Функция, оценивающая то, как часто модель ошибается, традиционно называется функцией потерь, функционалом качества или просто лоссом (loss function). Важно, чтобы её было легко оптимизировать: скажем, гладкая функция потерь – это хорошо, а кусочно постоянная – просто ужасно.\n\nФункции потерь бывают разными. От их выбора зависит то, насколько задачу в дальнейшем легко решать, и то, в каком смысле у нас получится приблизить предсказание модели к целевым значениям. Интуитивно понятно, что для нашей текущей задачи нам нужно взять вектор $$y$$ и вектор предсказаний модели и как-то сравнить, насколько они похожи. Так как эти вектора <<живут>> в одном векторном пространстве, расстояние между ними вполне может быть функцией потерь. Более того, положительная непрерывная функция от этого расстояния тоже подойдёт в качестве функции потерь. При этом способов задать расстояние между векторами тоже довольно много. От всего этого разнообразия глаза разбегаются, но мы обязательно поговорим про это позже. Сейчас давайте в качестве лосса возьмём квадрат $$L^2$$-нормы вектора разницы предсказаний модели и $$y$$. Во-первых, как мы увидим дальше, так задачу будет нетрудно решить, а во-вторых, у этого лосса есть ещё несколько дополнительных свойств:\n\n$$L^2$$-норма разницы – это евклидово расстояние $$|y - f_w(x)|_2$$ между вектором таргетов и вектором ответов модели, то есть мы их приближаем в смысле самого простого и понятного <<расстояния>>.\n\nКак мы увидим в разделе про вероятностные модели, с точки зрения статистики это соответствует гипотезе о том, что наши данные состоят из линейного <<сигнала>> и нормально распределенного <<шума>>.\n\nТак вот, наша функция потерь выглядит так:\n\n$$L(f, X, y) = |y - f(X)|_2^2 = $$\n\n$$= |y - Xw|2^2 = \\sum{i=1}^N(y_i - \\langle x_i, w \\rangle)^2$$\n\nТакой функционал ошибки не очень хорош для сравнения поведения моделей на выборках разного размера. Представьте, что вы хотите понять, насколько качество модели на тестовой выборке из $2500$ объектов хуже, чем на обучающей из $5000$ объектов. Вы измерили $$L^2$$-норму ошибки и получили в одном случае $300$, а в другом $500$. Эти числа не очень интерпретируемы. Гораздо лучше посмотреть на среднеквадратичное отклонение\n\n$$L(f, X, y) = \\frac1N\\sum_{i=1}^N(y_i - \\langle x_i, w \\rangle)^2$$\n\nПо этой метрике на тестовой выборке получаем $$0,12$$, а на обучающей $$0,1$$.\n\nФункция потерь $$\\frac1N\\sum_{i=1}^N(y_i - \\langle x_i, w \\rangle)^2$$ называется Mean Squared Error, MSE или среднеквадратическим отклонением. Разница с $$L^2$$-нормой чисто косметическая, на алгоритм решения задачи она не влияет:\n\n$$\\color{#348FEA}{\\text{MSE}(f, X, y) = \\frac{1}{N}|y - X w|_2^2}$$\n\nВ самом широком смысле, функции работают с объектами множеств: берут какой-то входящий объект из одного множества и выдают на выходе соответствующий ему объект из другого. Если мы имеем дело с отображением, которое на вход принимает функции, а на выходе выдаёт число, то такое отображение называют функционалом. Если вы посмотрите на нашу функцию потерь, то увидите, что это именно функционал. Для каждой конкретной линейной функции, которую задают веса $$w_i$$, мы получаем число, которое оценивает, насколько точно эта функция приближает наши значения $$y$$. Чем меньше это число, тем точнее наше решение, значит для того, чтобы найти лучшую модель, этот функционал нам надо минимизировать по $$w$$:\n\n$$\\color{#348FEA}{|y - Xw|_2^2 \\longrightarrow \\min_w}$$\n\nЭту задачу можно решать разными способами. В этой главе мы сначала решим эту задачу аналитически, а потом приближенно. Сравнение двух этих решений позволит нам проиллюстрировать преимущества того подхода, которому посвящена эта книга. На наш взгляд, это самый простой способ "на пальцах" показать суть машинного обучения.\n\nМНК: точный аналитический метод\n\nТочку минимума можно найти разными способами. Если вам интересно аналитическое решение, вы можете найти его в главе про матричные дифференцирования (раздел <<Примеры вычисления производных сложных функций>>). Здесь же мы воспользуемся геометрическим подходом.\n\nПусть $$x^{(1)},\\ldots,x^{(D)}$$ – столбцы матрицы $$X$$, то есть столбцы признаков. Тогда\n\n$$Xw = w_1x^{(1)}+\\ldots+w_Dx^{(D)},$$\n\nи задачу регрессии можно сформулировать следующим образом: найти линейную комбинацию столбцов $$x^{(1)},\\ldots,x^{(D)}$$, которая наилучшим способом приближает столбец $$y$$ по евклидовой норме – то есть найти проекцию вектора $$y$$ на подпространство, образованное векторами $$x^{(1)},\\ldots,x^{(D)}$$.\n\nРазложим $$y = y_{\\parallel} + y_{\\perp}$$, где $$y_{\\parallel} = Xw$$ – та самая проекция, а $$y_{\\perp}$$ – ортогональная составляющая, то есть $$y_{\\perp} = y - Xw\\perp x^{(1)},\\ldots,x^{(D)}$$. Как это можно выразить в матричном виде? Оказывается, очень просто:\n\n$$X^T(y - Xw) = 0$$\n\nВ самом деле, каждый элемент столбца $$X^T(y - Xw)$$ – это скалярное произведение строки $$X^T$$ (=столбца $$X$$ = одного из $$x^{(i)}$$) на $$y - Xw$$. Из уравнения $$X^T(y - Xw) = 0$$ уже очень легко выразить $$w$$:\n\n$$w = (X^TX)^{-1}X^Ty$$\n\nВопрос на подумать Для вычисления $w_{\\ast}$ нам приходится обращать (квадратную) матрицу $X^TX$, что возможно, только если она невырожденна. Что это значит с точки зрения анализа данных? Почему мы верим, что это выполняется во всех разумных ситуациях?\n\n{% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details="Как известно из линейной алгебры, для вещественной матрицы $X$ ранги матриц $X$ и $X^TX$ совпадают. Матрица $X^TX$ невырожденна тогда и только тогда, когда её ранг равен числу её столбцов, что равно числу столбцов матрицы $X$. Иными словами, формула регрессии поломается, только если столбцы матрицы $X$ линейно зависимы. Столбцы матрицы $X$ – это признаки. А если наши признаки линейно зависимы, то, наверное, что-то идёт не так и мы должны выкинуть часть из них, чтобы остались только линейно независимые.\n\nДругое дело, что зачастую признаки могут быть приближённо линейно зависимы, особенно если их много. Тогда матрица $X^TX$ будет близка к вырожденной, и это, как мы дальше увидим, будет вести к разным, в том числе вычислительным проблемам." %}\n\nВычислительная сложность аналитического решения – $$O(ND^2 + D^3)$$, где, $N$ – длина выборки, $D$ – число признаков у одного объекта. Слагаемое $$ND^2$$ отвечает за сложность перемножения матриц $$X^T$$ и $$X$$, а слагаемое $$D^3$$ – за сложность обращения их произведения. Перемножать матрицы $$(X^TX)^{-1}$$ и $$X^T$$ не стоит. Гораздо лучше сначала умножить $$y$$ на $$X^T$$, а затем полученный вектор на $$(X^TX)^{-1}$$: так будет быстрее и, кроме того, не нужно будет хранить матрицу $$(X^TX)^{-1}X^T$$.\n\nВычисление можно ускорить, используя продвинутые алгоритмы перемножения матриц или итерационные методы поиска обратной матрицы.\n\nПроблемы <<точного>> решения\n\nЗаметим, что для получения ответа нам нужно обратить матрицу $$X^TX$$. Это создает множество проблем: 1. Основная проблема в обращении матрицы — это то, что вычислительно обращать большие матрицы дело сложное, а мы бы хотели работать с датасетами, в которых у нас могут быть миллионы точек, 2. Матрица $$X^TX$$, хотя почти всегда обратима в разумных задачах машинного обучения, зачастую плохо обусловлена. Особенно если признаков много, между ними может появляться приближённая линейная зависимость, которую мы можем упустить на этапе формулировки задачи. В подобных случаях погрешность нахождения $w$ будет зависеть от квадрата числа обусловленности матрицы $X$, что очень плохо. Это делает полученное таким образом решение численно неустойчивым: малые возмущения $$y$$ могут приводить к катастрофическим изменениям $$w$$.\n\n{% include details.html summary="Пара слов про число обусловленности." details=" Пожертвовав математической строгостью, мы можем считать, что число обусловленности матрицы $X$ – это корень из отношения наибольшего и наименьшего из собственных чисел матрицы $X^TX$. Грубо говоря, оно показывает, насколько разного масштаба бывают собственные значения $X^TX$. Если рассмотреть $L^2$-норму ошибки предсказания, как функцию от $w$, то её линии уровня будут эллипсоидами, форма которых определяется квадратичной формой с матрицей $X^TX$ (проверьте это!). Таким образом, число обусловленности говорит о том, насколько вытянутыми являются эти эллипсоиды. " %}\n\n{% include details.html summary="Данные проблемы не являются поводом выбросить решение на помойку. Существует как минимум два способа улучшить его численные свойства, однако если вы не знаете про сингулярное разложение, то лучше вернитесь сюда, когда узнаете." details=" 1. Построим $$QR$$-разложение матрицы $$X$$. Напомним, что это разложение, в котором матрица $$Q$$ ортогональна по столбцам (то есть её столбцы ортогональны и имеют длину 1; в частности, $$Q^TQ=E$$), а $$R$$ квадратная и верхнетреугольная. Подставив его в формулу, получим\n\n  $$w = ((QR)^TQR)^{-1}(QR)^T y = (R^T\\underbrace{Q^TQ}_{=E}R)^{-1}R^TQ^Ty = R^{-1}R^{-T}R^TQ^Ty = R^{-1}Q^Ty$$\n\n  Отметим, что написать $$(R^TR)^{-1} = R^{-1}R^{-T}$$ мы имеем право благодаря тому, что $$R$$ квадратная. Полученная формула намного проще, обращение верхнетреугольной матрицы (=решение системы с верхнетреугольной левой частью) производится быстро и хорошо, погрешность вычисления $w$ будет зависеть просто от числа обусловленности матрицы $X$, а поскольку нахождение $QR$-разложения является достаточно стабильной операцией, мы получаем решение с более хорошими, чем у исходной формулы, численными свойствами.\n\nТакже можно использовать псевдообратную матрицу, построенную с помощью сингулярного разложения, о котором подробно написано в разделе про матричные разложения. А именно, пусть\n\n$$A = U\\underbrace{\\mathrm{diag}(\\sigma_1,\\ldots,\\sigma_r)}_{=\\Sigma}V^T$$\n\n– это усечённое сингулярное разложение, где $r$ – это ранг $A$. В таком случае диагональная матрица посередине является квадратной, $U$ и $V$ ортогональны по столбцам: $U^TU = E$, $V^TV = E$. Тогда\n\n$$w = (V\\Sigma \\underbrace{U^TU}_{=E}\\Sigma V^T)^{-1}V\\Sigma U^Ty$$\n\nЗаметим, что $$V\\Sigma^{-2}V^T\\cdot V\\Sigma^2V^T = E = V\\Sigma^2V^T\\cdot V\\Sigma^{-2}V^T$$, так что $$(V\\Sigma^2 V^T)^{-1} = V\\Sigma^{-2}V^T$$, откуда\n\n$$w = V\\Sigma^{-2}\\underbrace{V^TV}_{=E}V^T\\cdot V\\Sigma U^Ty = V\\Sigma^{-1}Uy$$\n\nХорошие численные свойства сингулярного разложения позволяют утверждать, что и это решение ведёт себя довольно неплохо.\n\nТем не менее, вычисление всё равно остаётся довольно долгим и будет по-прежнему страдать (хоть и не так сильно) в случае плохой обусловленности матрицы $$X$$. " %}\n\nПолностью вылечить проблемы мы не сможем, но никто и не обязывает нас останавливаться на <<точном>> решении (которое всё равно никогда не будет вполне точным). Поэтому ниже мы познакомим вас с совершенно другим методом.\n\nМНК: приближенный численный метод\n\nМинимизируемый функционал является гладким и выпуклым, а это значит, что можно эффективно искать точку его минимума с помощью итеративных градиентных методов. Более подробно вы можете прочитать о них в разделе про методы оптимизации, а здесь мы лишь коротко расскажем об одном самом базовом подходе.\n\nКак известно, градиент функции в точке направлен в сторону её наискорейшего роста, а антиградиент (противоположный градиенту вектор) в сторону наискорейшего убывания. То есть имея какое-то приближение оптимального значения параметра $$w$$, мы можем его улучшить, посчитав градиент функции потерь в точке и немного сдвинув вектор весов в направлении антиградиента:\n\n$$w_j \\mapsto w_j - \\alpha \\frac{d}{d{w_j}} L(f_w, X, y) $$\n\nгде $$\\alpha$$ – это параметр алгоритма (<<темп обучения>>), который контролирует величину шага в направлении антиградиента. Описанный алгоритм называется градиентным спуском.\n\nПосмотрим, как будет выглядеть градиентный спуск для функции потерь $L(f_w, X, y) = \\frac1N\\vert\\vert Xw - y\\vert\\vert^2$. Градиент квадрата евклидовой нормы мы уже считали; соответственно,\n\n$$ \\nabla_wL = \\frac2{N} X^T (Xw - y) $$\n\nСледовательно, стартовав из какого-то начального приближения, мы можем итеративно уменьшать значение функции, пока не сойдёмся (по крайней мере в теории) к минимуму (вообще говоря, локальному, но в данном случае глобальному).\n\nАлгоритм градиентного спуска\n\npython w = random_normal() # можно пробовать и другие виды инициализации repeat S times: # другой вариант: while abs(err) > tolerance f = X.dot(w) # посчитать предсказание err = f - y # посчитать ошибку grad = 2 * X.T.dot(err) / N # посчитать градиент w -= alpha * grad # обновить веса\n\nС теоретическими результатами о скорости и гарантиях сходимости градиентного спуска вы можете познакомиться в главе про методы оптимизации. Мы позволим себе лишь несколько общих замечаний:\n\nПоскольку задача выпуклая, выбор начальной точки влияет на скорость сходимости, но не настолько сильно, чтобы на практике нельзя было стартовать всегда из нуля или из любой другой приятной вам точки;\n\nЧисло обусловленности матрицы $X$ существенно влияет на скорость сходимости градиентного спуска: чем более вытянуты эллипсоиды уровня функции потерь, тем хуже;\n\nТемп обучения $\\alpha$ тоже сильно влияет на поведение градиентного спуска; вообще говоря, он является гиперпараметром алгоритма, и его, возможно, придётся подбирать отдельно. Другими гиперпараметрами являются максимальное число итераций $S$ и/или порог tolerance.\n\n{% include details.html summary="Иллюстрация." details=" Рассмотрим три задачи регрессии, для которых матрица $X$ имеет соответственно маленькое, среднее и большое числа обусловленности. Будем строить для них модели вида $y=w_1x_1 + w_2x_2$. Раскрасим плоскость $(w_1, w_2)$ в соответствии со значениями $|X_{\\text{train}}w - y_{\\text{train}}|^2$. Тёмная область содержит минимум этой функции – оптимальное значение $w_{\\ast}$. Также запустим из из двух точек градиентный спуск с разными значениями темпа обучения $\\alpha$ и посмотрим, что получится:\n\n{: .left} Заголовки графиков (\\"Round\\", \\"Elliptic\\", \\"Stripe-like\\") относятся к форме линий уровня потерь (чем более они вытянуты, тем хуже обусловлена задача и тем хуже может вести себя градиентный спуск).\n\nИтог: при неудачном выборе $\\alpha$ алгоритм не сходится или идёт вразнос, а для плохо обусловленной задачи он сходится абы куда. " %}\n\nВычислительная сложность градиентного спуска – $O(NDS)$, где, как и выше, $N$ – длина выборки, $D$ – число признаков у одного объекта. Сравните с оценкой $$O(N^2D + D^3)$$ для <<наивного>> вычисления аналитического решения.\n\nСложность по памяти – $O(ND)$ на хранение выборки. В памяти мы держим и выборку, и градиент, но в большинстве реалистичных сценариев доминирует выборка.\n\nСтохастический градиентный спуск\n\nНа каждом шаге градиентного спуска нам требуется выполнить потенциально дорогую операцию вычисления градиента по всей выборке (сложность $$O(ND)$$). Возникает идея заменить градиент его оценкой на подвыборке (в английской литературе такую подвыборку обычно именуют batch или mini-batch; в русской разговорной терминологии тоже часто встречается слово батч или мини-батч).\n\nА именно, если функция потерь имеет вид суммы по отдельным парам объект-таргет\n\n$$L(w, X, y) = \\frac1N\\sum_{i=1}^NL(w, x_i, y_i),$$\n\nа градиент, соответственно, записывается в виде\n\n$$\\nabla_wL(w, X, y) = \\frac1N\\sum_{i=1}^N\\nabla_wL(w, x_i, y_i),$$\n\nто предлагается брать оценку\n\n$$\\nabla_wL(w, X, y) \\approx \\frac1B\\sum_{t=1}^B\\nabla_wL(w, x_{i_t}, y_{i_t})$$\n\nдля некоторого подмножества этих пар $(x_{i_t}, y_{i_t})_{t=1}^B$. Обратите внимание на множители $\\frac1N$ и $\\frac1B$ перед суммами. Почему они нужны? Полный градиент $\\nabla_wL(w, X, y)$ можно воспринимать как среднее градиентов по всем объектам, то есть как оценку матожидания $\\mathbb{E}\\nabla_wL(w, x, y)$; тогда, конечно, оценка матожидания по меньшей подвыборке тоже будет иметь вид среднего градиентов по объектам этой подвыборки.\n\nКак делить выборку на батчи? Ясно, что можно было бы случайным образом сэмплировать их из полного датасета, но даже если использовать быстрый алгоритм вроде резервуарного сэмплирования, сложность этой операции не самая оптимальная. Поэтому используют линейный проход по выборке (которую перед этим лучше всё-таки случайным образом перемешать). Давайте введём ещё один параметр нашего алгоритма: размер батча, который мы обозначим $$B$$. Теперь на $$B$$ очередных примерах вычислим градиент и обновим веса модели. При этом вместо количества шагов алгоритма обычно задают количество эпох $$E$$. Это ещё один гиперпараметр. Одна эпоха – это один полный проход нашего сэмплера по выборке. Заметим, что если выборка очень большая, а модель компактная, то даже первый проход бывает можно не заканчивать.\n\nАлгоритм: ```python w = normal(0, 1) repeat E times: for i = B, i <= n, i += B X_batch = X[i-B : i] y_batch = y[i-B : i] f = X_batch.dot(w) # посчитать предсказание err = f - y_batch # посчитать ошибку grad = 2 * X_batch.T.dot(err) / B # посчитать градиент w -= alpha * grad\n\n```\n\nСложность по времени – $$O(NDE)$$. На первый взгляд, она такая же, как и у обычного градиентного спуска, но заметим, что мы сделали в $$N / B$$ раз больше шагов, то есть веса модели претерпели намного больше обновлений.\n\nСложность по памяти можно довести до $$O(BD)$$: ведь теперь всю выборку не надо держать в памяти, а достаточно загружать лишь текущий батч (а остальная выборка может лежать на диске, что удобно, так как в реальности задачи, в которых выборка целиком не влезает в оперативную память, встречаются сплошь и рядом). Заметим, впрочем, что при этом лучше бы $$B$$ взять побольше: ведь чтение с диска – намного более затратная по времени операция, чем чтение из оперативной памяти.\n\nВ целом, разницу между алгоритмами можно представлять как-то так: {: .left}\n\nШаги стохастического градиентного спуска заметно более шумные, но считать их получается значительно быстрее. В итоге они тоже сходятся к оптимальному значению из-за того, что матожидание оценки градиента на батче равно самому градиенту. По крайней мере, сходимость можно получить при хорошо подобранных коэффициентах темпа обучения в случае выпуклого функционала качества. Подробнее мы об этом поговорим в главе про оптимизацию. Для сложных моделей и лоссов стохастический градиентный спуск может сходиться плохо или застревать в локальных минимумах, поэтому придумано множество его улучшений. О некоторых из них также рассказано в главе про оптимизацию.\n\nСуществует определённая терминологическая путаница, иногда стохастическим градиентным спуском называют версию алогоритма, в которой размер батча равен единице (то есть максимально шумная и быстрая версия алгоритма), а версии с бОльшим размером батча называют batch gradient descent. В книгах, которые, возможно, старше вас, такая процедура иногда ещё называется incremental gradient descent. Это не очень принципиально, но вы будьте готовы, если что.\n\nВопрос на подумать. Вообще говоря, если объём данных не слишком велик и позволяет это сделать, объекты лучше случайным образом перемешивать перед тем, как подавать их в алгоритм стохастического градиентного спуска. Как вам кажется, почему?\n\nТакже можно использовать различные стратегии отбора объектов. Например, чаще брать объекты, на которых ошибка больше. Какие ещё стратегии вы могли бы придумать?\n\n{% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details=" Легко представить себе ситуацию, в которой объекты как-нибудь неудачно упорядочены, скажем, по возрастанию таргета. Тогда модель будет попеременно то запоминать, что все таргеты маленькие, то – что все таргеты большие. Это может и не повлиять на качество итоговой модели, но может привести и к довольно печальным последствиям. И вообще, чем более разнообразные батчи модель увидит в процессе обучения, тем лучше.\n\nСтратегий можно придумать много. Например, не брать объекты, на которых ошибка слишком большая (возможно, это выбросы – зачем на них учиться), или вообще не брать те, на которых ошибка достаточно мала (они <<ничему не учат>>). Рекомендуем, впрочем, прибегать к этим эвристикам, только если вы понимаете, зачем они вам нужны и почему есть надежда, что они помогут. " %}\n\nНеградиентные методы\n\nПосле прочтения этой главы у вас может сложиться ощущение, что приближённые способы решения ML задач и градиентные методы – это одно и тоже, но вы будете правы в этом только на 98%. В принципе, существуют и другие способы численно решать эти задачи, но в общем случае они работают гораздо хуже, чем градиентный спуск, и не обладают таким хорошим теоретическим обоснованием. Мы не будем рассказывать про них подробно, но можете на досуге почитать, скажем, про Stepwise regression, Orthogonal matching pursuit или LARS. У LARS, кстати, есть довольно интересное свойство: он может эффективно работать на выборках, в которых число признаков больше числа примеров. С алгоритмом LARS вы можете познакомиться в главе про оптимизацию.\n\nРегуляризация\n\nВсегда ли решение задачи регрессии единственно? Вообще говоря, нет. Так, если в выборке два признака будут линейно зависимы (и следовательно, ранг матрицы будет меньше $$D$$), то гарантировано найдётся такой вектор весов $$\\nu$$ что $$\\langle\\nu, x_i\\rangle = 0\\ \\ \\forall x_i$$. В этом случае, если какой-то $$w$$ является решением оптимизационной задачи, то и $$w + \\alpha \\nu $$ тоже является решением для любого $$\\alpha$$. То есть решение не только не обязано быть уникальным, так ещё может быть сколь угодно большим по модулю. Это создаёт вычислительные трудности. Малые погрешности признаков сильно возрастают при предсказании ответа, а в градиентном спуске накапливается погрешность из-за операций со слишком большими числами.\n\nКонечно, в жизни редко бывает так, что признаки строго линейно зависимы, а вот быть приближённо линейно зависимыми они вполне могут быть. Такая ситуация называется мультиколлинеарностью. В этом случае у нас, всё равно, возникают проблемы, близкие к описанным выше. Дело в том, что $$X\\nu\\sim 0$$ для вектора $\\nu$, состоящего из коэффициентов приближённой линейной зависимости, и, соответственно, $$X^TX\\nu\\approx 0$$, то есть матрица $$X^TX$$ снова будет близка к вырожденной. Как и любая симметричная матрица, она диагонализуется в некотором ортонормированном базисе, и некоторые из собственных значений $$\\lambda_i$$ близки к нулю. Если вектор $$X^Ty$$ в выражении $$(X^TX)^{-1}X^Ty$$ будет близким к соответствующему собственному вектору, то он будет умножаться на $$1 /{\\lambda_i}$$, что опять же приведёт к появлению у $$w$$ очень больших по модулю компонент (при этом $$w$$ ещё и будет вычислен с большой погрешностью из-за деления на маленькое число). И, конечно же, все ошибки и весь шум, которые имелись в матрице $$X$$ при вычислении $$y\\sim Xw$$ будут умножаться на эти большие и неточные числа и возрастать во много-много раз, что приведёт к проблемам, от которых нас не спасёт никакое сингулярное разложение.\n\nВажно ещё отметить, что в случае, когда несколько признаков линейно зависимы, веса $w_i$ при них теряют физический смысл. Может даже оказаться, что вес признака, с ростом которого таргет, казалось бы, должен увеличиваться, станет отрицательным. Это делает модель не только неточной, но и принципиально не интерпретируемой. Вообще, неадекватность знаков или величины весов – хорошее указание на мультиколлинеарность.\n\nДля того, чтобы справиться с этой проблемой, задачу обычно регуляризуют, то есть добавляют к ней дополнительное ограничение на вектор весов. Это ограничение можно, как и исходный лосс, задавать по-разному, но, как правило, ничего сложнее, чем $$L^1$$- и $$L^2$$-нормы, не требуется.\n\nВместо исходной задачи теперь предлагается решить такую:\n\n$$\\color{#348FEA}{\\min_w L(f, X, y) = \\min_w(|X w - y|_2^2 + \\lambda |w|^k_k )}$$\n\n$$\\lambda$$ – это очередной параметр, а $$|w|^k_k $$ – это один из двух вариантов:\n\n$$\\color{#348FEA}{|w|^2_2 = w^2_1 + \\ldots + w^2_D}$$\n\nили\n\n$$\\color{#348FEA}{|w|_1^1 = \\vert w_1 \\vert + \\ldots + \\vert w_D \\vert}$$\n\nДобавка $$\\lambda|w|^k_k$$ называется регуляризационным членом или регуляризатором, а число $\\lambda$ – коэффициентом регуляризации.\n\nКоэффициент $$\\lambda$$ является гиперпараметром модели и достаточно сильно влияет на качество итогового решения. Его подбирают по логарифмической шкале (скажем, от 1e-2 до 1e+2), используя для сравнения моделей с разными значениями $\\lambda$ дополнительную валидационную выборку. При этом качество модели с подобранным коэффициентом регуляризации уже проверяют на тестовой выборке, чтобы исключить переобучение. Более подробно о том, как нужно подбирать гиперпараметры, вы можете почитать в соответствующей главе.\n\nОтдельно надо договориться о том, что вес $w_0$, соответствующий отступу от начала координат (то есть признаку из всех единичек), мы регуляризовать не будем, потому что это не имеет смысла: если даже все значения $$y$$ равномерно велики, это не должно портить качество обучения. Обычно это не отображают в формулах, но если придираться к деталям, то стоило бы написать сумму по всем весам, кроме $$w_0$$:\n\n$$|w|^2_2 = \\sum_{\\color{red}{j=1}}^{D}w_j^2,$$\n\n$$|w|1 = \\sum{\\color{red}{j=1}}^{D} \\vert w_j \\vert$$\n\nВ случае $$L^2$$-регуляризации решение задачи изменяется не очень сильно. Например, продифференцировав новый лосс по $$w$$, легко получить, что <<точное>> решение имеет вид:\n\n$$w = (X^TX + \\lambda I)^{-1}X^Ty$$\n\nОтметим, что за этой формулой стоит и понятная численная интуиция: раз матрица $$X^TX$$ близка к вырожденной и обращать её сродни самоубийству. Мы лучше слегка исказим её добавкой $$\\lambda I$$, которая увеличит все собственные значения на $$\\lambda$$, отодвинув их от нуля. Да, аналитическое решение перестаёт быть <<точным>>, но за счёт снижения численных проблем мы получим более качественное решение, чем при использовании <<точной>> формулы.\n\nВ свою очередь, градиент функции потерь\n\n$$L(f_w, X, y) = |Xw - y|^2 + \\lambda|w|^2$$\n\nпо весам теперь выглядит так:\n\n$$ \\nabla_wL(f_w, X, y) = 2X^T(Xw - y) + 2\\lambda w $$\n\nПодставив этот градиент в алгоритм стохастического градиентного спуска, мы получаем обновлённую версию приближенного алгоритма, отличающуюся от старой только наличием дополнительного слагаемого.\n\nВопрос на подумать. Рассмотрим стохастический градиентный спуск для $L^2$-регуляризованной линейной регрессии с батчами размера $$1$$. Выберите правильный вариант шага SGD:\n\n(а) $$w_i\\mapsto w_i - 2\\alpha(\\langle w, x_j\\rangle - y_j)x_{ji} - \\frac{2\\alpha\\lambda}N w_i,\\quad i=1,\\ldots,D$$;\n\n(б) $$w_i\\mapsto w_i - 2\\alpha(\\langle w, x_j\\rangle - y_j)x_{ji} - 2\\alpha\\lambda w_i,\\quad i=1,\\ldots,D$$;\n\n(в) $$w_i\\mapsto w_i - 2\\alpha(\\langle w, x_j\\rangle - y_j)x_{ji} - 2\\lambda N w_i,\\quad i=1,\\ldots D$$.\n\n{% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details="Не регуляризованная функция потерь имеет вид $$\\mathcal{L}(X, y, w) = \\frac1N\\sum_{i=1}^N\\mathcal{L}(x_i, y_i, w)$$, и её можно воспринимать, как оценку по выборке $$(x_i, y_i)_{i=1}^N$$ идеальной функции потерь\n\n$$\\mathcal{L}(w) = \\mathbb{E}_{x, y}\\mathcal{L}(x, y, w)$$\n\nРегуляризационный член не зависит от выборки и добавляется отдельно:\n\n$$\\mathcal{L}{\\text{reg}}(w) = \\mathbb{E}{x, y}\\mathcal{L}(x, y, w) + \\lambda|w|^2$$\n\nСоответственно, идеальный градиент регуляризованной функции потерь имеет вид\n\n$$\\nabla_w\\mathcal{L}{\\text{reg}}(w) = \\mathbb{E}{x, y}\\nabla_w\\mathcal{L}(x, y, w) + 2\\lambda w,$$\n\nГрадиент по батчу – это тоже оценка градиента идеальной функции потерь, только не на выборке $$(X, y)$$, а на батче $$(x_{t_i}, y_{t_i})_{i=1}^B$$ размера $$B$$. Он будет выглядеть так:\n\n$$\\nabla_w\\mathcal{L}{\\text{reg}}(w) = \\frac1B\\sum{i=1}^B\\nabla_w\\mathcal{L}(x_{t_i}, y_{t_i}, w) + 2\\lambda w.$$\n\nКак видите, коэффициентов, связанных с числом объектов в батче или в исходной выборке, во втором слагаемом нет. Так что верным является второй вариант. Кстати, обратите внимание, что в третьем ещё и нет коэффициента $\\alpha$ перед производной регуляризационного слагаемого, это тоже ошибка.\n\n" %}\n\nВопрос на подумать. Распишите процедуру стохастического градиентного спуска для $L^1$-регуляризованной линейной регрессии. Как вам кажется, почему никого не волнует, что функция потерь, строго говоря, не дифференцируема? {% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details=" Распишем для случая батча размера 1:\n\n$$w_i\\mapsto w_i - \\alpha(\\langle w, x_j\\rangle - y_j)x_{ji} - \\frac{\\lambda}N\\cdot \\text{sign}(w_i),\\quad i=1,\\ldots,D$$\n\nФункция потерь не дифференцируема лишь в одной точке. Так как в машинном обучении чаще всего мы имеем дело с данными вероятностного характера, такая не влечёт каких-то особых проблем. Дело в том, что попадание прямо в ноль очень маловероятно из-за численных погрешностей в данных, так что мы можем просто доопределить производную в одной точке, а если даже пару раз попадём в неё за время обучения это не приведёт к каким-то значительным изменениям результатов. " %}\n\nОтметим, что $L^1$- и $L^2$-регуляризацию можно определять для любой функции потерь $L(w, X, y)$ (и не только в задаче регрессии, а и, например, в задаче классификации тоже). Новая функция потерь будет соответственно равна\n\n$$\\widetilde{L}(w, X, y) = L(w, X, y) + \\lambda|w|_1$$\n\nили\n\n$$\\widetilde{L}(w, X, y) = L(w, X, y) + \\lambda|w|_2^2$$\n\nРазреживание весов в $L^1$-регуляризации\n\n$$L^2$$-регуляризация работает прекрасно и используется в большинстве случаев, но есть одна полезная особенность $$L^1$$-регуляризации: её применение приводит к тому, что у признаков, которые не оказывают большого влияния на ответ, вес в результате оптимизации получается равным $0$. Это позволяет удобным образом удалять признаки, слабо влияющие на таргет. Кроме того, это даёт возможность автоматически избавляться от признаков, которые участвуют в соотношениях приближённой линейной зависимости, соответственно, спасает от проблем, связанных с мультиколлинеарностью, о которых мы писали выше.\n\nНе очень строгим, но довольно интуитивным образом это можно объяснить так: 1. В точке оптимума линии уровня регуляризационного члена касаются линий уровня основного лосса, потому что, во-первых, и те, и другие выпуклые, а во-вторых, если они пересекаются трансверсально, то существует более оптимальная точка:\n\n{: .left}\n\nЛинии уровня $$L^1$$-нормы – это $N$-мерные октаэдры. Точки их касания с линиями уровня лосса, скорее всего, лежат на грани размерности, меньшей $$N-1$$, то есть как раз в области, где часть координат равна нулю:\n\n{: .left}\n\nЗаметим, что данное построение говорит о том, как выглядит оптимальное решение задачи, но ничего не говорит о способе, которым это решение можно найти. На самом деле, найти такой оптимум непросто: у $$L^1$$ меры довольно плохая производная. Однако, способы есть. Можете на досуге прочитать, например, вот эту статью о том, как работало предсказание CTR в google в 2012 году. Там этой теме посвящается довольно много места. Кроме того, рекомендуем посмотреть про проксимальные методы в разделе этой книги про оптимизацию в ML.\n\nЗаметим также, что вообще-то оптимизация любой нормы $$L_x, \\ 0 < x \\leq 1$$, приведёт к появлению разреженных векторов весов, просто если c $$L^1$$ ещё хоть как-то можно работать, то с остальными всё будет ещё сложнее.\n\nДругие лоссы\n\nСтохастический градиентный спуск можно очевидным образом обобщить для решения задачи линейной регрессии с любой другой функцией потерь, не только квадратичной: ведь всё, что нам нужно от неё, – это чтобы у функции потерь был градиент. На практике это делают редко, но тем не менее рассмотрим ещё пару вариантов.\n\nMAE\n\nMean absolute error, абсолютная ошибка, появляется при замене $$L^2$$ нормы в MSE на $$L^1$$:\n\n$$\\color{#348FEA}{MAE(y, \\widehat{y}) = \\frac1N\\sum_{i=1}^N \\vert y_i - \\widehat{y}_i\\vert}$$\n\nМожно заметить, что в MAE по сравнению с MSE существенно меньший вклад в ошибку будут вносить примеры, сильно удалённые от ответов модели. Дело тут в том, что в MAE мы считаем модуль расстояния, а не квадрат, соответственно, вклад больших ошибок в MSE получается существенно больше. Такая функция потерь уместна в случаях, когда вы пытаетесь обучить регрессию на данных с большим количеством выбросов в таргете.\n\nИначе на эту разницу можно посмотреть так: MSE приближает матожидание условного распределения $$y \\mid x$$, а MAE – медиану.\n\nMAPE\n\nMean absolute percentage error, относительная ошибка.\n\n$$MAPE(y, \\widehat{y}) = \\frac1N\\sum_{i=1}^N \\left|\\frac{\\widehat{y}_i-y_i}{y_i}\\right|$$\n\nЧасто используется в задачах прогнозирования (например, погоды, загруженности дорог, кассовых сборов фильмов, цен), когда ответы могут быть различными по порядку величины, и при этом мы бы хотели верно угадать порядок, то есть мы не хотим штрафовать модель за предсказание 2000 вместо 1000 в разы сильней, чем за предсказание 2 вместо 1.\n\nВопрос на подумать. Кроме описанных выше в задаче линейной регрессии можно использовать и другие функции потерь, например, Huber loss:\n\n$$\\mathcal{L}(f, X, y) = \\sum_{i=1}^Nh_{\\delta}(y_i - \\langle w_i, x\\rangle),\\mbox{ где }h_{\\delta}(z) = \\begin{cases} \\frac12z^2,\\ |z|\\leqslant\\delta,\\ \\delta(|z| - \\frac12\\delta),\\ |z| > \\delta \\end{cases}$$\n\nЧисло $$\\delta$$ является гиперпараметром. Сложная формула при $$\\vert z\\vert > \\delta$$ нужна, чтобы функция $$h_{\\delta}(z)$$ была непрерывной. Попробуйте объяснить, зачем может быть нужна такая функция потерь. {% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details="Часто требования формулируют в духе <<функция потерь должна слабее штрафовать то-то и сильней штрафовать вот это>>. Например, $L^2$-регуляризованный лосс штрафует за большие по модулю веса. В данном случае можно заметить, что при небольших значениях ошибки берётся просто MSE, а при больших мы начинаем штрафовать нашу модель менее сурово. Например, это может быть полезно для того, чтобы выбросы не так сильно влияли на результат обучения. " %}\n\nЛинейная классификация\n\nТеперь давайте поговорим про задачу классификации. Для начала будем говорить про бинарную классификацию на два класса. Обобщить эту задачу до задачи классификации на $$K$$ классов не составит большого труда. Пусть теперь наши таргеты $$y$$ кодируют принадлежность к положительному или отрицательному классу, то есть принадлежность множеству $${-1,1}$$ (в этой главе договоримся именно так обозначать классы, хотя в жизни вам будут нередко встречаться и метки $${0,1}$$), а $$x$$ – по-прежнему векторы из $$\\mathbb{R}^D$$. Мы хотим обучить линейную модель так, чтобы плоскость, которую она задаёт, как можно лучше отделяла объекты одного класса от другого.\n\n{: .left}\n\nВ идеальной ситуации найдётся плоскость, которая разделит классы: положительный окажется с одной стороны от неё, а отрицательный с другой. Выборка, для которой это возможно, называется линейно разделимой. Увы, в реальной жизни такое встречается крайне редко.\n\nКак обучить линейную модель классификации, нам ещё предстоит понять, но уже ясно, что итоговое предсказание можно будет вычислить по формуле\n\n$$y = \\text{sign} \\langle w, x_i\\rangle$$\n\n{% include details.html summary="Почему бы не решать, как задачу регрессии?" details=" Мы можем попробовать предсказывать числа $$-1$$ и $$1$$, минимизируя для этого, например, MSE с последующим взятием знака, но ничего хорошего не получится. Во-первых, регрессия почти не штрафует за ошибки на объектах, которые лежат близко к разделяющей плоскости, но не с той стороны. Во вторых, ошибкой будет считаться предсказание, например, $$5$$ вместо $$1$$, хотя нам-то на самом деле не важно, какой у числа модуль, лишь бы знак был правильным. Если визуализировать такое решение, то проблемы тоже вполне заметны:\n\n{: .left}\n\nНам нужна прямая, которая разделяет эти точки, а не проходит через них! " %}\n\nСконструируем теперь функционал ошибки так, чтобы он вышеперечисленными проблемами не обладал. Мы хотим минимизировать число ошибок классификатора, то есть\n\n$$\\sum_i \\mathbb{I}[y_i \\neq sign \\langle w, x_i\\rangle]\\longrightarrow \\min_w$$\n\nДомножим обе части на $$y_i$$ и немного упростим\n\n$$\\sum_i \\mathbb{I}[y_i \\langle w, x_i\\rangle < 0]\\longrightarrow \\min_w$$\n\nВеличина $M = y_i \\langle w, x_i\\rangle$ называется отступом (margin) классификатора. Такая фунция потерь называется misclassification loss. Легко видеть, что\n\nотступ положителен, когда $sign(y_i) = sign(\\langle w, x_i\\rangle)$, то есть класс угадан верно; при этом чем больше отступ, тем больше расстояние от $x_i$ до разделяющей гиперплоскости, то есть «уверенность классификатора»;\n\nотступ отрицателен, когда $sign(y_i) \\ne sign(\\langle w, x_i\\rangle)$, то есть класс угадан неверно; при этом чем больше по модулю отступ, тем более сокрушительно ошибается классификатор.\n\nОт каждого из отступов мы вычисляем функцию\n\n$$F(M) = \\mathbb{I}[M < 0] = \\begin{cases}1,\\ M < 0,\\ 0,\\ M\\geqslant 0\\end{cases}$$\n\nОна кусочно-постоянная, и из-за этого всю сумму невозможно оптимизировать градиентными методами: ведь её производная равна нулю во всех точках, где она существует. Но мы можем мажорировать её какой-нибудь более гладкой функцией, и тогда задачу можно будет решить. Функции можно использовать разные, у них свои достоинства и недостатки, давайте рассмотрим несколько примеров:\n\n{: .left}\n\nВопрос на подумать. Допустим, мы как-то обучили классификатор, и подавляющее большинство отступов оказались отрицательными. Правда ли нас постигла катастрофа? {% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details=" Наверное, мы что-то сделали не так, но ситуацию можно локально выправить, если предсказывать классы, противоположные тем, которые выдаёт наша модель. " %}\n\nВопрос на подумать. Предположим, что у нас есть два классификатора с примерно одинаковыми и достаточно приемлемыми значениями интересующей нас метрики. При этом одна почти всегда выдаёт предсказания с большими по модулю отступами, а вторая – с относительно маленькими. Верно ли, что первая модель лучше, чем вторая? {% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details=" На первый взгляд кажется, что первая модель действительно лучше: ведь она предсказывает <<увереннее>>, но на самом деле всё не так однозначно: во многих случаях модель, которая умеет <<честно признать, что не очень уверена в ответе>>, может быть предпочтительней модели, которая врёт с той же непотопляемой уверенностью, что и говорит правду. В некоторых случаях лучше может оказаться модель, которая, по сути, просто отказывается от классификации на каких-то объектах. " %}\n\nОшибка перцептрона\n\nРеализуем простейшую идею: давайте считать отступы только на неправильно классифицированных объектах и учитывать их не бинарно, а линейно, пропорционально их размеру. Получается такая функция:\n\n$$F(M) = \\max(0, -M)$$\n\nДавайте запишем такой лосс с $$L^2$$-регуляризацией:\n\n$$L(w, x, y) = \\lambda\\vert\\vert w\\vert\\vert^2_2 + \\sum_i \\max(0, -y_i \\langle w, x_i\\rangle)$$\n\nНайдём градиент:\n\n$$ \\nabla_w L(w, x, y) = 2 \\lambda w + \\sum_i \\begin{cases} 0, & y_i \\langle w, x_i \\rangle > 0 \\ - y_i x_i, & y_i \\langle w, x_i \\rangle \\leq 0 \\end{cases} $$\n\nИмея аналитическую формулу для градиента, мы теперь можем так же, как и раньше, применить стохастический градиентный спуск, и задача будет решена.\n\nДанная функция потерь впервые была предложена для перцептрона Розенблатта, первой вычислительной модели нейросети, которая в итоге привела к появлению глубокого обучения.\n\nОна решает задачу линейной классификации, но у неё есть одна особенность: её решение не единственно и сильно зависит от начальных параметров. Например, все изображённые ниже классификаторы имеют одинаковый нулевой лосс:\n\n{: .left}\n\nHinge loss, SVM\n\nДля таких случаев, как на картинке выше, возникает логичное желание не только найти разделяющую прямую, но и постараться провести её на одинаковом удалении от обоих классов, то есть максимизировать минимальный отступ:\n\n{: .left}\n\nЭто можно сделать, слегка поменяв функцию ошибки, а именно положив её равной:\n\n$$F(M) = \\max(0, 1-M)$$\n\n$$L(w, x, y) = \\lambda|w|^2_2 + \\sum_i \\max(0, 1-y_i \\langle w, x_i\\rangle)$$\n\n$$ \\nabla_w L(w, x, y) = 2 \\lambda w + \\sum_i \\begin{cases} 0, & 1 - y_i \\langle w, x_i \\rangle \\leq 0 \\ - y_i x_i, & 1 - y_i \\langle w, x_i \\rangle > 0 \\end{cases} $$\n\nПочему же добавленная единичка приводит к желаемому результату?\n\nИнтуитивно это можно объяснить так: объекты, которые проклассифицированы правильно, но не очень "уверенно" (то есть $$0 \\leq y_i \\langle w, x_i\\rangle < 1$$), продолжают вносить свой вклад в градиент и пытаются "отодвинуть" от себя разделяющую плоскость как можно дальше.\n\nК данному выводу можно прийти и чуть более строго; для этого надо совершенно по-другому взглянуть на выражение, которое мы минимизируем. Поможет вот эта картинка:\n\n{: .left}\n\nЕсли мы максимизируем минимальный отступ, то надо максимизировать $$\\frac{2}{|w|_2}$$, то есть ширину полосы при условии того, что большинство объектов лежат с правильной стороны, что эквивалентно решению нашей исходной задачи:\n\n$$\\lambda|w|^2_2 + \\sum_i \\max(0, 1-y_i \\langle w, x_i\\rangle) \\longrightarrow\\min\\limits_{w}$$\n\nОтметим, что первое слагаемое у нас обратно пропорционально ширине полосы, но мы и максимизацию заменили на минимизацию, так что тут всё в порядке. Второе слагаемое – это штраф за то, что некоторые объекты неправильно расположены относительно разделительной полосы. В конце концов, никто нам не обещал, что классы наши линейно разделимы, и можно провести оптимальную плоскость вообще без ошибок.\n\nИтоговое положение плоскости задаётся всего несколькими обучающими примерами. Это ближайшие к плоскости правильно классифицированные объекты, которые называют опорными векторами или support vectors. Весь метод, соответственно, зовётся методом опорных векторов, или support vector machine или сокращённо SVM. Начиная с шестидесятых годов это был сильнейший из известных методов машинного обучения. В девяностые его сменили методы, основанные на деревьях решений, которые в свою очередь недавно передали "пальму первенства" нейросетям.\n\nПочему же SVM был столь популярен? Из-за небольшого количества параметров и доказуемой оптимальности. Сейчас для нас нормально выбирать специальный алгоритм под задачу и подбирать оптимальные гиперпараметры для этого алгоритма перебором, а когда-то трава была зеленее, а компьютеры медленнее, и такой роскоши у людей не было. Поэтому им нужны были модели, которые гарантированно неплохо работали бы в любой ситуации. Такой моделью и был SVM.\n\nДругие замечательные свойства SVM: существование уникального решения и доказуемо минимальная склонность к переобучению среди всех популярных классов линейных классификаторов. Кроме того, несложная модификация алгоритма, ядровый SVM, позволяет проводить нелинейные разделяющие поверхности.\n\nСтрогий вывод постановки задачи SVM можно прочитать тут или в лекции К.В. Воронцова.\n\nЛогистическая регрессия\n\nВ этом параграфе мы будем обозначать классы нулём и единицей.\n\nЕщё один интересный метод появляется из желания посмотреть на классификацию как на задачу предсказания вероятностей. Хороший пример – предсказание кликов в интернете (например, в рекламе и поиске). Наличие клика в обучающем логе не означает, что, если повторить полностью условия эксперимента, пользователь обязательно кликнет по объекту опять. Скорее у объектов есть какая-то "кликабельность", то есть истинная вероятность клика по данному объекту. Клик на каждом обучающем примере является реализацией этой случайной величины, и мы считаем, что в пределе в каждой точке отношение положительных и отрицательных примеров должно сходиться к этой вероятности.\n\nПроблема состоит в том, что вероятность, по определению, величина от 0 до 1, а простого способа обучить линейную модель так, чтобы это ограничение соблюдалось, нет. Из этой ситуации можно выйти так: научить линейную модель правильно предсказывать какой-то объект, связанный с вероятностью, но с диапазоном значений $$(-\\infty,\\infty)$$, и преобразовать ответы модели в вероятность. Таким объектом является logit или log odds – логарифм отношения вероятности положительного события к отрицательному $$\\log\\left(\\frac{p}{1-p}\\right)$$.\n\nЕсли ответом нашей модели является $$\\log\\left(\\frac{p}{1-p}\\right)$$, то искомую вероятность посчитать не трудно:\n\n$$\\langle w, x_i\\rangle = \\log\\left(\\frac{p}{1-p}\\right)$$\n\n$$e^{\\langle w, x_i\\rangle} = \\frac{p}{1-p}$$\n\n$$p=\\frac{1}{1 + e^{-\\langle w, x_i\\rangle}}$$\n\nФункция в правой части называется сигмоидой и обозначается\n\n$$\\color{#348FEA}{\\sigma(z) = \\frac1{1 + e^{-z}}}$$\n\nТаким образом, $p = \\sigma(\\langle w, x_i\\rangle)$\n\nКак теперь научиться оптимизировать $$w$$ так, чтобы модель как можно лучше предсказывала логиты? Нужно применить метод максимума правдоподобия для распределения Бернулли. Это самое простое распределение, которое возникает, к примеру, при бросках монетки, которая орлом выпадает вероятностью $$p$$. У нас только событием будет не орёл, а то, что пользователь кликнул на объект с такой вероятностью. Если хотите больше подробностей, почитайте про распределение Бернулли в теоретическом минимуме.\n\nПравдоподобие позволяет понять, насколько вероятно получить данные значения таргета $y$ при данных $X$ и весах $w$. Оно имеет вид\n\n$$ p(y\\mid X, w) =\\prod_i p(y_i\\mid x_i, w) $$\n\nи для распределения Бернулли его можно выписать следующим образом:\n\n$$ p(y\\mid X, w) =\\prod_i p_i^{y_i} (1-p_i)^{1-y_i} $$\n\nгде $$p_i$$ – это вероятность, посчитанная из ответов модели. Оптимизировать произведение неудобно, хочется иметь дело с суммой, так что мы перейдём к логарифмическому правдоподобию и подставим формулу для вероятности, которую мы получили выше:\n\n$$ \\ell(w, X, y) = \\sum_i \\big( y_i \\log(p_i) + (1-y_i)\\log(1-p_i) \\big) =$$\n\n$$ =\\sum_i \\big( y_i \\log(\\sigma(\\langle w, x_i \\rangle)) + (1-y_i)\\log(1 - \\sigma(\\langle w, x_i \\rangle)) \\big) $$\n\nЕсли заметить, что\n\n$$ \\sigma(-z) = \\frac{1}{1 + e^z} = \\frac{e^{-z}}{e^{-z} + 1} = 1 - \\sigma(z), $$\n\nто выражение можно переписать проще:\n\n$$ \\ell(w, X, y)=\\sum_i \\big( y_i \\log(\\sigma(\\langle w, x_i \\rangle)) + (1 - y_i) \\log(\\sigma(-\\langle w, x_i \\rangle)) \\big) $$\n\nНас интересует $w$, для которого правдоподобие максимально. Чтобы получить функцию потерь, которую мы будем минимизировать, умножим его на минус один:\n\n$$\\color{#348FEA}{L(w, X, y) = -\\sum_i \\big( y_i \\log(\\sigma(\\langle w, x_i \\rangle)) + (1 - y_i) \\log(\\sigma(-\\langle w, x_i \\rangle)) \\big)}$$\n\nВ отличие от линейной регрессии, для логистической нет явной формулы решения. Деваться некуда, будем использовать градиентный спуск. К счастью, градиент устроен очень просто:\n\n$$ \\nabla_w L(y, X, w) = -\\sum_i x_i \\big( y_i - \\sigma(\\langle w, x_i \\rangle)) \\big) $$ {% include details.html summary="Вывод формулы градиента" details=" Нам окажется полезным ещё одно свойство сигмоиды::\n\n$$ \\frac{d \\log \\sigma(z)}{d z} = \\left( \\log \\left( \\frac{1}{1 + e^{-z}} \\right) \\right)\' = \\frac{e^{-z}}{1 + e^{-z}} = \\sigma(-z) \\ \\frac{d \\log \\sigma(-z)}{d z} = -\\sigma(z) $$\n\nОтсюда:\n\n$$ \\nabla_w \\log \\sigma(\\langle w, x_i \\rangle) = \\sigma(-\\langle w, x_i \\rangle) x_i \\ \\nabla_w \\log \\sigma(-\\langle w, x_i \\rangle) = -\\sigma(\\langle w, x_i \\rangle) x_i $$\n\nи градиент оказывается равным\n\n$$ \\nabla_w L(y, X, w) = -\\sum_i \\big( y_i x_i \\sigma(-\\langle w, x_i \\rangle) - (1 - y_i) x_i \\sigma(\\langle w, x_i \\rangle)) \\big) = \\ = -\\sum_i \\big( y_i x_i (1 - \\sigma(\\langle w, x_i \\rangle)) - (1 - y_i) x_i \\sigma(\\langle w, x_i \\rangle)) \\big) = \\ = -\\sum_i \\big( y_i x_i - y_i x_i \\sigma(\\langle w, x_i \\rangle) - x_i \\sigma(\\langle w, x_i \\rangle) + y_i x_i \\sigma(\\langle w, x_i \\rangle)) \\big) = \\ = -\\sum_i \\big( y_i x_i - x_i \\sigma(\\langle w, x_i \\rangle)) \\big) $$ " %}\n\nПредсказание модели будет вычисляться, как мы договаривались, следующим образом:\n\n$$p=\\sigma(\\langle w, x_i\\rangle)$$\n\nЭто вероятность положительного класса, а как от неё перейти к предсказанию самого класса? В других методах нам достаточно было посчитать знак предсказания, но теперь все наши предсказания положительные и находятся в диапазоне от 0 до 1. Что же делать? Интуитивным и не совсем (и даже совсем не) правильным является ответ <<взять порог 0.5>>. Более корректным будет подобрать этот порог отдельно, для уже построенной регрессии минимизируя нужную вам метрику на отложенной тестовой выборке. Например, сделать так, чтобы доля положительных и отрицательных классов примерно совпадала с реальной.\n\nОтдельно заметим, что метод называется логистической регрессией, а не логистической классификацией именно потому, что предсказываем мы не классы, а вещественные числа – логиты.\n\nВопрос на подумать. Проверьте, что, если метки классов – это $\\pm1$, а не $0$ и $1$, то функцию потерь для логистической регрессии можно записать в более компактном виде:\n\n$$\\mathcal{L}(w, X, y) = \\sum_{i=1}^N\\log(1 + e^{-y_i\\langle w, x_i\\rangle})$$\n\nВопрос на подумать. Правда ли разделяющая поверхность модели логистической регрессии является гиперплоскостью? {% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details=" Разделяющая поверхность отделяет множество точек, которым мы присваиваем класс $$0$$ (или $$-1$$), и множество точек, которым мы присваиваем класс $$1$$. Представляется логичным провести отсечку по какому-либо значению предсказанной вероятности. Однако, выбор этого значения — дело не очевидное. Как мы увидим в главе про калибровку классификаторов, это может быть не настоящая вероятность. Допустим, мы решили провести границу по значению $$\\frac12$$. Тогда разделяющая поверхность как раз задаётся равенством $$p = \\frac12$$, что равносильно $$\\langle w, x\\rangle = 0$$. А это гиперплоскость. " %}\n\nВопрос на подумать. Допустим, что матрица объекты-признаки $X$ имеет полный ранг по столбцам (то есть все её столбцы линейно независимы). Верно ли, что решение задачи восстановления логистической регрессии единственно? {% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details="В этот раз хорошего геометрического доказательства, как было для линейной регрессии, пожалуй, нет; нам придётся честно посчитать вторую производную и доказать, что она является положительно определённой. Сделаем это для случая, когда метки классов – это $\\pm1$. Формулы так получатся немного попроще. Напомним, что в этом случае\n\n$$L(w, X, y) = -\\sum_{i=1}^N\\log(1 + e^{-y_i\\langle w, x_i\\rangle})$$\n\nСледовательно,\n\n$$\\frac{\\partial}{\\partial w_{j}}L(w, X, y) = \\sum_{i=1}^N\\frac{y_ix_{ij}e^{-y_i\\langle w, x_i\\rangle}}{1 + e^{-y_i\\langle w, x_i\\rangle}} = \\sum_{i=1}^Ny_ix_{ij}\\left(1 - \\frac1{1 + e^{-y_i\\langle w, x_i\\rangle}}\\right)$$\n\n$$\\frac{\\partial^2L}{\\partial w_j\\partial w_k}(w, X, y) = \\sum_{i=1}^Ny^2_ix_{ij}x_{ik}\\frac{e^{-y_i\\langle w, x_i\\rangle}}{(1 + e^{-y_i\\langle w, x_i\\rangle})^2} =$$\n\n$$ = \\sum_{i=1}^Ny^2_ix_{ij}x_{ik}\\sigma(y_i\\langle w, x_i\\rangle)(1 - \\sigma(y_i\\langle w, x_i\\rangle))$$\n\nТеперь заметим, что $$y_i^2 = 1$$ и что, если обозначить через $$D$$ диагональную матрицу с элементами $$\\sigma(y_i\\langle w, x_i\\rangle)(1 - \\sigma(y_i\\langle w, x_i\\rangle))$$ на диагонали, матрицу вторых производных можно представить в виде:\n\n$$\\nabla^2L = \\left(\\frac{\\partial^2\\mathcal{L}}{\\partial w_j\\partial w_k}\\right) = X^TDX$$\n\nТак как $$0 < \\sigma(y_i\\langle w, x_i\\rangle) < 1$$, у матрицы $$D$$ на диагонали стоят положительные числа, из которых можно извлечь квадратные корни, представив $$D$$ в виде $$D = D^{1/2}D^{1/2}$$. В свою очередь, матрица $$X$$ имеет полный ранг по столбцам. Стало быть, для любого вектора приращения $$u\\ne 0$$ имеем\n\n$$u^TX^TDXu = u^TX^T(D^{1/2})^TD^{1/2}Xu = \\vert D^{1/2}Xu \\vert^2 > 0$$\n\nТаким образом, функция $$L$$ выпукла вниз как функция от $$w$$, и, соответственно, точка её экстремума непременно будет точкой минимума.\n\nА теперь – почему это не совсем правда. Дело в том, что, говоря «точка её экстремума непременно будет точкой минимума», мы уже подразумеваем существование этой самой точки экстремума. Только вот существует этот экстремум не всегда. Можно показать, что для линейно разделимой выборки функция потерь логистической регрессии не ограничена снизу, и, соответственно, никакого экстремума нет. Доказательство мы оставляем читателю. " %}\n\nВопрос на подумать. На картинке ниже представлены результаты работы на одном и том же датасете трёх моделей логистической регрессии с разными коэффициентами $L^2$-регуляризации:\n\n{: .left}\n\nНаверху показаны предсказанные вероятности положительного класса, внизу – вид разделяющей поверхности.\n\nКак вам кажется, какие картинки соответствуют самому большому коэффициенту регуляризации, а какие – самому маленькому? Почему? {% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details=" Коэффициент регуляризации максимален у левой модели. На это нас могут натолкнуть два соображения. Во-первых, разделяющая прямая проведена достаточно странно, то есть можно заподозрить, что регуляризационный член в лосс-функции перевесил функцию потерь исходной задачи. Во-вторых, модель предсказывает довольно близкие к $$\\frac12$$ вероятности – это значит, что значения $$\\langle w, x\\rangle$$ близки к нулю, то есть сам вектор $w$ близок к нулевому. Это также свидетельствует о том, что регуляризационный член играет слишком важную роль при оптимизации.\n\nНаименьший коэффициент регуляризации у правой модели. Её предсказания достаточно <<уверенные>> (цвета на верхнем графике сочные, то есть вероятности быстро приближаются к $$0$$ или $$1$$). Это может свидетельствовать о том, что числа $$\\langle w, x\\rangle$$ достаточно велики по модулю, то есть $$\\vert\\vert w \\vert\\vert$$ достаточно велик. " %}\n\nМногоклассовая классификация\n\nВ этом разделе мы будем следовать изложению из лекций Евгения Соколова.\n\nПусть каждый объект нашей выборки относится к одному из $K$ классов: $\\mathbb{Y} = {1, \\ldots, K}$. Чтобы предсказывать эти классы с помощью линейных моделей, нам придётся свести задачу многоклассовой классификации к набору бинарных, которые мы уже хорошо умеем решать. Мы разберём два самых популярных способа это сделать – one-vs-all и all-vs-all, а проиллюстрировать их нам поможет вот такой игрушечный датасет\n\n{: .left}\n\nОдин против всех (one-versus-all)\n\nОбучим $K$ линейных классификаторов $b_1(x), \\ldots, b_K(x)$, выдающих оценки принадлежности классам $1, \\ldots, K$ соответственно. В случае с линейными моделями эти классификаторы будут иметь вид\n\n$$b_k(x) = \\text{sgn}\\left(\\langle w_k, x \\rangle + w_{0k}\\right)$$\n\nКлассификатор с номером $k$ будем обучать по выборке $\\left(x_i, 2\\mathbb{I}[y_i = k] - 1\\right)_{i = 1}^{N}$; иными словами, мы учим классификатор отличать $k$-й класс от всех остальных.\n\nЛогично, чтобы итоговый классификатор выдавал класс, соответствующий самому уверенному из бинарных алгоритмов. Уверенность можно в каком-то смысле измерить с помощью значений линейных функций:\n\n$$a(x) = \\text{argmax}k \\left(\\langle w_k, x \\rangle + w{0k}\\right) $$\n\nДавайте посмотрим, что даст этот подход применительно к нашему датасету. Обучим три линейных модели, отличающих один класс от остальных:\n\n{: .left}\n\nТеперь сравним значения линейных функций\n\n{: .left}\n\nи для каждой точки выберем тот класс, которому соответствует большее значение, то есть самый <<уверенный>> классификатор:\n\n{: .left}\n\nХочется сказать, что самый маленький класс <<обидели>>.\n\nПроблема данного подхода заключается в том, что каждый из классификаторов $b_1(x), \\dots, b_K(x)$ обучается на своей выборке, и значения линейных функций $\\langle w_k, x \\rangle + w_{0k}$ или, проще говоря, "выходы" классификаторов могут иметь разные масштабы. Из-за этого сравнивать их будет неправильно. Нормировать вектора весов, чтобы они выдавали ответы в одной и той же шкале, не всегда может быть разумным решением: так, в случае с SVM веса перестанут являться решением задачи, поскольку нормировка изменит норму весов.\n\nВсе против всех (all-versus-all)\n\nОбучим $C_K^2$ классификаторов $a_{ij}(x)$, $i, j = 1, \\dots, K$, $i \\neq j$. Например, в случае с линейными моделями эти модели будут иметь вид\n\n$$b_{ij}(x) = \\text{sgn}\\left( \\langle w_{ij}, x \\rangle + w_{0,ij} \\right)$$\n\nКлассификатор $a_{ij}(x)$ будем настраивать по подвыборке $X_{ij} \\subset X$, содержащей только объекты классов $i$ и $j$. Соответственно, классификатор $a_{ij}(x)$ будет выдавать для любого объекта либо класс $i$, либо класс $j$. Проиллюстрируем это для нашей выборки:\n\n{: .left}\n\nЧтобы классифицировать новый объект, подадим его на вход каждого из построенных бинарных классификаторов. Каждый из них проголосует за своей класс; в качестве ответа выберем тот класс, за который наберется больше всего голосов:\n\n$$a(x) = \\text{argmax}k\\sum{i = 1}^{K} \\sum_{j \\neq i}\\mathbb{I}[a_{ij}(x) = k]$$\n\nДля нашего датасета получается следующая картинка:\n\n{: .left}\n\nОбратите внимание на серый треугольник на стыке областей. Это точки, для которых голоса разделились (в данном случае каждый классификатор выдал какой-то свой класс, то есть у каждого класса было по одному голосу). Для этих точек нет явного способа выдать обоснованное предсказание.\n\nМногоклассовая логистическая регрессия\n\nНекоторые методы бинарной классификации можно напрямую обобщить на случай многих классов. Выясним, как это можно проделать с логистической регрессией.\n\nВ логистической регрессии для двух классов мы строили линейную модель\n\n$$b(x) = \\langle w, x \\rangle + w_0$$\n\nа затем переводили её прогноз в вероятность с помощью сигмоидной функции $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$. Допустим, что мы теперь решаем многоклассовую задачу и построили $K$ линейных моделей\n\n$$b_k(x) = \\langle w_k, x \\rangle + w_{0k},$$\n\nкаждая из которых даёт оценку принадлежности объекта одному из классов. Как преобразовать вектор оценок $(b_1(x), \\ldots, b_K(x))$ в вероятности? Для этого можно воспользоваться оператором $\\text{softmax}(z_1, \\ldots, z_K)$, который производит <<нормировку>> вектора:\n\n$$\\text{softmax}(z_1, \\ldots, z_K) = \\left(\\frac{\\exp(z_1)}{\\sum_{k = 1}^{K} \\exp(z_k)}, \\dots, \\frac{\\exp(z_K)}{\\sum_{k = 1}^{K} \\exp(z_k)}\\right).$$\n\nВ этом случае вероятность $k$-го класса будет выражаться как\n\n$$P(y = k \\vert x, w) = \\frac{ \\exp{(\\langle w_k, x \\rangle + w_{0k})}}{ \\sum_{j = 1}^{K} \\exp{(\\langle w_j, x \\rangle + w_{0j})}}.$$\n\nОбучать эти веса предлагается с помощью метода максимального правдоподобия: так же, как и в случае с двухклассовой логистической регрессией:\n\n$$\\sum_{i = 1}^{N} \\log P(y = y_i \\vert x_i, w) \\to \\max_{w_1, \\dots, w_K}$$\n\nМасштабируемость линейных моделей\n\nМы уже обсуждали, что SGD позволяет обучению хорошо масштабироваться по числу объектов, так как мы можем не загружать их целиком в оперативную память. А что делать, если признаков очень много, или мы не знаем заранее, сколько их будет? Такое может быть актуально, например, в следующих ситуациях:\n\nКлассификация текстов: мы можем представить текст в формате <<мешка слов>>, то есть неупорядоченного набора слов, встретившихся в данном тексте, и обучить на нём, например, определение тональности отзыва в интернете. Наличие каждого слова из языка в тексте у нас будет кодироваться отдельной фичой. Тогда размерность каждого элемента обучающей выборки будет порядка нескольких сотен тысяч.\n\nВ задаче предсказания кликов по рекламе можно получить выборку любой размерности, например, так: в качестве фичи закодируем индикатор того, что пользователь X побывал на веб-странице Y. Суммарная размерность тогда будет порядка $$10^9 \\cdot 10^7 = 10^{16}$$. Кроме того, всё время появляются новые пользователи и веб-страницы, так что на этапе применения нас ждут сюрпризы.\n\nЕсть несколько хаков, которые позволяют бороться с такими проблемами: * Несмотря на то, что полная размерность объекта в выборке огромна, количество ненулевых элементов в нём невелико. Значит, можно использовать разреженное кодирование, то есть вместо плотного вектора хранить словарь, в котором будут перечислены индексы и значения ненулевых элементов вектора. * Даже хранить все веса не обязательно! Можно хранить их в хэш-таблице и вычислять индекс по формуле hash(feature) % tablesize. Хэш может вычисляться прямо от слова или id пользователя. Таким образом, несколько фичей будут иметь общий вес, который тем не менее обучится оптимальным образом. Такой подход называется hashing trick. Ясно, что сжатие вектора весов приводит к потерям в качестве, но, как правило, ценой совсем небольших потерь можно сжать этот вектор на много порядков.\n\nПримером открытой библиотеки, в которой реализованы эти возможности, является vowpal wabbit.\n\nParameter server\n\nЕсли при решении задачи ставки столь высоки, что мы не можем разменивать качество на сжатие вектора весов, а признаков всё-таки очень много, то задачу можно решать распределённо, храня все признаки в шардированной хеш-таблице\n\n{: .left}\n\nКружки здесь означают отдельные сервера. Жёлтые загружают данные, а серые хранят части модели. Для обучения жёлтый кружок запрашивает у серого нужные ему для предсказания веса, считает градиент и отправляет его обратно, где тот потом применяется. Схема обладает бесконечной масштабируемостью, но задач, где это оправдано, не очень много.\n\nПодытожим\n\nНа линейную модель можно смотреть как на однослойную нейросеть, поэтому многие методы, которые были изначально разработаны для них, сейчас переиспользуются в задачах глубокого обучения, а базовые подходы к регрессии, классификации и оптимизации вообще выглядят абсолютно так же. Так что несмотря на то, что в целом линейные модели на сегодня применяются редко, то, из чего они состоят и как строятся, знать очень и очень полезно.\n\nНадеемся также, что главным итогом прочтения этой главы для вас будет осознание того, что решение любой ML-задачи состоит из выбора функции потерь, параметризованного класса моделей и способа оптимизации. В следующих главах мы познакомимся с другими моделями и оптимизаторами, но эти базовые принципы не изменятся.')]