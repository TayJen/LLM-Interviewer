[Document(metadata={'source': '/Users/eugenekillevsky/PycharmProjects/LLM-Interviewer/data/RAG/ml-handbook-master/chapters/intro/intro.md'}, page_content='title: Введение author: filipp_sinicin, stanislav_fedotov\n\nОб этой книге\n\nЭта книга написана коллективом добрых людей, состоящим из преподавателей и выпускников Школы анализа данных. Своим появлением она обязана двум замечательным курсам. Во-первых, это курс Константина Вячеславовича Воронцова, на котором выросло подавляющее большинство авторов книги, да и вообще ML-специалистов в России. Во-вторых, это курс NLP Course | For You Лены Войта, благодаря которому мы поняли, как должен выглядеть современный учебник, и на который мы будем регулярно ссылаться в частях, связанных с анализом текстов.\n\nИдея была такая: записать сложившийся в ШАДе курс машинного обучения в виде книги, при этом избежав каких-либо компромиссов: нигде ничего не упрощать чрезмерно, дать необходимую теорию, описать и исторически важные алгоритмы, и применяющиеся сегодня, вместе с теорией рассказывать и практические вопросы о реализации алгоритмов и работе с данными.\n\nМатематика — это один из языков, на котором написан учебник. Мы будем стараться давать необходимые пояснения, но всё же уверенное владение линейной алгеброй, математическим анализом и теорией вероятностей будет большим плюсом. Знания статистики и методов выпуклой оптимизации не обязательны, хотя сделают чтение комфортнее.\n\nЧитая книгу, вы, возможно, заметите в ней ошибки, неточности и плохо объяснённые детали. В таком случае, пожалуйста, дайте нам знать об этом, написав (сюда) — так вы поможете и другим читателям.\n\nИтак, приступим.\n\nМашинное обучение\n\nМашинное обучение — это наука, изучающая алгоритмы, автоматически улучшающиеся благодаря опыту.\n\nС момента возникновения компьютеров человечество пытается автоматизировать всё больше и больше задач. Многие проблемы получается алгоритмически решить (и запрограммировать), но по разным причинам выходит это не всегда. Например, бывают задачи, которые люди не могут решить сами; более того, их доказуемо нельзя эффективно решить, и компьютер здесь чуда тоже не совершит (речь об NP-трудных задачах). Но бывают и задачи, которые для людей особенного труда не составляют, но которые почему-то трудно или вообще невозможно запрограммировать, например:\n\nперевести текст с одного языка на другой;\n\nдиагностировать болезнь по симптомам;\n\nсравнить, какой из двух документов в интернете лучше подходит под данный поисковый запрос;\n\nсказать, что изображено на картинке;\n\nоценить, по какой цене удастся продать квартиру.\n\nЭти задачи объединяет как минимум несколько вещей. Во-первых, их решение можно записать как функцию, которая отображает объекты, или примеры (samples) в предсказания (targets). Например, больных в диагнозы, документы в оценку релевантности. Во-вторых, нам подойдёт не идеальное, а достаточно хорошее решение: ведь и доктора иногда делают ошибки в диагнозах, и вы не всегда можете сказать, что же именно изображено на картинке. В-третьих, у нас есть много примеров правильных ответов (скажем, переводов предложения на другой язык или подписей к заданной картинке), а примеры неправильных ответов (если они нужны), как правило, не составляет труда сконструировать. Функция, отображающая объекты в предсказания, именуется моделью, а имеющийся у нас набор примеров иногда ещё называют обучающей выборкой или датасетом (от слова dataset). Обучающая выборка состоит из\n\nобъектов (это могут быть скачанные из интернета картинки, истории известных нам больных, активность пользователей на сервисе)\n\nи ответов для них (это могут быть подписи к картинкам, диагнозы, факты ухода пользователей с сервиса), которые мы также будем иногда называть таргетами\n\nПо обучающей выборке мы хотим построить модель, предсказания которой достаточно хороши. Но что значит «достаточно хороши»? Обычно качество предсказаний измеряют с помощью метрик качества, то есть функций, которые показывают, насколько сильно полученные предсказания, выдаваемые моделью, похожи на правильные ответы. Метрики могут быть очень разными:\n\nдля задачи с диагнозами хорошими метриками могут быть, например, доля правильно поставленных диагнозов или доля больных, которым удалось поставить правильный диагноз (а вы поняли разницу?);\n\nв задаче с ценой квартиры — доля квартир, для которых разница между предсказанным и истинным значением цены не превысила какого-то порога, или средний модуль разницы между предсказанным и истинным значением;\n\nв задаче с поисковыми документами — доля пар документов, которые мы упорядочили неправильно.\n\nЦель обычно в том, чтобы получить как можно лучшее (наибольшее или наименьшее возможное, в зависимости от ситуации) значение метрики.\n\nВопрос на подумать. Важно помнить, что разные нужды заказчика могут диктовать самые разные метрики. Вернёмся к задаче постановки диагноза пациентам больницы. Какие метрики вы предложили бы использовать в каждом из следующих случаев:\n\nобычный год в обычном терапевтическом отделении обычной больницы;\n\nопределение очень унизительной болезни, которая жутким клеймом падёт на каждого, кому поставили такой диагноз;\n\nопределение опасной и очень заразной болезни.\n\n{% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details="Конечно, даже в каждом из этих довольно частных случаев могут быть разные ситуации и разные метрики, но вот как, например, можно было бы ответить:\n\nОбычный год в обычном терапевтическом отделении обычной больницы — тогда главного врача вполне устроит, если доля правильно поставленных диагнозов будет высокой (эта метрика называется accuracy).\n\nОпределение очень неприятной болезни, которая жутким клеймом падёт на каждого, кому поставили такой диагноз, — тогда нам важно максимизировать долю действительно больных среди тех, кому мы имели несчастье поставить этот диагноз (эта метрика называется точностью, или precision).\n\nОпределение опасной и очень заразной болезни — тогда нам важно не пропустить ни одного заражённого, и метрика будет иметь вид доли правильно определённых носителей (эта метрика называется полнотой, или recall).\n\nРазумеется, это самые простые метрики, и в реальной жизни аналитику приходится сталкиваться с более сложной иерархией метрик; немного подробнее мы поговорим об этом в главе про измерение качества моделей.\n\n%}\n\nВопрос на подумать. Рассмотрим задачу детектирования людей на изображении (под детектированием чаще всего понимают указание местоположения человека, например прямоугольника, в котором он находится). Подумайте, какие метрики можно было бы использовать в различных ситуациях для измерения качества решения этой задачи. Не забудьте, что метрики — это способ численно измерить то, насколько модель помогает нам в жизни, так что важно думать о том, зачем вообще нам детектировать людей.\n\n{% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details="Вот несколько вариантов, которые можно было бы придумать:\n\nМы разрабатываем программу для проведения видеоконференций и хотим добавить эффект, который облачает участника в рыцарские доспехи, — в этом случае нам важно корректно определять местоположение и в качестве метрики мы могли бы брать среднеквадратичное отклонение координат каких-нибудь опорных точек тела от истинных.\n\nМы строим систему безопасности вокруг какого-то важного объекта, и нам важно обнаруживать вторжение — в этом случае нам не очень принципиально, насколько точно отмечено местоположение человека в кадре, но все люди должны быть обнаружены. Таким образом, в качестве метрики можно рассмотреть полноту: на какой доле кадров, где действительно были люди, наша модель отметила их наличие.\n\nМы строим систему, определяющую, не превышает ли количество людей в помещении некоторый порог (например, в рамках борьбы с пандемией), — в этом случае метрикой может быть, скажем, среднеквадратичное отклонение числа детектированных моделью людей от истинного их количества.\n\n%}\n\nКак найти функцию, которая минимизирует метрику на данной выборке? В общем случае решить эту задачу не представляется возможным. Но, быть может, если ограничиться каким-то классом функций, то на нём получится найти оптимум?\n\nДля примера давайте возьмём задачу предсказания цены квартиры, в качестве класса моделей — константные функции $$f(x) = c$$ (то есть будем для всех квартир предсказывать одно и то же значение цены), а в качестве метрики — среднее абсолютное отклонение (mean absolute error, она же MAE).\n\n$$MAE(f, X, y) = L(f, X, y) = \\frac1N\\sum\\limits_{i=1}^N \\vert f(x_i) - y_i\\vert \\rightarrow \\min\\limits_f,$$\n\nгде $$f$$ — это модель (та самая, $$f(x) = c$$), $X = \\{x_1,\\ldots,x_N\\}$ — обучающие примеры (данные о квартирах, которые мы смогли достать), $y = \\{y_1,\\ldots,y_N\\}$ — правильные ответы (то есть цены на известные нам квартиры). Так как предсказание модели константное, по нему легко можно взять производную, которую мы приравняем к нулю, чтобы найти оптимальное значение $$c$$:\n\n$$\\nabla_cL(f, X, y) = \\frac1N\\sum\\limits_{i=1}^N sign(c - y_i) = 0$$\n\nНемного погрешив против математической строгости, можно сказать, что $0$ (и, соответственно, оптимум нашей метрики) достигается в точке $$f(x) = \\mathrm{median}(y)$$.\n\nВопрос на подумать. Давайте теперь в задаче предсказания цены квартиры рассмотрим метрику среднеквадратичное отклонение (MSE):\n\n$$MSE(f, X, y) = \\frac1N\\sum_{i=1}^N(f(x_i) - y_i)^2$$\n\nКаким будет оптимальное значение параметра $c$ для константной модели $f(x) = c$?\n\n{% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details="Это будет среднее значение:\n\n$$\\overline{y} = \\frac1N\\sum_{i=1}^Ny_i$$\n\n%}\n\nПрекрасно, значит, в классе константных функций мы можем найти оптимальную модель. Может быть, это можно сделать и в каком-нибудь более интересном классе? Этому вопросу и будет посвящена большая часть нашей книги. Классический курс ML состоит из описания классов моделей и способов работы с ними. Несмотря на то что для решения большинства практических задач на сегодня достаточно знать только два типа моделей — градиентный бустинг на решающих деревьях и нейросетевые модели — мы постараемся рассказать и про другие, чтобы развить у вас глубинное понимание предмета и дать возможность не только использовать лучшие сложившиеся практики, но и, при вашем желании, участвовать в проработке новых идей и поиске новых методов — уже в роли исследователя, а не просто инженера.\n\nНе любое сочетание задач, моделей и метрик имеет смысл. Скажем, если вы предсказываете класс опасности вещества (бывает от 1-го до 4-го) по его химической формуле, MAE по умолчанию представляется не очень обоснованной метрикой, уж точно менее удачной, чем доля правильных предсказаний: ведь несмотря на то, что классы вроде как представлены целыми числами, их порядок не имеет чёткой семантики. И наоборот, если мы предсказываем возраст человека в годах по его фотографии, точность предсказаний кажется уже не столь удачной метрикой (хотя возраст принимает лишь конечное и не очень большое число различных значений), а вот MAE — вполне. В ходе курса вы научитесь выбирать правильные метрики для различных задач и классов моделей — и поймёте, что этот выбор не всегда так однозначен, как хотелось бы.\n\nПри этом стоит отметить, что при постановке бизнес-задачи метрика обычно выбирается из соображений целесообразности, а не из-за её совместимости с какими-то моделями (больше того, модель обычно возникает уже в процессе оптимизации метрики как средство). Например, человек, ставящий аналитику задачу о предсказании цены квартиры, может попросить в качестве метрики использовать доход риелторского агентства или аудиторию сайта этого агентства. Ясно, что в такой ситуации особо ничего не продифференцируешь, потому что доход можно лишь неявно связать с качеством предсказания цен на квартиры. Или же метрика окажется принципиально недифференцируемой (как, например, точность предсказания). Что же делать? Обычно поступают так: выбирают прокси-метрику, оптимизируя которую будут улучшать и исходную метрику или хотя бы уповать на это. Например, в качестве прокси для дохода риелторского агентства можно использовать метрику абсолютной близости и понадеяться, что точные предсказания приведут к успеху и популярности компании. А вместо точности предсказания класса опасного вещества мы можем попробовать взять среднеквадратичное отклонение (вот это плохая идея, а как надо делать — вы узнаете дальше). В таком случае метрику, которую используют при поиске оптимальной модели, называют функцией потерь, ошибкой, лоссом (от loss) или лосс-функцией. Выбор удачной функции потерь и класса моделей для конкретной задачи — тонкое и хорошо оплачиваемое искусство, которому мы надеемся научить вас в нашем курсе.\n\nВиды задач\n\nОписанные выше задачи являются примерами задач обучения с учителем (supervised learning), так как правильные ответы для каждого объекта обучающей выборки заранее известны. Задачи обучения с учителем делятся на следующие виды в зависимости от того, каким может быть множество $\\mathbb{Y}$ всех возможных ответов (таргетов): 1. $\\mathbb{Y} = \\mathbb{R}$ или $\\mathbb{Y} = \\mathbb{R}^M$ — регрессия. Примерами задач регрессии является предсказание продолжительности поездки на каршеринге, спрос на конкретный товар в конкретный день или погода в вашем городе на завтра (температура, влажность и давление — это пример, когда мы предсказываем сразу несколько вещественных чисел). 2. $\\mathbb{Y} = \\{0, 1\\}$ — бинарная классификация. Например, мы можем предсказывать, кликнет ли пользователь по рекламному объявлению, вернёт ли клиент кредит в установленный срок, сдаст ли студент сессию, случится ли определённое заболевание у пациента (на основе, скажем, его генома), есть ли на картинке банан. 3. $\\mathbb{Y} = \\{1, \\dots, K\\}$ — многоклассовая (multiclass) классификация. Например, определение предметной области для научной статьи (математика, биология, психология и т. д.). 4. $\\mathbb{Y} = \\{0, 1\\}^K$ — многоклассовая классификация с пересекающимися классами (multilabel classification). Например, задача автоматического проставления тегов для ресторанов (логично, что ресторан может одновременно иметь несколько тегов). 5. $\\mathbb{Y}$ — конечное упорядоченное множество — ранжирование. Основным примером является задача ранжирования поисковой выдачи, где для любого запроса нужно отсортировать все возможные документы по релевантности этому запросу; при этом оценка релевантности имеет смысл только в контексте сравнения двух документов между собой, её абсолютное значение информации не несёт.\n\nОтвет может быть и более сложным. Так, в задаче сегментации изображения требуется для каждого пикселя предсказать, к какому объекту или типу объектов он относится, а в задаче машинного перевода мы должны сгенерировать предложение (или целый текст), являющееся переводом исходного. Интерес представляют и задачи порождения новых объектов, то есть генерации правдоподобных объектов, из ничего или на основе уже существующих. На первый взгляд может показаться, что таким моделям сложно найти применение: понятно, почему мы хотим научиться отличать изображение кошки от изображения собаки, но не очень понятно, зачем нам уметь генерировать изображения собак. Однако с помощью такой модели также можно научиться увеличивать разрешение изображения и применять любимые всеми маски в Snapchat и Instagram.\n\nЧем сложнее задача, тем больше данных нужно, чтобы её решить. Например, существенные успехи в задачах распознавания изображений были достигнуты лишь с появлением очень больших датасетов (и, стоит добавить, вычислительных мощностей). Вычислительные ресурсы продолжают совершенствовать, но во многих ситуациях размеченных данных (то есть объектов, которым кто-то сопоставил ответ) было бы по-прежнему слишком мало: например, для решения задачи аннотирования изображений (image captioning) потребовалось бы огромное количество пар (изображение, описание).\n\nБороться с этой проблемой можно двумя способами. Первый — использование краудсорсинга, то есть привлечение людей, готовых за относительно небольшие деньги разметить много данных. Во многих ситуациях (например, когда речь заходит об оценке поисковой выдачи) без него никак, и мы обязательно познакомим вас с этим подходом. Второй же способ состоит в использовании наряду с размеченными и неразмеченных данных. В самом деле, в той же задаче аннотирования изображений у нас есть огромное количество никак не связанных друг с другом изображений и текстов — и мы можем использовать их для того, чтобы, например, помочь компьютеру понять, какие слова в принципе могут стоять рядом в предложении. Подходы, связанные с использованием неразмеченных данных для решения задач обучения с учителем, объединяются термином self-supervised learning и очень активно используются сейчас. Важной составляющей является обучение представлений (representation learning) — задача построения компактных векторов небольшой размерности из сложных по структуре данных (например, изображений, звука, текстов, графов) так, чтобы близкие по структуре или семантике данные получали метрически близкие представления. Делать это можно разными способами — например, используя фрагменты моделей, обученных для решения какой-либо другой задачи, или строя модель, предсказывающую скрытую часть объекта по оставшейся его части — например, пропущенное слово в предложении. Этому будет посвящена отдельная глава нашего учебника.\n\nЕсть и относительно небольшой класс задач, относящихся к обучению без учителя (unsupervised learning), — это задачи, для которых нам известны только данные, а ответы неизвестны или вообще не существуют, а их поиск не является самоцелью. Классическим примером обучения без учителя является кластеризация — задача разделения объектов на группы, обладающие некоторыми (неизвестными нам, но, как мы в глубине души надеемся, интерпретируемыми) свойствами. Примером может служить кластеризация документов из электронной библиотеки по темам или кластеризация новостей с целью выделения крупных сюжетов.\n\nБывают и другие виды (и даже парадигмы) машинного обучения, так что если вы встретите задачу, которую никак не получается отнести к одному из перечисленных выше типов, не расстраивайтесь и знайте, что где-то дальше в учебнике вас ждёт рассказ про такие задачи.\n\nВопрос на подумать. Для следующих задач определите, относятся ли они к обучению с учителем, или без, или к чему-то промежуточному, попробуйте отнести их к более узким видам задач.\n\nПредсказание курса евро к доллару на следующий день.\n\nСтилизация текста. Например, перевод на бюрократический язык: «Пиппина и Мерри похитили!» $\\mapsto$ «Граждане Тук, Перегрин Паладинович, 2990 года рождения, и Брендибак, Мериадок Сарадокович, 2982 года рождения, были похищены неустановленными лицами».\n\nДетектирование котиков на изображении.\n\nОбучение робокота запрыгивать на стол из произвольной позы.\n\nПоиск наборов товаров, которые посетители супермаркета часто покупают вместе.\n\n{% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details="\n\nЭто задача регрессии: предсказывается вещественное число (пусть и с небольшим количеством знаков после запятой).\n\nЭто задача генерации новых объектов (на основе уже существующих).\n\nВ зависимости от того, для чего мы детектируем котиков, это может быть задача регрессии (предсказание координат вершин прямоугольника, в котором находится котик) или классификации (если нас просто интересует, есть котик или нет).\n\nЭту задачу можно решать по-разному. Например, создав физическую модель движения робокота и рассчитав оптимальную последовательность движений. Но если мы всё-таки хотим решать её с помощью машинного обучения, то можно поступить следующим образом. Создадим компьютерную симуляцию (чтобы не ломать настоящего робота) и модель, которая будет в каждый момент на основе конфигурации сочленений, высоты от пола, расстояния до стола, фазы Луны и других важных параметров предсказывать, как нужно дальше поворачивать лапы, изгибать спину кота и так далее, — и эту модель будем прогонять в симуляции, так или иначе меняя её в зависимости от того, насколько удачно робот справляется со своей задачей. Такая парадигма называется обучением с подкреплением (reinforcement learning), и о ней мы поговорим в отдельной главе.\n\nВы можете спросить: а почему это не обучение с учителем? Ведь у нас есть объекты — последовательности движений и ответы — запрыгнул кот на стол или нет. Проблема в том, что перебрать кучу траекторий (ввиду сложности задачи — действительно огромную кучу) и для каждой получить ответ — это очень долго и сложно; кроме того, нам хотелось бы иметь фреймворк, в котором можно было бы относительно легко адаптироваться, скажем, к изменению высоты стола.\n\nЭто задача обучения без учителя.\n\n%}\n\nВопрос на подумать. Ранжирование — это задача с таргетом из конечного упорядоченного множества $(1,\\ldots,K)$. Но, казалось бы, её запросто можно было бы рассматривать как задачу классификации на $K$ классов или задачу регрессии. В чём же проблема? Почему так не делают?\n\n{% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details="На самом деле для решения задач ранжирования обычно строят модель, предсказывающую некоторое вещественное число, по которому затем сортируют объекты, — так почему бы это не регрессия? Дело в том, что функции потерь и метрики в этой задаче совсем другие: ведь нам неважно, какие именно вещественные числа мы предсказываем; мы просто хотим, чтобы более релевантным объектам сопоставлялись числа побольше.\n\nПоймём теперь, почему задача «предскажите 10 самых релевантных объектов» непохожа на задачу классификации. Причина в том, что мир меняется, появляются новые объекты, и если к нам в руки попадёт объект более релевантный, чем текущий топ-1, все номера позиций поедут, и выученное нами соответствие объектов и номеров можно будет выкидывать на помойку.\n\n" %}\n\nВыбор модели, переобучение\n\nМожет показаться, что мы вас обманули, когда пугали сложностями: очевидно, что для любой задачи машинного обучения можно построить идеальную модель, надо всего лишь запомнить всю обучающую выборку с ответами. Такая модель может достичь идеального качества по любой метрике, но радости от неё довольно мало, ведь мы хотим, чтобы она выявила какие-то закономерности в данных и помогла нам с ответами там, где мы их не знаем. Важно понимать, какая у построенной модели обобщающая способность, то есть насколько она способна выучить общие закономерности, присущие не только обучающей выборке, и давать адекватные предсказания на новых данных. Для того чтобы предохранить себя от конфуза, поступают обычно так: делят выборку с данными на две части: обучающую выборку и тестовую выборку (train и test). Обучающую выборку используют для собственно обучения модели, а метрики считают на тестовой.\n\nТакой подход позволяет отделить модели, которые просто удачно подстроились к обучающим данным, от моделей, в которых произошла генерализация (generalization), то есть от таких, которые на самом деле кое-что поняли о том, как устроены данные, и могут выдавать полезные предсказания для объектов, которые не видели.\n\nНапример, рассмотрим три модели регрессионной зависимости, построенные на одном и том же синтетическом датасете с одним-единственным признаком. Жёлтым нарисованы точки обучающей выборки. Здесь мы представим, что есть «истинная» закономерность (пунктир), которая искажена шумом (погрешности измерения, влияние других факторов и т.д.).\n\n{: .left wight=1000px}\n\nЛевая, линейная модель недостаточно хороша: она сделала, что могла, но плохо приближает зависимость, особенно при при маленьких и при больших $x$. Правая «запомнила» всю обучающую выборку (и в самом деле, чтобы вычислить значение этой функции, нам надо знать координаты всех исходных точек) вместо того, чтобы моделировать исходную зависимость. Наконец, центральная, хоть и не проходит через точки обучающей выборки, довольно неплохо моделирует истинную зависимость.\n\nАлгоритм, избыточно подстроившийся под данные, называют переобученным.\n\nС увеличением сложности модели ошибка на обучающей выборке падает. Во многих задачах очень сложная модель будет работать примерно так же, как модель, «просто запомнившая всю обучающую выборку», но с генерализацией всё будет плохо: ведь выученные закономерности будут слишком специфическими, подогнанными под то, что происходит на обучающей выборке. Мы видим это на трёх графиках сверху: линейная функция очень проста, но и закономерность приближает лишь очень грубо; на правом же графике мы видим довольно хитрую функцию, которая точно подобрана под значения из обучающей выборки, но явно слишком эксцентрична, чтобы соответствовать какой-то природной зависимости. Оптимальная же генерализация достигается на модели не слишком сложной и не слишком простой.\n\nВ качестве иллюстрации для того же самого датасета рассмотрим модели вида\n\n$$y\\ =\\ \\text{ многочлен степени }D$$\n\nЯсно, что с ростом $D$ сложность модели растёт, и она достигает всё лучшего качества на обучающей выборке. А что, если у нас есть ещё тестовая выборка? Каким будет качество на ней? Вот так могут выглядеть графики среднеквадратичного отклонения (MSE) для обучающей и тестовой выборок:\n\n{: .left wight=1000px}\n\nМы видим здесь типичную для классических моделей картину: MSE на обучающей выборке падает (может быть, даже до нуля), а на тестовой сперва падает, а затем начинает снова расти.\n\nЗамечание. Для моделей глубинного обучения всё немного интереснее: в некоторых ситуациях есть грань, за которой метрика на тестовой выборке снова начинает падать. Но об этом в своё время. Пока давайте запомним, что слишком сложная модель — это вредно, а переобучение — боль.\n\nТочный способ выбрать алгоритм оптимальной сложности по данной задаче нам пока неизвестен, хотя какую-то теоретическую базу имеющимся философским наблюдениям мы дадим в главе про теорию обучения; при этом есть хорошо продуманная методология сравнения разных моделей и выбора среди них оптимальной — об этом мы обязательно расскажем вам в следующих главах. А пока дадим самый простой и неизменно ценный совет: не забывайте считать метрики на тестовой выборке и никогда не смешивайте её с обучающей!\n\nВопрос на подумать. Обсуждая переобучение, мы упоминали про сложность модели, но не сказали, что это такое. Как бы вы её определили? Как описать / сравнить сложность моделей для двух приведённых ниже задач? Почему, кстати, мы решили, что средняя модель ОК, а правая переобученная?\n\n{: .left wight=1000px}\n\n{: .left wight=1000px}\n\n{% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details="Сложность модели можно очень грубо охарактеризовать числом настраиваемых параметров модели, то есть тех, которые мы можем определить по данным в процессе обучения. Это не имеет никакого математического смысла, и о каких-то более серьёзных оценках мы поговорим в главе про теорию машинного обучения, но никто не бросит в вас камень, если вы скажете, что модель с 10 тысячами параметров сложнее, чем модель с 1000 параметров.\n\nВ первой задаче левая модель — это, судя по всему, линейная функция, у неё два параметра, вторая — наверное, квадратичная с тремя параметрами, а правая — многочлен какой-то высокой степени (на самом деле 11-й), у неё параметров намного больше. Центральная модель явно лучше, чем левая, справляется с тем, чтобы приблизить истинную закономерность; правая тоже вроде неплохо справляется с тем, чтобы приблизить её для обучающих данных, но вот два резких провала и крутое пике слева никак не объясняются имеющимися данными, и на двух тестовых точках в районе $0,5$ модель отчаянно врёт — так что есть причины считать, что она переобучилась.\n\nСо второй задачей ситуация во многом похожая. Центральная модель явно лучше разделяет жёлтые и серые точки. На правой же картинке мы видим довольно неестественные выпячивания жёлтой и серой областей: например, к серой точке в центре картинки (которая наверняка была выбросом) протянулось серое «щупальце», захватившее и несколько тестовых (и даже обучающих) точек другого класса. В целом можно поспорить о том, плох ли правый классификатор, но он явно рисует слишком сложные границы, чтобы можно было поверить, что они отражают что-то из реальной жизни.\n\n%}\n\nЧто же такое модель\n\nВ курсе мы будем активно использовать слово «модель», и нужно заранее договориться о том, что мы имеем в виду, чтобы вам не казалось, что мы одним и тем же словом называем совсем разные вещи.\n\nМир, в котором мы живём, бесконечно сложен, и вряд ли у нас есть шанс хоть что-то измерить и предсказать совершенно точно, будь то форма Земли или вероятность того, что пользователь останется доволен поисковой выдачей. Но как-то работать нужно, и поэтому люди вводят (упрощённые) описания мироздания, которые и называют моделями.\n\nНапример, «Земля плоская» — это модель, и не такая плохая, как вам может показаться. Ей активно пользуются, когда всё происходит в масштабах одного города и кривизной поверхности можно пренебрегать. С другой стороны, если мы попробуем рассчитать кратчайший путь из Парижа в Лос-Анджелес, модель плоской Земли выдаст неадекватный ответ, она войдёт в противоречие с имеющимися данными, и её придётся заменить на «Земля круглая», «Земля имеет форму эллипсоида» и так далее — в той мере, в которой нам важна точность и в какой нам это позволяет (не)совершенство измерительной техники. Так, модель «Земля — это безымянная штука с шершавостями на месте горных хребтов» очень точная и замечательная, но, возможно, будет избыточно сложной для большинства практических задач и при этом слишком тяжёлой в плане вычислений.\n\nТо же самое верно и для машинного обучения. Если мы предсказываем цену квартиры, мы не можем учесть все на свете тонкости и сделать это совершенно точно. С одной стороны, нам фантазии не хватит, чтобы придумать все на свете факторы, влияющие на цену (да и вообще в природу на квантовом уровне внесена случайность). С другой стороны, данных у нас конечное количество, и, скажем, если мы даже знаем, что квартира, хозяин которой ходит с эльфийскими ушами и по ночам играет на арфе, продаётся за большие деньги, у нас просто нет другой квартиры, которая отличалась бы от этой лишь ушами хозяина, чтобы сделать хоть сколь-нибудь ответственный вывод о том, что именно уши повлияли на цену. Поэтому мы с самого начала должны примириться с тем, что мы ищем не истинную зависимость, а лишь приближённую — то есть строим модель. Например, такую: «цена квартиры линейно зависит от метража и логарифмически — от расстояния до ближайшего метро». Или такую: «цена квартиры линейно зависит от суммарного метража и как многочлен второй степени — от метража ванных комнат». Или вот такую: «в некоторых случаях цена квартиры линейно зависит от метража и логарифмически — от расстояния до ближайшего метро, а в некоторых она просто равна константе». В каждой модели у нас есть обучаемые (настраиваемые) параметры (в данном случае коэффициенты зависимостей, а у третьей модели ещё и правило, разделяющее одни случаи и другие), которые мы будем оптимизировать в ходе обучения. Кроме того, нам придётся научиться сравнивать разные модели.\n\nНо, как мы видели в примере с формой Земли, не все модели являются предсказательными. Например, мы можем задуматься о том, как устроена вся совокупность квартир (виденных нами, не виденных нами и не существующих пока, но правдоподобных). «Половина квартир — двухкомнатные» — это тоже модель, и она, как мы увидим, может оказаться полезной для построения уже предсказательной модели цены, равно как и модель «расстояние от квартиры до ближайшего метро имеет нормальное распределение». Больше того, и для цены мы можем давать неточный ответ, а что-то в духе «ну, цена этой квартиры заключена между такими-то значениями» или «цена этой квартиры имеет нормальное распределение с такими-то параметрами, то есть вот такое значение, конечно, вероятнее всего, но вообще мы не очень уверены, и вот так можно выразить нашу неуверенность». Модели данных вы можете применить и для того, чтобы генерировать новые объекты, — и если генерировать квартиры с ценами звучит как сомнительное занятие, то генерация осмысленного текста или аниме точно заслуживает право на существование. Наконец, понимание того, как устроены данные, может оказаться полезным для поиска аномалий: так, если какая-то квартира уж совсем никак не соответствует модели (скажем, если это землянка по цене в три раза выше средней по рынку или если в ней на две жилые комнаты 10 ванных комнат), у нас есть повод заподозрить шутников или мошенников (но в то же время это может означать, что мы просто недостаточно данных собрали и модель в процессе обучения не повидала подобного).\n\nВ следующих главах вы узнаете, что модели машинного обучения бывают очень разными, что они могут служить разным целям и к ним могут предъявляться нетривиальные требования.\n\nМодель vs алгоритм: в чём разница?\n\nИзучая машинное обучение, вы нередко будете сталкиваться со словами модель и алгоритм, и иногда вам будет казаться, что это одно и то же, а иногда нет. Сейчас мы опишем, в чём, на наш взгляд, состоит разница между ними, и будем придерживаться этого разделения дальше в учебнике. Впрочем, не все придерживаются тех же терминологических соглашений, так что будьте осмотрительны.\n\nКак уже было описано выше, модель — это некоторое суждение о мире вокруг нас. У модели могут быть настраиваемые параметры (например, коэффициенты линейной функции), и тогда нужен какой-то способ настроить их по конкретной обучающей выборке. Например, с помощью метода наименьших квадратов. Это и есть алгоритм (алгоритм обучения): инструкция, как по выборке обучить параметры модели. Естественно, для одной и той же модели может быть несколько различных алгоритмов обучения, и точно так же один и тот же алгоритм (например, уже упомянутый метод наименьших квадратов) может быть использован для обучения различных моделей (например, и для модели «цена квартиры линейно зависит от расстояния до метро», и для модели «цена квартиры логарифмически зависит от расстояния до метро»).'), Document(metadata={'source': '/Users/eugenekillevsky/PycharmProjects/LLM-Interviewer/data/RAG/ml-handbook-master/chapters/linear_models/intro.md'}, page_content='title: Линейные модели author: filipp_sinicin, evgenii_sokolov\n\nМы начнем с самых простых и понятных моделей машинного обучения: линейных. В этой главе мы разберёмся, что это такое, почему они работают и в каких случаях их стоит использовать. Так как это первый класс моделей, с которым вы столкнётесь, мы постараемся подробно проговорить все важные моменты. Заодно объясним, как работает машинное обучение, на сравнительно простых примерах.\n\nПочему модели линейные?\n\nПредставьте, что у вас есть множество объектов $\\mathbb{X}$, а вы хотели бы каждому объекту сопоставить какое-то значение. К примеру, у вас есть набор операций по банковской карте, а вы бы хотели, понять, какие из этих операций сделали мошенники. Если вы разделите все операции на два класса и нулём обозначите законные действия, а единицей мошеннические, то у вас получится простейшая задача классификации. Представьте другую ситуацию: у вас есть данные геологоразведки, по которым вы хотели бы оценить перспективы разных месторождений. В данном случае по набору геологических данных ваша модель будет, к примеру, оценивать потенциальную годовую доходность шахты. Это пример задачи регрессии. Числа, которым мы хотим сопоставить объекты из нашего множества иногда называют таргетами (от английского target).\n\nТаким образом, задачи классификации и регрессии можно сформулировать как поиск отображения из множества объектов $\\mathbb{X}$ в множество возможных таргетов.\n\nМатематически задачи можно описать так: - классификация: $$\\mathbb{X} \\to {0, 1, \\ldots, K}$$, где $0, \\ldots, K$ – номера классов, - регрессия: $\\mathbb{X} \\to \\mathbb{R}$.\n\nОчевидно, что просто сопоставить какие-то объекты каким-то числам — дело довольно бессмысленное. Мы же хотим быстро обнаруживать мошенников или принимать решение, где строить шахту. Значит нам нужен какой-то критерий качества. Мы бы хотели найти такое отображение, которое лучше всего приближает истинное соответствие между объектами и таргетами. Что значит <<лучше всего>> – вопрос сложный. Мы к нему будем много раз возвращаться. Однако, есть более простой вопрос: среди каких отображений мы будем искать самое лучшее? Возможных отображений может быть много, но мы можем упростить себе задачу и договориться, что хотим искать решение только в каком-то заранее заданном параметризированном семействе функций. Вся эта глава будет посвящена самому простому такому семейству — линейным функциям вида\n\n$$ y = w_1 x_1 + \\ldots + w_D x_D + w_0, $$\n\nгде $y$ – целевая переменная (таргет), $(x_1, \\ldots, x_D)$ – вектор, соответствующий объекту выборки (вектор признаков), а $w_1, \\ldots, w_D, w_0$ – параметры модели. Признаки ещё называют фичами (от английского features). Вектор $w = (w_1,\\ldots,w_D)$ часто называют вектором весов, так как на предсказание модели можно смотреть как на взвешенную сумму признаков объекта, а число $w_0$ – свободным коэффициентом, или сдвигом (bias). Более компактно линейную модель можно записать в виде\n\n$$y = \\langle x, w\\rangle + w_0$$\n\nТеперь, когда мы выбрали семейство функций, в котором будем искать решение, задача стала существенно проще. Мы теперь ищем не какое-то абстрактное отображение, а конкретный вектор $(w_0,w_1,\\ldots,w_D)\\in\\mathbb{R}^{D+1}$.\n\nЗамечание. Чтобы применять линейную модель, нужно, чтобы каждый объект уже был представлен вектором численных признаков $x_1,\\ldots,x_D$. Конечно, просто текст или граф в линейную модель не положить, придётся сначала придумать для него численные фичи. Модель называют линейной, если она является линейной по этим численным признакам.\n\nРазберёмся, как будет работать такая модель в случае, если $$D = 1$$. То есть у наших объектов есть ровно один численный признак, по которому они отличаются. Теперь наша линейная модель будет выглядеть совсем просто: $$y = w_1 x_1 + w_0$$. Для задачи регрессии мы теперь пытаемся приблизить значение игрек какой-то линейной функцией от переменной икс. А что будет значить линейность для задачи классификации? Давайте вспомним про пример с поиском мошеннических транзакций по каратам. Допустим, нам известна ровно одна численная переменная — объём транзакции. Для бинарной классификации транзакций на законные и потенциально мошеннические мы будем искать так называемое разделяющее правило: там, где значение функции положительно, мы будем предсказывать один класс, где отрицательно – другой. В нашем примере простейшим правилом будет какое-то пороговое значение объёма транзакций, после которого есть смысл пометить транзакцию как подозрительную.\n\n{: .left}\n\nВ случае более высоких размерностей вместо прямой будет гиперплоскость с аналогичным смыслом.\n\nВопрос на подумать. Если вы посмотрите содержание учебника, то не найдёте в нём ни <<полиномиальных>> моделей, ни каких-нибудь <<логарифмических>>, хотя, казалось бы, зависимости бывают довольно сложными. Почему так?\n\n{% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details="Линейные зависимости не так просты, как кажется. Пусть мы решаем задачу регрессии. Если мы подозреваем, что целевая переменная $$y$$ не выражается через $$x_1, x_2$$ как линейная функция, а зависит ещё от логарифма $$x_1$$ и ещё как-нибудь от того, разные ли знаки у признаков, то мы можем ввести дополнительные слагаемые в нашу линейную зависимость, просто объявим эти слагаемые новыми переменными, и добавив перед ними соответствующие регрессионные коэффициенты\n\n$$y \\approx w_1 x_1 + w_2 x_2 + w_3\\log{x_1} + w_4\\text{sgn}(x_1x_2) + w_0,$$\n\nи в итоге из двумерной нелинейной задачи мы получили четырёхмерную линейную регрессию." %}\n\nВопрос на подумать. А как быть, если одна из фичей является категориальной, то есть принимает значения из (обычно конечного числа) значений, не являющихся числами? Например, это может быть время года, уровень образования, марка машины и так далее. Как правило, с такими значениями невозможно производить арифметические операции или же результаты их применения не имеют смысла.\n\n{% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details=" В линейную модель можно подать только численные признаки, так что категориальную фичу придётся как-то закодировать. Рассмотрим для примера вот такой датасет\n\n{: .left}\n\nЗдесь два категориальных признака – pet_type и color. Первый принимает четыре различных значения, второй – пять.\n\nСамый простой способ – использовать one-hot кодирование (one-hot encoding). Пусть исходный признак мог принимать $M$ значений $c_1,\\ldots, c_M$. Давайте заменим категориальный признак на $M$ признаков, которые принимают значения $0$ и $1$: $i$-й будет отвечать на вопрос <<принимает ли признак значение $c_i$?>>. Иными словами, вместо ячейки со значением $c_i$ у объекта появляется строка нулей и единиц, в которой единица стоит только на $i$-м месте.\n\nВ нашем примере получится вот такая табличка:\n\n{: .left}\n\nМожно было бы на этом остановится, но добавленные признаки обладают одним неприятным свойством: в каждом из них ровно одна единица, так что сумма соответствующих столбцов равна столбцу из единиц. А это уже плохо. Представьте, что у нас есть линейная модель\n\n$$y \\sim w_1x_1 + \\ldots + w_{D-1}x_{d-1} + w_{c_1}x_{c_1} + \\ldots + w_{c_M}x_{c_M} + w_0$$\n\nПреобразуем немного правую часть:\n\n$$y\\sim w_1x_1 + \\ldots + w_{D-1}x_{d-1} + \\underbrace{(w_{c_1} - w_{c_N})}{=:w\'{c_1}}x_{c_1} + \\ldots + \\underbrace{(w_{c_{M-1}} - w_{c_N})}{=:w\'{C_{N-1}}}x_{c_{M-1}} + w_{c_N}\\underbrace{(x_{c_1} + \\ldots + x_{c_M})}_{=1} + w_0 = $$\n\n$$ = w_1x_1 + \\ldots + w_{D-1}x_{d-1} + w\'{c_1}x{c_1} + \\ldots + w\'{c{M-1}}x_{c_{M-1}} + (w_{c_N} + w_0){=w\'{0}}$$\n\nКак видим, от одного из новых признаков можно избавиться, не меняя модель. Больше того, это стоит сделать, потому что наличие <<лишних>> признаков ведёт к переобучению или вовсе ломает модель – подробнее об этом мы поговорим в разделе про регуляризацию. Поэтому при использовании one-hot-encoding обычно выкидывают признак, соответствующий одному из значений. Например, в нашем примере итоговая матрица объекты-признаки будет иметь вид:\n\n{: .left}\n\nКонечно, one-hot кодирование – это самый наивный способ работы с категориальными признаками, и для более сложных фичей или фичей с большим количеством значений оно плохо подходит. С рядом более продвинутых техник вы познакомитесь в разделе про обучение представлений. " %}\n\nПомимо простоты, у линейных моделей есть несколько других достоинств. К примеру, мы можем достаточно легко судить, как влияют на результат те или иные признаки. Скажем, если вес $w_i$ положителен, то с ростом $i$-го признака таргет в случае регрессии будет увеличиваться, а в случае классификации наш выбор будет сдвигаться в пользу одного из классов. Значение весов тоже имеет прозрачную интерпретацию: чем вес $w_i$ больше, тем <<важнее>> $i$-й признак для итогового предсказания. То есть, если вы построили линейную модель, вы неплохо можете объяснить заказчику те или иные её результаты. Это качество моделей называют интерпретируемостью. Оно особенно ценится в индустриальных задачах, цена ошибки в которых высока. Если от работы вашей модели может зависеть жизнь человека, то очень важно понимать, как модель принимает те или иные решения и какими принципами руководствуется. При этом, не все методы машинного обучения хорошо интерпретируемы, к примеру, поведение искусственных нейронных сетей или градиентного бустинга интерпретировать довольно сложно.\n\nВ то же время слепо доверять весам линейных моделей тоже не стоит по целому ряду причин:\n\nЛинейные модели всё-таки довольно узкий класс функций, они неплохо работают для небольших датасетов и простых задач. Однако, если вы решаете линейной моделью более сложную задачу, то вам, скорее всего, придётся выдумывать дополнительные признаки, являющиеся сложными функциями от исходных. Поиск таких дополнительных признаков называется feature engineering, технически он устроен примерно так, как мы описали в вопросе про "полиномиальные модели". Вот только поиском таких искусственных фичей можно сильно увлечься, так что осмысленность интерпретации будет сильно зависеть от здравого смысла эксперта, строившего модель.\n\nЕсли между признаками есть приближённая линейная зависимость, коэффициенты в линейной модели могут совершенно потерять физический смысл (об этой проблеме и о том, как с ней бороться, мы поговорим дальше, когда будем обсуждать регуляризацию).\n\nОсобенно осторожно стоит верить в утверждения вида <<этот коэффициент маленький, значит, этот признак не важен>>. Во-первых, всё зависит от масштаба признака: вдруг коэффициент мал, чтобы скомпенсировать его. Во-вторых, зависимость действительно может быть слабой, но кто знает, в какой ситуации она окажется важна. Такие решения принимаются на основе данных, например, путём проверки статистического критерия (об этом мы коротко упомянем в разделе по вероятностные модели).\n\nКонкретные значения весов могут меняться в зависимости от обучающей выборки, хотя с ростом её размера они будут потихоньку сходиться к весам <<наилучшей>> линейной модели, которую можно было бы построить по всем-всем-всем данным на свете.\n\nОбсудив немного общие свойства линейных моделей, перейдём к тому, как их всё-таки обучать. Сначала разберёмся с регрессией, а затем настанет черёд классификации.\n\nЛинейная регрессия и метод наименьших квадратов (МНК)\n\nМы начнём с использования линейных моделей для решения задачи регрессии. Простейшим примером постановки задачи линейной регрессии является метод наименьших квадратов (Ordinary least squares).\n\nПусть у нас задан датасет $(X, y)$, где $$y=(y_i){i=1}^N \\in \\mathbb{R}^N$$ – вектор значений целевой переменной, а $$X=(x_i){i = 1}^N \\in \\mathbb{R}^{N \\times D}, x_i \\in \\mathbb{R}^D$$ – матрица объекты-признаки, в которой $i$-я строка – это вектор признаков $i$-го объекта выборки. Мы хотим моделировать зависимость $$y_i$$ от $$x_i$$ как линейную функцию со свободным членом. Общий вид такой функции из $$\\mathbb{R}^D$$ в $$\\mathbb{R}$$ выглядит следующим образом:\n\n$$\\color{#348FEA}{f_w(x_i) = \\langle w, x_i \\rangle + w_0}$$\n\nСвободный член $$w_0$$ часто опускают, потому что такого же результата можно добиться, добавив ко всем $$x_i$$ признак, тождественно равный единице; тогда роль свободного члена будет играть соответствующий ему вес:\n\n$$\\begin{pmatrix}x_{i1} & \\ldots & x_{iD} \\end{pmatrix}\\cdot\\begin{pmatrix}w_1\\ \\vdots \\ w_D\\end{pmatrix} + w_0 = \\begin{pmatrix}1 & x_{i1} & \\ldots & x_{iD} \\end{pmatrix}\\cdot\\begin{pmatrix}w_0 \\ w_1\\ \\vdots \\ w_D \\end{pmatrix}$$\n\nПоскольку это сильно упрощает запись, в дальнейшем мы будем считать, что это уже сделано и зависимость имеет вид просто $$f_w(x_i) = \\langle w, x_i \\rangle$$.\n\nСведение к задаче оптимизации\n\nМы хотим, чтобы на нашем датасете (то есть на парах $$(x_i, y_i)$$ из обучающей выборки) функция $$f_w$$ как можно лучше приближала нашу зависимость.\n\n{: .left}\n\nДля того, чтобы чётко сформулировать задачу, нам осталось только одно: на математическом языке выразить желание <<приблизить $$f_w(x)$$ к $$y$$>>. Говоря простым языком, мы должны научиться измерять качество модели и минимизировать её ошибку, как-то меняя обучаемые параметры. В нашем примере обучаемые параметры — это веса $$w$$. Функция, оценивающая то, как часто модель ошибается, традиционно называется функцией потерь, функционалом качества или просто лоссом (loss function). Важно, чтобы её было легко оптимизировать: скажем, гладкая функция потерь – это хорошо, а кусочно постоянная – просто ужасно.\n\nФункции потерь бывают разными. От их выбора зависит то, насколько задачу в дальнейшем легко решать, и то, в каком смысле у нас получится приблизить предсказание модели к целевым значениям. Интуитивно понятно, что для нашей текущей задачи нам нужно взять вектор $$y$$ и вектор предсказаний модели и как-то сравнить, насколько они похожи. Так как эти вектора <<живут>> в одном векторном пространстве, расстояние между ними вполне может быть функцией потерь. Более того, положительная непрерывная функция от этого расстояния тоже подойдёт в качестве функции потерь. При этом способов задать расстояние между векторами тоже довольно много. От всего этого разнообразия глаза разбегаются, но мы обязательно поговорим про это позже. Сейчас давайте в качестве лосса возьмём квадрат $$L^2$$-нормы вектора разницы предсказаний модели и $$y$$. Во-первых, как мы увидим дальше, так задачу будет нетрудно решить, а во-вторых, у этого лосса есть ещё несколько дополнительных свойств:\n\n$$L^2$$-норма разницы – это евклидово расстояние $$|y - f_w(x)|_2$$ между вектором таргетов и вектором ответов модели, то есть мы их приближаем в смысле самого простого и понятного <<расстояния>>.\n\nКак мы увидим в разделе про вероятностные модели, с точки зрения статистики это соответствует гипотезе о том, что наши данные состоят из линейного <<сигнала>> и нормально распределенного <<шума>>.\n\nТак вот, наша функция потерь выглядит так:\n\n$$L(f, X, y) = |y - f(X)|_2^2 = $$\n\n$$= |y - Xw|2^2 = \\sum{i=1}^N(y_i - \\langle x_i, w \\rangle)^2$$\n\nТакой функционал ошибки не очень хорош для сравнения поведения моделей на выборках разного размера. Представьте, что вы хотите понять, насколько качество модели на тестовой выборке из $2500$ объектов хуже, чем на обучающей из $5000$ объектов. Вы измерили $$L^2$$-норму ошибки и получили в одном случае $300$, а в другом $500$. Эти числа не очень интерпретируемы. Гораздо лучше посмотреть на среднеквадратичное отклонение\n\n$$L(f, X, y) = \\frac1N\\sum_{i=1}^N(y_i - \\langle x_i, w \\rangle)^2$$\n\nПо этой метрике на тестовой выборке получаем $$0,12$$, а на обучающей $$0,1$$.\n\nФункция потерь $$\\frac1N\\sum_{i=1}^N(y_i - \\langle x_i, w \\rangle)^2$$ называется Mean Squared Error, MSE или среднеквадратическим отклонением. Разница с $$L^2$$-нормой чисто косметическая, на алгоритм решения задачи она не влияет:\n\n$$\\color{#348FEA}{\\text{MSE}(f, X, y) = \\frac{1}{N}|y - X w|_2^2}$$\n\nВ самом широком смысле, функции работают с объектами множеств: берут какой-то входящий объект из одного множества и выдают на выходе соответствующий ему объект из другого. Если мы имеем дело с отображением, которое на вход принимает функции, а на выходе выдаёт число, то такое отображение называют функционалом. Если вы посмотрите на нашу функцию потерь, то увидите, что это именно функционал. Для каждой конкретной линейной функции, которую задают веса $$w_i$$, мы получаем число, которое оценивает, насколько точно эта функция приближает наши значения $$y$$. Чем меньше это число, тем точнее наше решение, значит для того, чтобы найти лучшую модель, этот функционал нам надо минимизировать по $$w$$:\n\n$$\\color{#348FEA}{|y - Xw|_2^2 \\longrightarrow \\min_w}$$\n\nЭту задачу можно решать разными способами. В этой главе мы сначала решим эту задачу аналитически, а потом приближенно. Сравнение двух этих решений позволит нам проиллюстрировать преимущества того подхода, которому посвящена эта книга. На наш взгляд, это самый простой способ "на пальцах" показать суть машинного обучения.\n\nМНК: точный аналитический метод\n\nТочку минимума можно найти разными способами. Если вам интересно аналитическое решение, вы можете найти его в главе про матричные дифференцирования (раздел <<Примеры вычисления производных сложных функций>>). Здесь же мы воспользуемся геометрическим подходом.\n\nПусть $$x^{(1)},\\ldots,x^{(D)}$$ – столбцы матрицы $$X$$, то есть столбцы признаков. Тогда\n\n$$Xw = w_1x^{(1)}+\\ldots+w_Dx^{(D)},$$\n\nи задачу регрессии можно сформулировать следующим образом: найти линейную комбинацию столбцов $$x^{(1)},\\ldots,x^{(D)}$$, которая наилучшим способом приближает столбец $$y$$ по евклидовой норме – то есть найти проекцию вектора $$y$$ на подпространство, образованное векторами $$x^{(1)},\\ldots,x^{(D)}$$.\n\nРазложим $$y = y_{\\parallel} + y_{\\perp}$$, где $$y_{\\parallel} = Xw$$ – та самая проекция, а $$y_{\\perp}$$ – ортогональная составляющая, то есть $$y_{\\perp} = y - Xw\\perp x^{(1)},\\ldots,x^{(D)}$$. Как это можно выразить в матричном виде? Оказывается, очень просто:\n\n$$X^T(y - Xw) = 0$$\n\nВ самом деле, каждый элемент столбца $$X^T(y - Xw)$$ – это скалярное произведение строки $$X^T$$ (=столбца $$X$$ = одного из $$x^{(i)}$$) на $$y - Xw$$. Из уравнения $$X^T(y - Xw) = 0$$ уже очень легко выразить $$w$$:\n\n$$w = (X^TX)^{-1}X^Ty$$\n\nВопрос на подумать Для вычисления $w_{\\ast}$ нам приходится обращать (квадратную) матрицу $X^TX$, что возможно, только если она невырожденна. Что это значит с точки зрения анализа данных? Почему мы верим, что это выполняется во всех разумных ситуациях?\n\n{% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details="Как известно из линейной алгебры, для вещественной матрицы $X$ ранги матриц $X$ и $X^TX$ совпадают. Матрица $X^TX$ невырожденна тогда и только тогда, когда её ранг равен числу её столбцов, что равно числу столбцов матрицы $X$. Иными словами, формула регрессии поломается, только если столбцы матрицы $X$ линейно зависимы. Столбцы матрицы $X$ – это признаки. А если наши признаки линейно зависимы, то, наверное, что-то идёт не так и мы должны выкинуть часть из них, чтобы остались только линейно независимые.\n\nДругое дело, что зачастую признаки могут быть приближённо линейно зависимы, особенно если их много. Тогда матрица $X^TX$ будет близка к вырожденной, и это, как мы дальше увидим, будет вести к разным, в том числе вычислительным проблемам." %}\n\nВычислительная сложность аналитического решения – $$O(ND^2 + D^3)$$, где, $N$ – длина выборки, $D$ – число признаков у одного объекта. Слагаемое $$ND^2$$ отвечает за сложность перемножения матриц $$X^T$$ и $$X$$, а слагаемое $$D^3$$ – за сложность обращения их произведения. Перемножать матрицы $$(X^TX)^{-1}$$ и $$X^T$$ не стоит. Гораздо лучше сначала умножить $$y$$ на $$X^T$$, а затем полученный вектор на $$(X^TX)^{-1}$$: так будет быстрее и, кроме того, не нужно будет хранить матрицу $$(X^TX)^{-1}X^T$$.\n\nВычисление можно ускорить, используя продвинутые алгоритмы перемножения матриц или итерационные методы поиска обратной матрицы.\n\nПроблемы <<точного>> решения\n\nЗаметим, что для получения ответа нам нужно обратить матрицу $$X^TX$$. Это создает множество проблем: 1. Основная проблема в обращении матрицы — это то, что вычислительно обращать большие матрицы дело сложное, а мы бы хотели работать с датасетами, в которых у нас могут быть миллионы точек, 2. Матрица $$X^TX$$, хотя почти всегда обратима в разумных задачах машинного обучения, зачастую плохо обусловлена. Особенно если признаков много, между ними может появляться приближённая линейная зависимость, которую мы можем упустить на этапе формулировки задачи. В подобных случаях погрешность нахождения $w$ будет зависеть от квадрата числа обусловленности матрицы $X$, что очень плохо. Это делает полученное таким образом решение численно неустойчивым: малые возмущения $$y$$ могут приводить к катастрофическим изменениям $$w$$.\n\n{% include details.html summary="Пара слов про число обусловленности." details=" Пожертвовав математической строгостью, мы можем считать, что число обусловленности матрицы $X$ – это корень из отношения наибольшего и наименьшего из собственных чисел матрицы $X^TX$. Грубо говоря, оно показывает, насколько разного масштаба бывают собственные значения $X^TX$. Если рассмотреть $L^2$-норму ошибки предсказания, как функцию от $w$, то её линии уровня будут эллипсоидами, форма которых определяется квадратичной формой с матрицей $X^TX$ (проверьте это!). Таким образом, число обусловленности говорит о том, насколько вытянутыми являются эти эллипсоиды. " %}\n\n{% include details.html summary="Данные проблемы не являются поводом выбросить решение на помойку. Существует как минимум два способа улучшить его численные свойства, однако если вы не знаете про сингулярное разложение, то лучше вернитесь сюда, когда узнаете." details=" 1. Построим $$QR$$-разложение матрицы $$X$$. Напомним, что это разложение, в котором матрица $$Q$$ ортогональна по столбцам (то есть её столбцы ортогональны и имеют длину 1; в частности, $$Q^TQ=E$$), а $$R$$ квадратная и верхнетреугольная. Подставив его в формулу, получим\n\n  $$w = ((QR)^TQR)^{-1}(QR)^T y = (R^T\\underbrace{Q^TQ}_{=E}R)^{-1}R^TQ^Ty = R^{-1}R^{-T}R^TQ^Ty = R^{-1}Q^Ty$$\n\n  Отметим, что написать $$(R^TR)^{-1} = R^{-1}R^{-T}$$ мы имеем право благодаря тому, что $$R$$ квадратная. Полученная формула намного проще, обращение верхнетреугольной матрицы (=решение системы с верхнетреугольной левой частью) производится быстро и хорошо, погрешность вычисления $w$ будет зависеть просто от числа обусловленности матрицы $X$, а поскольку нахождение $QR$-разложения является достаточно стабильной операцией, мы получаем решение с более хорошими, чем у исходной формулы, численными свойствами.\n\nТакже можно использовать псевдообратную матрицу, построенную с помощью сингулярного разложения, о котором подробно написано в разделе про матричные разложения. А именно, пусть\n\n$$A = U\\underbrace{\\mathrm{diag}(\\sigma_1,\\ldots,\\sigma_r)}_{=\\Sigma}V^T$$\n\n– это усечённое сингулярное разложение, где $r$ – это ранг $A$. В таком случае диагональная матрица посередине является квадратной, $U$ и $V$ ортогональны по столбцам: $U^TU = E$, $V^TV = E$. Тогда\n\n$$w = (V\\Sigma \\underbrace{U^TU}_{=E}\\Sigma V^T)^{-1}V\\Sigma U^Ty$$\n\nЗаметим, что $$V\\Sigma^{-2}V^T\\cdot V\\Sigma^2V^T = E = V\\Sigma^2V^T\\cdot V\\Sigma^{-2}V^T$$, так что $$(V\\Sigma^2 V^T)^{-1} = V\\Sigma^{-2}V^T$$, откуда\n\n$$w = V\\Sigma^{-2}\\underbrace{V^TV}_{=E}V^T\\cdot V\\Sigma U^Ty = V\\Sigma^{-1}Uy$$\n\nХорошие численные свойства сингулярного разложения позволяют утверждать, что и это решение ведёт себя довольно неплохо.\n\nТем не менее, вычисление всё равно остаётся довольно долгим и будет по-прежнему страдать (хоть и не так сильно) в случае плохой обусловленности матрицы $$X$$. " %}\n\nПолностью вылечить проблемы мы не сможем, но никто и не обязывает нас останавливаться на <<точном>> решении (которое всё равно никогда не будет вполне точным). Поэтому ниже мы познакомим вас с совершенно другим методом.\n\nМНК: приближенный численный метод\n\nМинимизируемый функционал является гладким и выпуклым, а это значит, что можно эффективно искать точку его минимума с помощью итеративных градиентных методов. Более подробно вы можете прочитать о них в разделе про методы оптимизации, а здесь мы лишь коротко расскажем об одном самом базовом подходе.\n\nКак известно, градиент функции в точке направлен в сторону её наискорейшего роста, а антиградиент (противоположный градиенту вектор) в сторону наискорейшего убывания. То есть имея какое-то приближение оптимального значения параметра $$w$$, мы можем его улучшить, посчитав градиент функции потерь в точке и немного сдвинув вектор весов в направлении антиградиента:\n\n$$w_j \\mapsto w_j - \\alpha \\frac{d}{d{w_j}} L(f_w, X, y) $$\n\nгде $$\\alpha$$ – это параметр алгоритма (<<темп обучения>>), который контролирует величину шага в направлении антиградиента. Описанный алгоритм называется градиентным спуском.\n\nПосмотрим, как будет выглядеть градиентный спуск для функции потерь $L(f_w, X, y) = \\frac1N\\vert\\vert Xw - y\\vert\\vert^2$. Градиент квадрата евклидовой нормы мы уже считали; соответственно,\n\n$$ \\nabla_wL = \\frac2{N} X^T (Xw - y) $$\n\nСледовательно, стартовав из какого-то начального приближения, мы можем итеративно уменьшать значение функции, пока не сойдёмся (по крайней мере в теории) к минимуму (вообще говоря, локальному, но в данном случае глобальному).\n\nАлгоритм градиентного спуска\n\npython w = random_normal() # можно пробовать и другие виды инициализации repeat S times: # другой вариант: while abs(err) > tolerance f = X.dot(w) # посчитать предсказание err = f - y # посчитать ошибку grad = 2 * X.T.dot(err) / N # посчитать градиент w -= alpha * grad # обновить веса\n\nС теоретическими результатами о скорости и гарантиях сходимости градиентного спуска вы можете познакомиться в главе про методы оптимизации. Мы позволим себе лишь несколько общих замечаний:\n\nПоскольку задача выпуклая, выбор начальной точки влияет на скорость сходимости, но не настолько сильно, чтобы на практике нельзя было стартовать всегда из нуля или из любой другой приятной вам точки;\n\nЧисло обусловленности матрицы $X$ существенно влияет на скорость сходимости градиентного спуска: чем более вытянуты эллипсоиды уровня функции потерь, тем хуже;\n\nТемп обучения $\\alpha$ тоже сильно влияет на поведение градиентного спуска; вообще говоря, он является гиперпараметром алгоритма, и его, возможно, придётся подбирать отдельно. Другими гиперпараметрами являются максимальное число итераций $S$ и/или порог tolerance.\n\n{% include details.html summary="Иллюстрация." details=" Рассмотрим три задачи регрессии, для которых матрица $X$ имеет соответственно маленькое, среднее и большое числа обусловленности. Будем строить для них модели вида $y=w_1x_1 + w_2x_2$. Раскрасим плоскость $(w_1, w_2)$ в соответствии со значениями $|X_{\\text{train}}w - y_{\\text{train}}|^2$. Тёмная область содержит минимум этой функции – оптимальное значение $w_{\\ast}$. Также запустим из из двух точек градиентный спуск с разными значениями темпа обучения $\\alpha$ и посмотрим, что получится:\n\n{: .left} Заголовки графиков (\\"Round\\", \\"Elliptic\\", \\"Stripe-like\\") относятся к форме линий уровня потерь (чем более они вытянуты, тем хуже обусловлена задача и тем хуже может вести себя градиентный спуск).\n\nИтог: при неудачном выборе $\\alpha$ алгоритм не сходится или идёт вразнос, а для плохо обусловленной задачи он сходится абы куда. " %}\n\nВычислительная сложность градиентного спуска – $O(NDS)$, где, как и выше, $N$ – длина выборки, $D$ – число признаков у одного объекта. Сравните с оценкой $$O(N^2D + D^3)$$ для <<наивного>> вычисления аналитического решения.\n\nСложность по памяти – $O(ND)$ на хранение выборки. В памяти мы держим и выборку, и градиент, но в большинстве реалистичных сценариев доминирует выборка.\n\nСтохастический градиентный спуск\n\nНа каждом шаге градиентного спуска нам требуется выполнить потенциально дорогую операцию вычисления градиента по всей выборке (сложность $$O(ND)$$). Возникает идея заменить градиент его оценкой на подвыборке (в английской литературе такую подвыборку обычно именуют batch или mini-batch; в русской разговорной терминологии тоже часто встречается слово батч или мини-батч).\n\nА именно, если функция потерь имеет вид суммы по отдельным парам объект-таргет\n\n$$L(w, X, y) = \\frac1N\\sum_{i=1}^NL(w, x_i, y_i),$$\n\nа градиент, соответственно, записывается в виде\n\n$$\\nabla_wL(w, X, y) = \\frac1N\\sum_{i=1}^N\\nabla_wL(w, x_i, y_i),$$\n\nто предлагается брать оценку\n\n$$\\nabla_wL(w, X, y) \\approx \\frac1B\\sum_{t=1}^B\\nabla_wL(w, x_{i_t}, y_{i_t})$$\n\nдля некоторого подмножества этих пар $(x_{i_t}, y_{i_t})_{t=1}^B$. Обратите внимание на множители $\\frac1N$ и $\\frac1B$ перед суммами. Почему они нужны? Полный градиент $\\nabla_wL(w, X, y)$ можно воспринимать как среднее градиентов по всем объектам, то есть как оценку матожидания $\\mathbb{E}\\nabla_wL(w, x, y)$; тогда, конечно, оценка матожидания по меньшей подвыборке тоже будет иметь вид среднего градиентов по объектам этой подвыборки.\n\nКак делить выборку на батчи? Ясно, что можно было бы случайным образом сэмплировать их из полного датасета, но даже если использовать быстрый алгоритм вроде резервуарного сэмплирования, сложность этой операции не самая оптимальная. Поэтому используют линейный проход по выборке (которую перед этим лучше всё-таки случайным образом перемешать). Давайте введём ещё один параметр нашего алгоритма: размер батча, который мы обозначим $$B$$. Теперь на $$B$$ очередных примерах вычислим градиент и обновим веса модели. При этом вместо количества шагов алгоритма обычно задают количество эпох $$E$$. Это ещё один гиперпараметр. Одна эпоха – это один полный проход нашего сэмплера по выборке. Заметим, что если выборка очень большая, а модель компактная, то даже первый проход бывает можно не заканчивать.\n\nАлгоритм: ```python w = normal(0, 1) repeat E times: for i = B, i <= n, i += B X_batch = X[i-B : i] y_batch = y[i-B : i] f = X_batch.dot(w) # посчитать предсказание err = f - y_batch # посчитать ошибку grad = 2 * X_batch.T.dot(err) / B # посчитать градиент w -= alpha * grad\n\n```\n\nСложность по времени – $$O(NDE)$$. На первый взгляд, она такая же, как и у обычного градиентного спуска, но заметим, что мы сделали в $$N / B$$ раз больше шагов, то есть веса модели претерпели намного больше обновлений.\n\nСложность по памяти можно довести до $$O(BD)$$: ведь теперь всю выборку не надо держать в памяти, а достаточно загружать лишь текущий батч (а остальная выборка может лежать на диске, что удобно, так как в реальности задачи, в которых выборка целиком не влезает в оперативную память, встречаются сплошь и рядом). Заметим, впрочем, что при этом лучше бы $$B$$ взять побольше: ведь чтение с диска – намного более затратная по времени операция, чем чтение из оперативной памяти.\n\nВ целом, разницу между алгоритмами можно представлять как-то так: {: .left}\n\nШаги стохастического градиентного спуска заметно более шумные, но считать их получается значительно быстрее. В итоге они тоже сходятся к оптимальному значению из-за того, что матожидание оценки градиента на батче равно самому градиенту. По крайней мере, сходимость можно получить при хорошо подобранных коэффициентах темпа обучения в случае выпуклого функционала качества. Подробнее мы об этом поговорим в главе про оптимизацию. Для сложных моделей и лоссов стохастический градиентный спуск может сходиться плохо или застревать в локальных минимумах, поэтому придумано множество его улучшений. О некоторых из них также рассказано в главе про оптимизацию.\n\nСуществует определённая терминологическая путаница, иногда стохастическим градиентным спуском называют версию алогоритма, в которой размер батча равен единице (то есть максимально шумная и быстрая версия алгоритма), а версии с бОльшим размером батча называют batch gradient descent. В книгах, которые, возможно, старше вас, такая процедура иногда ещё называется incremental gradient descent. Это не очень принципиально, но вы будьте готовы, если что.\n\nВопрос на подумать. Вообще говоря, если объём данных не слишком велик и позволяет это сделать, объекты лучше случайным образом перемешивать перед тем, как подавать их в алгоритм стохастического градиентного спуска. Как вам кажется, почему?\n\nТакже можно использовать различные стратегии отбора объектов. Например, чаще брать объекты, на которых ошибка больше. Какие ещё стратегии вы могли бы придумать?\n\n{% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details=" Легко представить себе ситуацию, в которой объекты как-нибудь неудачно упорядочены, скажем, по возрастанию таргета. Тогда модель будет попеременно то запоминать, что все таргеты маленькие, то – что все таргеты большие. Это может и не повлиять на качество итоговой модели, но может привести и к довольно печальным последствиям. И вообще, чем более разнообразные батчи модель увидит в процессе обучения, тем лучше.\n\nСтратегий можно придумать много. Например, не брать объекты, на которых ошибка слишком большая (возможно, это выбросы – зачем на них учиться), или вообще не брать те, на которых ошибка достаточно мала (они <<ничему не учат>>). Рекомендуем, впрочем, прибегать к этим эвристикам, только если вы понимаете, зачем они вам нужны и почему есть надежда, что они помогут. " %}\n\nНеградиентные методы\n\nПосле прочтения этой главы у вас может сложиться ощущение, что приближённые способы решения ML задач и градиентные методы – это одно и тоже, но вы будете правы в этом только на 98%. В принципе, существуют и другие способы численно решать эти задачи, но в общем случае они работают гораздо хуже, чем градиентный спуск, и не обладают таким хорошим теоретическим обоснованием. Мы не будем рассказывать про них подробно, но можете на досуге почитать, скажем, про Stepwise regression, Orthogonal matching pursuit или LARS. У LARS, кстати, есть довольно интересное свойство: он может эффективно работать на выборках, в которых число признаков больше числа примеров. С алгоритмом LARS вы можете познакомиться в главе про оптимизацию.\n\nРегуляризация\n\nВсегда ли решение задачи регрессии единственно? Вообще говоря, нет. Так, если в выборке два признака будут линейно зависимы (и следовательно, ранг матрицы будет меньше $$D$$), то гарантировано найдётся такой вектор весов $$\\nu$$ что $$\\langle\\nu, x_i\\rangle = 0\\ \\ \\forall x_i$$. В этом случае, если какой-то $$w$$ является решением оптимизационной задачи, то и $$w + \\alpha \\nu $$ тоже является решением для любого $$\\alpha$$. То есть решение не только не обязано быть уникальным, так ещё может быть сколь угодно большим по модулю. Это создаёт вычислительные трудности. Малые погрешности признаков сильно возрастают при предсказании ответа, а в градиентном спуске накапливается погрешность из-за операций со слишком большими числами.\n\nКонечно, в жизни редко бывает так, что признаки строго линейно зависимы, а вот быть приближённо линейно зависимыми они вполне могут быть. Такая ситуация называется мультиколлинеарностью. В этом случае у нас, всё равно, возникают проблемы, близкие к описанным выше. Дело в том, что $$X\\nu\\sim 0$$ для вектора $\\nu$, состоящего из коэффициентов приближённой линейной зависимости, и, соответственно, $$X^TX\\nu\\approx 0$$, то есть матрица $$X^TX$$ снова будет близка к вырожденной. Как и любая симметричная матрица, она диагонализуется в некотором ортонормированном базисе, и некоторые из собственных значений $$\\lambda_i$$ близки к нулю. Если вектор $$X^Ty$$ в выражении $$(X^TX)^{-1}X^Ty$$ будет близким к соответствующему собственному вектору, то он будет умножаться на $$1 /{\\lambda_i}$$, что опять же приведёт к появлению у $$w$$ очень больших по модулю компонент (при этом $$w$$ ещё и будет вычислен с большой погрешностью из-за деления на маленькое число). И, конечно же, все ошибки и весь шум, которые имелись в матрице $$X$$ при вычислении $$y\\sim Xw$$ будут умножаться на эти большие и неточные числа и возрастать во много-много раз, что приведёт к проблемам, от которых нас не спасёт никакое сингулярное разложение.\n\nВажно ещё отметить, что в случае, когда несколько признаков линейно зависимы, веса $w_i$ при них теряют физический смысл. Может даже оказаться, что вес признака, с ростом которого таргет, казалось бы, должен увеличиваться, станет отрицательным. Это делает модель не только неточной, но и принципиально не интерпретируемой. Вообще, неадекватность знаков или величины весов – хорошее указание на мультиколлинеарность.\n\nДля того, чтобы справиться с этой проблемой, задачу обычно регуляризуют, то есть добавляют к ней дополнительное ограничение на вектор весов. Это ограничение можно, как и исходный лосс, задавать по-разному, но, как правило, ничего сложнее, чем $$L^1$$- и $$L^2$$-нормы, не требуется.\n\nВместо исходной задачи теперь предлагается решить такую:\n\n$$\\color{#348FEA}{\\min_w L(f, X, y) = \\min_w(|X w - y|_2^2 + \\lambda |w|^k_k )}$$\n\n$$\\lambda$$ – это очередной параметр, а $$|w|^k_k $$ – это один из двух вариантов:\n\n$$\\color{#348FEA}{|w|^2_2 = w^2_1 + \\ldots + w^2_D}$$\n\nили\n\n$$\\color{#348FEA}{|w|_1^1 = \\vert w_1 \\vert + \\ldots + \\vert w_D \\vert}$$\n\nДобавка $$\\lambda|w|^k_k$$ называется регуляризационным членом или регуляризатором, а число $\\lambda$ – коэффициентом регуляризации.\n\nКоэффициент $$\\lambda$$ является гиперпараметром модели и достаточно сильно влияет на качество итогового решения. Его подбирают по логарифмической шкале (скажем, от 1e-2 до 1e+2), используя для сравнения моделей с разными значениями $\\lambda$ дополнительную валидационную выборку. При этом качество модели с подобранным коэффициентом регуляризации уже проверяют на тестовой выборке, чтобы исключить переобучение. Более подробно о том, как нужно подбирать гиперпараметры, вы можете почитать в соответствующей главе.\n\nОтдельно надо договориться о том, что вес $w_0$, соответствующий отступу от начала координат (то есть признаку из всех единичек), мы регуляризовать не будем, потому что это не имеет смысла: если даже все значения $$y$$ равномерно велики, это не должно портить качество обучения. Обычно это не отображают в формулах, но если придираться к деталям, то стоило бы написать сумму по всем весам, кроме $$w_0$$:\n\n$$|w|^2_2 = \\sum_{\\color{red}{j=1}}^{D}w_j^2,$$\n\n$$|w|1 = \\sum{\\color{red}{j=1}}^{D} \\vert w_j \\vert$$\n\nВ случае $$L^2$$-регуляризации решение задачи изменяется не очень сильно. Например, продифференцировав новый лосс по $$w$$, легко получить, что <<точное>> решение имеет вид:\n\n$$w = (X^TX + \\lambda I)^{-1}X^Ty$$\n\nОтметим, что за этой формулой стоит и понятная численная интуиция: раз матрица $$X^TX$$ близка к вырожденной и обращать её сродни самоубийству. Мы лучше слегка исказим её добавкой $$\\lambda I$$, которая увеличит все собственные значения на $$\\lambda$$, отодвинув их от нуля. Да, аналитическое решение перестаёт быть <<точным>>, но за счёт снижения численных проблем мы получим более качественное решение, чем при использовании <<точной>> формулы.\n\nВ свою очередь, градиент функции потерь\n\n$$L(f_w, X, y) = |Xw - y|^2 + \\lambda|w|^2$$\n\nпо весам теперь выглядит так:\n\n$$ \\nabla_wL(f_w, X, y) = 2X^T(Xw - y) + 2\\lambda w $$\n\nПодставив этот градиент в алгоритм стохастического градиентного спуска, мы получаем обновлённую версию приближенного алгоритма, отличающуюся от старой только наличием дополнительного слагаемого.\n\nВопрос на подумать. Рассмотрим стохастический градиентный спуск для $L^2$-регуляризованной линейной регрессии с батчами размера $$1$$. Выберите правильный вариант шага SGD:\n\n(а) $$w_i\\mapsto w_i - 2\\alpha(\\langle w, x_j\\rangle - y_j)x_{ji} - \\frac{2\\alpha\\lambda}N w_i,\\quad i=1,\\ldots,D$$;\n\n(б) $$w_i\\mapsto w_i - 2\\alpha(\\langle w, x_j\\rangle - y_j)x_{ji} - 2\\alpha\\lambda w_i,\\quad i=1,\\ldots,D$$;\n\n(в) $$w_i\\mapsto w_i - 2\\alpha(\\langle w, x_j\\rangle - y_j)x_{ji} - 2\\lambda N w_i,\\quad i=1,\\ldots D$$.\n\n{% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details="Не регуляризованная функция потерь имеет вид $$\\mathcal{L}(X, y, w) = \\frac1N\\sum_{i=1}^N\\mathcal{L}(x_i, y_i, w)$$, и её можно воспринимать, как оценку по выборке $$(x_i, y_i)_{i=1}^N$$ идеальной функции потерь\n\n$$\\mathcal{L}(w) = \\mathbb{E}_{x, y}\\mathcal{L}(x, y, w)$$\n\nРегуляризационный член не зависит от выборки и добавляется отдельно:\n\n$$\\mathcal{L}{\\text{reg}}(w) = \\mathbb{E}{x, y}\\mathcal{L}(x, y, w) + \\lambda|w|^2$$\n\nСоответственно, идеальный градиент регуляризованной функции потерь имеет вид\n\n$$\\nabla_w\\mathcal{L}{\\text{reg}}(w) = \\mathbb{E}{x, y}\\nabla_w\\mathcal{L}(x, y, w) + 2\\lambda w,$$\n\nГрадиент по батчу – это тоже оценка градиента идеальной функции потерь, только не на выборке $$(X, y)$$, а на батче $$(x_{t_i}, y_{t_i})_{i=1}^B$$ размера $$B$$. Он будет выглядеть так:\n\n$$\\nabla_w\\mathcal{L}{\\text{reg}}(w) = \\frac1B\\sum{i=1}^B\\nabla_w\\mathcal{L}(x_{t_i}, y_{t_i}, w) + 2\\lambda w.$$\n\nКак видите, коэффициентов, связанных с числом объектов в батче или в исходной выборке, во втором слагаемом нет. Так что верным является второй вариант. Кстати, обратите внимание, что в третьем ещё и нет коэффициента $\\alpha$ перед производной регуляризационного слагаемого, это тоже ошибка.\n\n" %}\n\nВопрос на подумать. Распишите процедуру стохастического градиентного спуска для $L^1$-регуляризованной линейной регрессии. Как вам кажется, почему никого не волнует, что функция потерь, строго говоря, не дифференцируема? {% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details=" Распишем для случая батча размера 1:\n\n$$w_i\\mapsto w_i - \\alpha(\\langle w, x_j\\rangle - y_j)x_{ji} - \\frac{\\lambda}N\\cdot \\text{sign}(w_i),\\quad i=1,\\ldots,D$$\n\nФункция потерь не дифференцируема лишь в одной точке. Так как в машинном обучении чаще всего мы имеем дело с данными вероятностного характера, такая не влечёт каких-то особых проблем. Дело в том, что попадание прямо в ноль очень маловероятно из-за численных погрешностей в данных, так что мы можем просто доопределить производную в одной точке, а если даже пару раз попадём в неё за время обучения это не приведёт к каким-то значительным изменениям результатов. " %}\n\nОтметим, что $L^1$- и $L^2$-регуляризацию можно определять для любой функции потерь $L(w, X, y)$ (и не только в задаче регрессии, а и, например, в задаче классификации тоже). Новая функция потерь будет соответственно равна\n\n$$\\widetilde{L}(w, X, y) = L(w, X, y) + \\lambda|w|_1$$\n\nили\n\n$$\\widetilde{L}(w, X, y) = L(w, X, y) + \\lambda|w|_2^2$$\n\nРазреживание весов в $L^1$-регуляризации\n\n$$L^2$$-регуляризация работает прекрасно и используется в большинстве случаев, но есть одна полезная особенность $$L^1$$-регуляризации: её применение приводит к тому, что у признаков, которые не оказывают большого влияния на ответ, вес в результате оптимизации получается равным $0$. Это позволяет удобным образом удалять признаки, слабо влияющие на таргет. Кроме того, это даёт возможность автоматически избавляться от признаков, которые участвуют в соотношениях приближённой линейной зависимости, соответственно, спасает от проблем, связанных с мультиколлинеарностью, о которых мы писали выше.\n\nНе очень строгим, но довольно интуитивным образом это можно объяснить так: 1. В точке оптимума линии уровня регуляризационного члена касаются линий уровня основного лосса, потому что, во-первых, и те, и другие выпуклые, а во-вторых, если они пересекаются трансверсально, то существует более оптимальная точка:\n\n{: .left}\n\nЛинии уровня $$L^1$$-нормы – это $N$-мерные октаэдры. Точки их касания с линиями уровня лосса, скорее всего, лежат на грани размерности, меньшей $$N-1$$, то есть как раз в области, где часть координат равна нулю:\n\n{: .left}\n\nЗаметим, что данное построение говорит о том, как выглядит оптимальное решение задачи, но ничего не говорит о способе, которым это решение можно найти. На самом деле, найти такой оптимум непросто: у $$L^1$$ меры довольно плохая производная. Однако, способы есть. Можете на досуге прочитать, например, вот эту статью о том, как работало предсказание CTR в google в 2012 году. Там этой теме посвящается довольно много места. Кроме того, рекомендуем посмотреть про проксимальные методы в разделе этой книги про оптимизацию в ML.\n\nЗаметим также, что вообще-то оптимизация любой нормы $$L_x, \\ 0 < x \\leq 1$$, приведёт к появлению разреженных векторов весов, просто если c $$L^1$$ ещё хоть как-то можно работать, то с остальными всё будет ещё сложнее.\n\nДругие лоссы\n\nСтохастический градиентный спуск можно очевидным образом обобщить для решения задачи линейной регрессии с любой другой функцией потерь, не только квадратичной: ведь всё, что нам нужно от неё, – это чтобы у функции потерь был градиент. На практике это делают редко, но тем не менее рассмотрим ещё пару вариантов.\n\nMAE\n\nMean absolute error, абсолютная ошибка, появляется при замене $$L^2$$ нормы в MSE на $$L^1$$:\n\n$$\\color{#348FEA}{MAE(y, \\widehat{y}) = \\frac1N\\sum_{i=1}^N \\vert y_i - \\widehat{y}_i\\vert}$$\n\nМожно заметить, что в MAE по сравнению с MSE существенно меньший вклад в ошибку будут вносить примеры, сильно удалённые от ответов модели. Дело тут в том, что в MAE мы считаем модуль расстояния, а не квадрат, соответственно, вклад больших ошибок в MSE получается существенно больше. Такая функция потерь уместна в случаях, когда вы пытаетесь обучить регрессию на данных с большим количеством выбросов в таргете.\n\nИначе на эту разницу можно посмотреть так: MSE приближает матожидание условного распределения $$y \\mid x$$, а MAE – медиану.\n\nMAPE\n\nMean absolute percentage error, относительная ошибка.\n\n$$MAPE(y, \\widehat{y}) = \\frac1N\\sum_{i=1}^N \\left|\\frac{\\widehat{y}_i-y_i}{y_i}\\right|$$\n\nЧасто используется в задачах прогнозирования (например, погоды, загруженности дорог, кассовых сборов фильмов, цен), когда ответы могут быть различными по порядку величины, и при этом мы бы хотели верно угадать порядок, то есть мы не хотим штрафовать модель за предсказание 2000 вместо 1000 в разы сильней, чем за предсказание 2 вместо 1.\n\nВопрос на подумать. Кроме описанных выше в задаче линейной регрессии можно использовать и другие функции потерь, например, Huber loss:\n\n$$\\mathcal{L}(f, X, y) = \\sum_{i=1}^Nh_{\\delta}(y_i - \\langle w_i, x\\rangle),\\mbox{ где }h_{\\delta}(z) = \\begin{cases} \\frac12z^2,\\ |z|\\leqslant\\delta,\\ \\delta(|z| - \\frac12\\delta),\\ |z| > \\delta \\end{cases}$$\n\nЧисло $$\\delta$$ является гиперпараметром. Сложная формула при $$\\vert z\\vert > \\delta$$ нужна, чтобы функция $$h_{\\delta}(z)$$ была непрерывной. Попробуйте объяснить, зачем может быть нужна такая функция потерь. {% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details="Часто требования формулируют в духе <<функция потерь должна слабее штрафовать то-то и сильней штрафовать вот это>>. Например, $L^2$-регуляризованный лосс штрафует за большие по модулю веса. В данном случае можно заметить, что при небольших значениях ошибки берётся просто MSE, а при больших мы начинаем штрафовать нашу модель менее сурово. Например, это может быть полезно для того, чтобы выбросы не так сильно влияли на результат обучения. " %}\n\nЛинейная классификация\n\nТеперь давайте поговорим про задачу классификации. Для начала будем говорить про бинарную классификацию на два класса. Обобщить эту задачу до задачи классификации на $$K$$ классов не составит большого труда. Пусть теперь наши таргеты $$y$$ кодируют принадлежность к положительному или отрицательному классу, то есть принадлежность множеству $${-1,1}$$ (в этой главе договоримся именно так обозначать классы, хотя в жизни вам будут нередко встречаться и метки $${0,1}$$), а $$x$$ – по-прежнему векторы из $$\\mathbb{R}^D$$. Мы хотим обучить линейную модель так, чтобы плоскость, которую она задаёт, как можно лучше отделяла объекты одного класса от другого.\n\n{: .left}\n\nВ идеальной ситуации найдётся плоскость, которая разделит классы: положительный окажется с одной стороны от неё, а отрицательный с другой. Выборка, для которой это возможно, называется линейно разделимой. Увы, в реальной жизни такое встречается крайне редко.\n\nКак обучить линейную модель классификации, нам ещё предстоит понять, но уже ясно, что итоговое предсказание можно будет вычислить по формуле\n\n$$y = \\text{sign} \\langle w, x_i\\rangle$$\n\n{% include details.html summary="Почему бы не решать, как задачу регрессии?" details=" Мы можем попробовать предсказывать числа $$-1$$ и $$1$$, минимизируя для этого, например, MSE с последующим взятием знака, но ничего хорошего не получится. Во-первых, регрессия почти не штрафует за ошибки на объектах, которые лежат близко к разделяющей плоскости, но не с той стороны. Во вторых, ошибкой будет считаться предсказание, например, $$5$$ вместо $$1$$, хотя нам-то на самом деле не важно, какой у числа модуль, лишь бы знак был правильным. Если визуализировать такое решение, то проблемы тоже вполне заметны:\n\n{: .left}\n\nНам нужна прямая, которая разделяет эти точки, а не проходит через них! " %}\n\nСконструируем теперь функционал ошибки так, чтобы он вышеперечисленными проблемами не обладал. Мы хотим минимизировать число ошибок классификатора, то есть\n\n$$\\sum_i \\mathbb{I}[y_i \\neq sign \\langle w, x_i\\rangle]\\longrightarrow \\min_w$$\n\nДомножим обе части на $$y_i$$ и немного упростим\n\n$$\\sum_i \\mathbb{I}[y_i \\langle w, x_i\\rangle < 0]\\longrightarrow \\min_w$$\n\nВеличина $M = y_i \\langle w, x_i\\rangle$ называется отступом (margin) классификатора. Такая фунция потерь называется misclassification loss. Легко видеть, что\n\nотступ положителен, когда $sign(y_i) = sign(\\langle w, x_i\\rangle)$, то есть класс угадан верно; при этом чем больше отступ, тем больше расстояние от $x_i$ до разделяющей гиперплоскости, то есть «уверенность классификатора»;\n\nотступ отрицателен, когда $sign(y_i) \\ne sign(\\langle w, x_i\\rangle)$, то есть класс угадан неверно; при этом чем больше по модулю отступ, тем более сокрушительно ошибается классификатор.\n\nОт каждого из отступов мы вычисляем функцию\n\n$$F(M) = \\mathbb{I}[M < 0] = \\begin{cases}1,\\ M < 0,\\ 0,\\ M\\geqslant 0\\end{cases}$$\n\nОна кусочно-постоянная, и из-за этого всю сумму невозможно оптимизировать градиентными методами: ведь её производная равна нулю во всех точках, где она существует. Но мы можем мажорировать её какой-нибудь более гладкой функцией, и тогда задачу можно будет решить. Функции можно использовать разные, у них свои достоинства и недостатки, давайте рассмотрим несколько примеров:\n\n{: .left}\n\nВопрос на подумать. Допустим, мы как-то обучили классификатор, и подавляющее большинство отступов оказались отрицательными. Правда ли нас постигла катастрофа? {% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details=" Наверное, мы что-то сделали не так, но ситуацию можно локально выправить, если предсказывать классы, противоположные тем, которые выдаёт наша модель. " %}\n\nВопрос на подумать. Предположим, что у нас есть два классификатора с примерно одинаковыми и достаточно приемлемыми значениями интересующей нас метрики. При этом одна почти всегда выдаёт предсказания с большими по модулю отступами, а вторая – с относительно маленькими. Верно ли, что первая модель лучше, чем вторая? {% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details=" На первый взгляд кажется, что первая модель действительно лучше: ведь она предсказывает <<увереннее>>, но на самом деле всё не так однозначно: во многих случаях модель, которая умеет <<честно признать, что не очень уверена в ответе>>, может быть предпочтительней модели, которая врёт с той же непотопляемой уверенностью, что и говорит правду. В некоторых случаях лучше может оказаться модель, которая, по сути, просто отказывается от классификации на каких-то объектах. " %}\n\nОшибка перцептрона\n\nРеализуем простейшую идею: давайте считать отступы только на неправильно классифицированных объектах и учитывать их не бинарно, а линейно, пропорционально их размеру. Получается такая функция:\n\n$$F(M) = \\max(0, -M)$$\n\nДавайте запишем такой лосс с $$L^2$$-регуляризацией:\n\n$$L(w, x, y) = \\lambda\\vert\\vert w\\vert\\vert^2_2 + \\sum_i \\max(0, -y_i \\langle w, x_i\\rangle)$$\n\nНайдём градиент:\n\n$$ \\nabla_w L(w, x, y) = 2 \\lambda w + \\sum_i \\begin{cases} 0, & y_i \\langle w, x_i \\rangle > 0 \\ - y_i x_i, & y_i \\langle w, x_i \\rangle \\leq 0 \\end{cases} $$\n\nИмея аналитическую формулу для градиента, мы теперь можем так же, как и раньше, применить стохастический градиентный спуск, и задача будет решена.\n\nДанная функция потерь впервые была предложена для перцептрона Розенблатта, первой вычислительной модели нейросети, которая в итоге привела к появлению глубокого обучения.\n\nОна решает задачу линейной классификации, но у неё есть одна особенность: её решение не единственно и сильно зависит от начальных параметров. Например, все изображённые ниже классификаторы имеют одинаковый нулевой лосс:\n\n{: .left}\n\nHinge loss, SVM\n\nДля таких случаев, как на картинке выше, возникает логичное желание не только найти разделяющую прямую, но и постараться провести её на одинаковом удалении от обоих классов, то есть максимизировать минимальный отступ:\n\n{: .left}\n\nЭто можно сделать, слегка поменяв функцию ошибки, а именно положив её равной:\n\n$$F(M) = \\max(0, 1-M)$$\n\n$$L(w, x, y) = \\lambda|w|^2_2 + \\sum_i \\max(0, 1-y_i \\langle w, x_i\\rangle)$$\n\n$$ \\nabla_w L(w, x, y) = 2 \\lambda w + \\sum_i \\begin{cases} 0, & 1 - y_i \\langle w, x_i \\rangle \\leq 0 \\ - y_i x_i, & 1 - y_i \\langle w, x_i \\rangle > 0 \\end{cases} $$\n\nПочему же добавленная единичка приводит к желаемому результату?\n\nИнтуитивно это можно объяснить так: объекты, которые проклассифицированы правильно, но не очень "уверенно" (то есть $$0 \\leq y_i \\langle w, x_i\\rangle < 1$$), продолжают вносить свой вклад в градиент и пытаются "отодвинуть" от себя разделяющую плоскость как можно дальше.\n\nК данному выводу можно прийти и чуть более строго; для этого надо совершенно по-другому взглянуть на выражение, которое мы минимизируем. Поможет вот эта картинка:\n\n{: .left}\n\nЕсли мы максимизируем минимальный отступ, то надо максимизировать $$\\frac{2}{|w|_2}$$, то есть ширину полосы при условии того, что большинство объектов лежат с правильной стороны, что эквивалентно решению нашей исходной задачи:\n\n$$\\lambda|w|^2_2 + \\sum_i \\max(0, 1-y_i \\langle w, x_i\\rangle) \\longrightarrow\\min\\limits_{w}$$\n\nОтметим, что первое слагаемое у нас обратно пропорционально ширине полосы, но мы и максимизацию заменили на минимизацию, так что тут всё в порядке. Второе слагаемое – это штраф за то, что некоторые объекты неправильно расположены относительно разделительной полосы. В конце концов, никто нам не обещал, что классы наши линейно разделимы, и можно провести оптимальную плоскость вообще без ошибок.\n\nИтоговое положение плоскости задаётся всего несколькими обучающими примерами. Это ближайшие к плоскости правильно классифицированные объекты, которые называют опорными векторами или support vectors. Весь метод, соответственно, зовётся методом опорных векторов, или support vector machine или сокращённо SVM. Начиная с шестидесятых годов это был сильнейший из известных методов машинного обучения. В девяностые его сменили методы, основанные на деревьях решений, которые в свою очередь недавно передали "пальму первенства" нейросетям.\n\nПочему же SVM был столь популярен? Из-за небольшого количества параметров и доказуемой оптимальности. Сейчас для нас нормально выбирать специальный алгоритм под задачу и подбирать оптимальные гиперпараметры для этого алгоритма перебором, а когда-то трава была зеленее, а компьютеры медленнее, и такой роскоши у людей не было. Поэтому им нужны были модели, которые гарантированно неплохо работали бы в любой ситуации. Такой моделью и был SVM.\n\nДругие замечательные свойства SVM: существование уникального решения и доказуемо минимальная склонность к переобучению среди всех популярных классов линейных классификаторов. Кроме того, несложная модификация алгоритма, ядровый SVM, позволяет проводить нелинейные разделяющие поверхности.\n\nСтрогий вывод постановки задачи SVM можно прочитать тут или в лекции К.В. Воронцова.\n\nЛогистическая регрессия\n\nВ этом параграфе мы будем обозначать классы нулём и единицей.\n\nЕщё один интересный метод появляется из желания посмотреть на классификацию как на задачу предсказания вероятностей. Хороший пример – предсказание кликов в интернете (например, в рекламе и поиске). Наличие клика в обучающем логе не означает, что, если повторить полностью условия эксперимента, пользователь обязательно кликнет по объекту опять. Скорее у объектов есть какая-то "кликабельность", то есть истинная вероятность клика по данному объекту. Клик на каждом обучающем примере является реализацией этой случайной величины, и мы считаем, что в пределе в каждой точке отношение положительных и отрицательных примеров должно сходиться к этой вероятности.\n\nПроблема состоит в том, что вероятность, по определению, величина от 0 до 1, а простого способа обучить линейную модель так, чтобы это ограничение соблюдалось, нет. Из этой ситуации можно выйти так: научить линейную модель правильно предсказывать какой-то объект, связанный с вероятностью, но с диапазоном значений $$(-\\infty,\\infty)$$, и преобразовать ответы модели в вероятность. Таким объектом является logit или log odds – логарифм отношения вероятности положительного события к отрицательному $$\\log\\left(\\frac{p}{1-p}\\right)$$.\n\nЕсли ответом нашей модели является $$\\log\\left(\\frac{p}{1-p}\\right)$$, то искомую вероятность посчитать не трудно:\n\n$$\\langle w, x_i\\rangle = \\log\\left(\\frac{p}{1-p}\\right)$$\n\n$$e^{\\langle w, x_i\\rangle} = \\frac{p}{1-p}$$\n\n$$p=\\frac{1}{1 + e^{-\\langle w, x_i\\rangle}}$$\n\nФункция в правой части называется сигмоидой и обозначается\n\n$$\\color{#348FEA}{\\sigma(z) = \\frac1{1 + e^{-z}}}$$\n\nТаким образом, $p = \\sigma(\\langle w, x_i\\rangle)$\n\nКак теперь научиться оптимизировать $$w$$ так, чтобы модель как можно лучше предсказывала логиты? Нужно применить метод максимума правдоподобия для распределения Бернулли. Это самое простое распределение, которое возникает, к примеру, при бросках монетки, которая орлом выпадает вероятностью $$p$$. У нас только событием будет не орёл, а то, что пользователь кликнул на объект с такой вероятностью. Если хотите больше подробностей, почитайте про распределение Бернулли в теоретическом минимуме.\n\nПравдоподобие позволяет понять, насколько вероятно получить данные значения таргета $y$ при данных $X$ и весах $w$. Оно имеет вид\n\n$$ p(y\\mid X, w) =\\prod_i p(y_i\\mid x_i, w) $$\n\nи для распределения Бернулли его можно выписать следующим образом:\n\n$$ p(y\\mid X, w) =\\prod_i p_i^{y_i} (1-p_i)^{1-y_i} $$\n\nгде $$p_i$$ – это вероятность, посчитанная из ответов модели. Оптимизировать произведение неудобно, хочется иметь дело с суммой, так что мы перейдём к логарифмическому правдоподобию и подставим формулу для вероятности, которую мы получили выше:\n\n$$ \\ell(w, X, y) = \\sum_i \\big( y_i \\log(p_i) + (1-y_i)\\log(1-p_i) \\big) =$$\n\n$$ =\\sum_i \\big( y_i \\log(\\sigma(\\langle w, x_i \\rangle)) + (1-y_i)\\log(1 - \\sigma(\\langle w, x_i \\rangle)) \\big) $$\n\nЕсли заметить, что\n\n$$ \\sigma(-z) = \\frac{1}{1 + e^z} = \\frac{e^{-z}}{e^{-z} + 1} = 1 - \\sigma(z), $$\n\nто выражение можно переписать проще:\n\n$$ \\ell(w, X, y)=\\sum_i \\big( y_i \\log(\\sigma(\\langle w, x_i \\rangle)) + (1 - y_i) \\log(\\sigma(-\\langle w, x_i \\rangle)) \\big) $$\n\nНас интересует $w$, для которого правдоподобие максимально. Чтобы получить функцию потерь, которую мы будем минимизировать, умножим его на минус один:\n\n$$\\color{#348FEA}{L(w, X, y) = -\\sum_i \\big( y_i \\log(\\sigma(\\langle w, x_i \\rangle)) + (1 - y_i) \\log(\\sigma(-\\langle w, x_i \\rangle)) \\big)}$$\n\nВ отличие от линейной регрессии, для логистической нет явной формулы решения. Деваться некуда, будем использовать градиентный спуск. К счастью, градиент устроен очень просто:\n\n$$ \\nabla_w L(y, X, w) = -\\sum_i x_i \\big( y_i - \\sigma(\\langle w, x_i \\rangle)) \\big) $$ {% include details.html summary="Вывод формулы градиента" details=" Нам окажется полезным ещё одно свойство сигмоиды::\n\n$$ \\frac{d \\log \\sigma(z)}{d z} = \\left( \\log \\left( \\frac{1}{1 + e^{-z}} \\right) \\right)\' = \\frac{e^{-z}}{1 + e^{-z}} = \\sigma(-z) \\ \\frac{d \\log \\sigma(-z)}{d z} = -\\sigma(z) $$\n\nОтсюда:\n\n$$ \\nabla_w \\log \\sigma(\\langle w, x_i \\rangle) = \\sigma(-\\langle w, x_i \\rangle) x_i \\ \\nabla_w \\log \\sigma(-\\langle w, x_i \\rangle) = -\\sigma(\\langle w, x_i \\rangle) x_i $$\n\nи градиент оказывается равным\n\n$$ \\nabla_w L(y, X, w) = -\\sum_i \\big( y_i x_i \\sigma(-\\langle w, x_i \\rangle) - (1 - y_i) x_i \\sigma(\\langle w, x_i \\rangle)) \\big) = \\ = -\\sum_i \\big( y_i x_i (1 - \\sigma(\\langle w, x_i \\rangle)) - (1 - y_i) x_i \\sigma(\\langle w, x_i \\rangle)) \\big) = \\ = -\\sum_i \\big( y_i x_i - y_i x_i \\sigma(\\langle w, x_i \\rangle) - x_i \\sigma(\\langle w, x_i \\rangle) + y_i x_i \\sigma(\\langle w, x_i \\rangle)) \\big) = \\ = -\\sum_i \\big( y_i x_i - x_i \\sigma(\\langle w, x_i \\rangle)) \\big) $$ " %}\n\nПредсказание модели будет вычисляться, как мы договаривались, следующим образом:\n\n$$p=\\sigma(\\langle w, x_i\\rangle)$$\n\nЭто вероятность положительного класса, а как от неё перейти к предсказанию самого класса? В других методах нам достаточно было посчитать знак предсказания, но теперь все наши предсказания положительные и находятся в диапазоне от 0 до 1. Что же делать? Интуитивным и не совсем (и даже совсем не) правильным является ответ <<взять порог 0.5>>. Более корректным будет подобрать этот порог отдельно, для уже построенной регрессии минимизируя нужную вам метрику на отложенной тестовой выборке. Например, сделать так, чтобы доля положительных и отрицательных классов примерно совпадала с реальной.\n\nОтдельно заметим, что метод называется логистической регрессией, а не логистической классификацией именно потому, что предсказываем мы не классы, а вещественные числа – логиты.\n\nВопрос на подумать. Проверьте, что, если метки классов – это $\\pm1$, а не $0$ и $1$, то функцию потерь для логистической регрессии можно записать в более компактном виде:\n\n$$\\mathcal{L}(w, X, y) = \\sum_{i=1}^N\\log(1 + e^{-y_i\\langle w, x_i\\rangle})$$\n\nВопрос на подумать. Правда ли разделяющая поверхность модели логистической регрессии является гиперплоскостью? {% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details=" Разделяющая поверхность отделяет множество точек, которым мы присваиваем класс $$0$$ (или $$-1$$), и множество точек, которым мы присваиваем класс $$1$$. Представляется логичным провести отсечку по какому-либо значению предсказанной вероятности. Однако, выбор этого значения — дело не очевидное. Как мы увидим в главе про калибровку классификаторов, это может быть не настоящая вероятность. Допустим, мы решили провести границу по значению $$\\frac12$$. Тогда разделяющая поверхность как раз задаётся равенством $$p = \\frac12$$, что равносильно $$\\langle w, x\\rangle = 0$$. А это гиперплоскость. " %}\n\nВопрос на подумать. Допустим, что матрица объекты-признаки $X$ имеет полный ранг по столбцам (то есть все её столбцы линейно независимы). Верно ли, что решение задачи восстановления логистической регрессии единственно? {% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details="В этот раз хорошего геометрического доказательства, как было для линейной регрессии, пожалуй, нет; нам придётся честно посчитать вторую производную и доказать, что она является положительно определённой. Сделаем это для случая, когда метки классов – это $\\pm1$. Формулы так получатся немного попроще. Напомним, что в этом случае\n\n$$L(w, X, y) = -\\sum_{i=1}^N\\log(1 + e^{-y_i\\langle w, x_i\\rangle})$$\n\nСледовательно,\n\n$$\\frac{\\partial}{\\partial w_{j}}L(w, X, y) = \\sum_{i=1}^N\\frac{y_ix_{ij}e^{-y_i\\langle w, x_i\\rangle}}{1 + e^{-y_i\\langle w, x_i\\rangle}} = \\sum_{i=1}^Ny_ix_{ij}\\left(1 - \\frac1{1 + e^{-y_i\\langle w, x_i\\rangle}}\\right)$$\n\n$$\\frac{\\partial^2L}{\\partial w_j\\partial w_k}(w, X, y) = \\sum_{i=1}^Ny^2_ix_{ij}x_{ik}\\frac{e^{-y_i\\langle w, x_i\\rangle}}{(1 + e^{-y_i\\langle w, x_i\\rangle})^2} =$$\n\n$$ = \\sum_{i=1}^Ny^2_ix_{ij}x_{ik}\\sigma(y_i\\langle w, x_i\\rangle)(1 - \\sigma(y_i\\langle w, x_i\\rangle))$$\n\nТеперь заметим, что $$y_i^2 = 1$$ и что, если обозначить через $$D$$ диагональную матрицу с элементами $$\\sigma(y_i\\langle w, x_i\\rangle)(1 - \\sigma(y_i\\langle w, x_i\\rangle))$$ на диагонали, матрицу вторых производных можно представить в виде:\n\n$$\\nabla^2L = \\left(\\frac{\\partial^2\\mathcal{L}}{\\partial w_j\\partial w_k}\\right) = X^TDX$$\n\nТак как $$0 < \\sigma(y_i\\langle w, x_i\\rangle) < 1$$, у матрицы $$D$$ на диагонали стоят положительные числа, из которых можно извлечь квадратные корни, представив $$D$$ в виде $$D = D^{1/2}D^{1/2}$$. В свою очередь, матрица $$X$$ имеет полный ранг по столбцам. Стало быть, для любого вектора приращения $$u\\ne 0$$ имеем\n\n$$u^TX^TDXu = u^TX^T(D^{1/2})^TD^{1/2}Xu = \\vert D^{1/2}Xu \\vert^2 > 0$$\n\nТаким образом, функция $$L$$ выпукла вниз как функция от $$w$$, и, соответственно, точка её экстремума непременно будет точкой минимума.\n\nА теперь – почему это не совсем правда. Дело в том, что, говоря «точка её экстремума непременно будет точкой минимума», мы уже подразумеваем существование этой самой точки экстремума. Только вот существует этот экстремум не всегда. Можно показать, что для линейно разделимой выборки функция потерь логистической регрессии не ограничена снизу, и, соответственно, никакого экстремума нет. Доказательство мы оставляем читателю. " %}\n\nВопрос на подумать. На картинке ниже представлены результаты работы на одном и том же датасете трёх моделей логистической регрессии с разными коэффициентами $L^2$-регуляризации:\n\n{: .left}\n\nНаверху показаны предсказанные вероятности положительного класса, внизу – вид разделяющей поверхности.\n\nКак вам кажется, какие картинки соответствуют самому большому коэффициенту регуляризации, а какие – самому маленькому? Почему? {% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details=" Коэффициент регуляризации максимален у левой модели. На это нас могут натолкнуть два соображения. Во-первых, разделяющая прямая проведена достаточно странно, то есть можно заподозрить, что регуляризационный член в лосс-функции перевесил функцию потерь исходной задачи. Во-вторых, модель предсказывает довольно близкие к $$\\frac12$$ вероятности – это значит, что значения $$\\langle w, x\\rangle$$ близки к нулю, то есть сам вектор $w$ близок к нулевому. Это также свидетельствует о том, что регуляризационный член играет слишком важную роль при оптимизации.\n\nНаименьший коэффициент регуляризации у правой модели. Её предсказания достаточно <<уверенные>> (цвета на верхнем графике сочные, то есть вероятности быстро приближаются к $$0$$ или $$1$$). Это может свидетельствовать о том, что числа $$\\langle w, x\\rangle$$ достаточно велики по модулю, то есть $$\\vert\\vert w \\vert\\vert$$ достаточно велик. " %}\n\nМногоклассовая классификация\n\nВ этом разделе мы будем следовать изложению из лекций Евгения Соколова.\n\nПусть каждый объект нашей выборки относится к одному из $K$ классов: $\\mathbb{Y} = {1, \\ldots, K}$. Чтобы предсказывать эти классы с помощью линейных моделей, нам придётся свести задачу многоклассовой классификации к набору бинарных, которые мы уже хорошо умеем решать. Мы разберём два самых популярных способа это сделать – one-vs-all и all-vs-all, а проиллюстрировать их нам поможет вот такой игрушечный датасет\n\n{: .left}\n\nОдин против всех (one-versus-all)\n\nОбучим $K$ линейных классификаторов $b_1(x), \\ldots, b_K(x)$, выдающих оценки принадлежности классам $1, \\ldots, K$ соответственно. В случае с линейными моделями эти классификаторы будут иметь вид\n\n$$b_k(x) = \\text{sgn}\\left(\\langle w_k, x \\rangle + w_{0k}\\right)$$\n\nКлассификатор с номером $k$ будем обучать по выборке $\\left(x_i, 2\\mathbb{I}[y_i = k] - 1\\right)_{i = 1}^{N}$; иными словами, мы учим классификатор отличать $k$-й класс от всех остальных.\n\nЛогично, чтобы итоговый классификатор выдавал класс, соответствующий самому уверенному из бинарных алгоритмов. Уверенность можно в каком-то смысле измерить с помощью значений линейных функций:\n\n$$a(x) = \\text{argmax}k \\left(\\langle w_k, x \\rangle + w{0k}\\right) $$\n\nДавайте посмотрим, что даст этот подход применительно к нашему датасету. Обучим три линейных модели, отличающих один класс от остальных:\n\n{: .left}\n\nТеперь сравним значения линейных функций\n\n{: .left}\n\nи для каждой точки выберем тот класс, которому соответствует большее значение, то есть самый <<уверенный>> классификатор:\n\n{: .left}\n\nХочется сказать, что самый маленький класс <<обидели>>.\n\nПроблема данного подхода заключается в том, что каждый из классификаторов $b_1(x), \\dots, b_K(x)$ обучается на своей выборке, и значения линейных функций $\\langle w_k, x \\rangle + w_{0k}$ или, проще говоря, "выходы" классификаторов могут иметь разные масштабы. Из-за этого сравнивать их будет неправильно. Нормировать вектора весов, чтобы они выдавали ответы в одной и той же шкале, не всегда может быть разумным решением: так, в случае с SVM веса перестанут являться решением задачи, поскольку нормировка изменит норму весов.\n\nВсе против всех (all-versus-all)\n\nОбучим $C_K^2$ классификаторов $a_{ij}(x)$, $i, j = 1, \\dots, K$, $i \\neq j$. Например, в случае с линейными моделями эти модели будут иметь вид\n\n$$b_{ij}(x) = \\text{sgn}\\left( \\langle w_{ij}, x \\rangle + w_{0,ij} \\right)$$\n\nКлассификатор $a_{ij}(x)$ будем настраивать по подвыборке $X_{ij} \\subset X$, содержащей только объекты классов $i$ и $j$. Соответственно, классификатор $a_{ij}(x)$ будет выдавать для любого объекта либо класс $i$, либо класс $j$. Проиллюстрируем это для нашей выборки:\n\n{: .left}\n\nЧтобы классифицировать новый объект, подадим его на вход каждого из построенных бинарных классификаторов. Каждый из них проголосует за своей класс; в качестве ответа выберем тот класс, за который наберется больше всего голосов:\n\n$$a(x) = \\text{argmax}k\\sum{i = 1}^{K} \\sum_{j \\neq i}\\mathbb{I}[a_{ij}(x) = k]$$\n\nДля нашего датасета получается следующая картинка:\n\n{: .left}\n\nОбратите внимание на серый треугольник на стыке областей. Это точки, для которых голоса разделились (в данном случае каждый классификатор выдал какой-то свой класс, то есть у каждого класса было по одному голосу). Для этих точек нет явного способа выдать обоснованное предсказание.\n\nМногоклассовая логистическая регрессия\n\nНекоторые методы бинарной классификации можно напрямую обобщить на случай многих классов. Выясним, как это можно проделать с логистической регрессией.\n\nВ логистической регрессии для двух классов мы строили линейную модель\n\n$$b(x) = \\langle w, x \\rangle + w_0$$\n\nа затем переводили её прогноз в вероятность с помощью сигмоидной функции $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$. Допустим, что мы теперь решаем многоклассовую задачу и построили $K$ линейных моделей\n\n$$b_k(x) = \\langle w_k, x \\rangle + w_{0k},$$\n\nкаждая из которых даёт оценку принадлежности объекта одному из классов. Как преобразовать вектор оценок $(b_1(x), \\ldots, b_K(x))$ в вероятности? Для этого можно воспользоваться оператором $\\text{softmax}(z_1, \\ldots, z_K)$, который производит <<нормировку>> вектора:\n\n$$\\text{softmax}(z_1, \\ldots, z_K) = \\left(\\frac{\\exp(z_1)}{\\sum_{k = 1}^{K} \\exp(z_k)}, \\dots, \\frac{\\exp(z_K)}{\\sum_{k = 1}^{K} \\exp(z_k)}\\right).$$\n\nВ этом случае вероятность $k$-го класса будет выражаться как\n\n$$P(y = k \\vert x, w) = \\frac{ \\exp{(\\langle w_k, x \\rangle + w_{0k})}}{ \\sum_{j = 1}^{K} \\exp{(\\langle w_j, x \\rangle + w_{0j})}}.$$\n\nОбучать эти веса предлагается с помощью метода максимального правдоподобия: так же, как и в случае с двухклассовой логистической регрессией:\n\n$$\\sum_{i = 1}^{N} \\log P(y = y_i \\vert x_i, w) \\to \\max_{w_1, \\dots, w_K}$$\n\nМасштабируемость линейных моделей\n\nМы уже обсуждали, что SGD позволяет обучению хорошо масштабироваться по числу объектов, так как мы можем не загружать их целиком в оперативную память. А что делать, если признаков очень много, или мы не знаем заранее, сколько их будет? Такое может быть актуально, например, в следующих ситуациях:\n\nКлассификация текстов: мы можем представить текст в формате <<мешка слов>>, то есть неупорядоченного набора слов, встретившихся в данном тексте, и обучить на нём, например, определение тональности отзыва в интернете. Наличие каждого слова из языка в тексте у нас будет кодироваться отдельной фичой. Тогда размерность каждого элемента обучающей выборки будет порядка нескольких сотен тысяч.\n\nВ задаче предсказания кликов по рекламе можно получить выборку любой размерности, например, так: в качестве фичи закодируем индикатор того, что пользователь X побывал на веб-странице Y. Суммарная размерность тогда будет порядка $$10^9 \\cdot 10^7 = 10^{16}$$. Кроме того, всё время появляются новые пользователи и веб-страницы, так что на этапе применения нас ждут сюрпризы.\n\nЕсть несколько хаков, которые позволяют бороться с такими проблемами: * Несмотря на то, что полная размерность объекта в выборке огромна, количество ненулевых элементов в нём невелико. Значит, можно использовать разреженное кодирование, то есть вместо плотного вектора хранить словарь, в котором будут перечислены индексы и значения ненулевых элементов вектора. * Даже хранить все веса не обязательно! Можно хранить их в хэш-таблице и вычислять индекс по формуле hash(feature) % tablesize. Хэш может вычисляться прямо от слова или id пользователя. Таким образом, несколько фичей будут иметь общий вес, который тем не менее обучится оптимальным образом. Такой подход называется hashing trick. Ясно, что сжатие вектора весов приводит к потерям в качестве, но, как правило, ценой совсем небольших потерь можно сжать этот вектор на много порядков.\n\nПримером открытой библиотеки, в которой реализованы эти возможности, является vowpal wabbit.\n\nParameter server\n\nЕсли при решении задачи ставки столь высоки, что мы не можем разменивать качество на сжатие вектора весов, а признаков всё-таки очень много, то задачу можно решать распределённо, храня все признаки в шардированной хеш-таблице\n\n{: .left}\n\nКружки здесь означают отдельные сервера. Жёлтые загружают данные, а серые хранят части модели. Для обучения жёлтый кружок запрашивает у серого нужные ему для предсказания веса, считает градиент и отправляет его обратно, где тот потом применяется. Схема обладает бесконечной масштабируемостью, но задач, где это оправдано, не очень много.\n\nПодытожим\n\nНа линейную модель можно смотреть как на однослойную нейросеть, поэтому многие методы, которые были изначально разработаны для них, сейчас переиспользуются в задачах глубокого обучения, а базовые подходы к регрессии, классификации и оптимизации вообще выглядят абсолютно так же. Так что несмотря на то, что в целом линейные модели на сегодня применяются редко, то, из чего они состоят и как строятся, знать очень и очень полезно.\n\nНадеемся также, что главным итогом прочтения этой главы для вас будет осознание того, что решение любой ML-задачи состоит из выбора функции потерь, параметризованного класса моделей и способа оптимизации. В следующих главах мы познакомимся с другими моделями и оптимизаторами, но эти базовые принципы не изменятся.'), Document(metadata={'source': '/Users/eugenekillevsky/PycharmProjects/LLM-Interviewer/data/RAG/ml-handbook-master/chapters/neural_nets/intro.md'}, page_content='title: Первое знакомство с полносвязными нейросетями author: radoslav_neichev, filipp_sinicin cover_file_path: ./src/intro_cover.png\n\nЭтот список будет заменен оглавлением, за вычетом заголовка "Contents", к которому добавлен класс no_toc. {:toc}\n\nОсновные определения\n\nИскусственная нейронная сеть (далее — нейронная сеть) — это сложная дифференцируемая функция, задающая отображение из исходного признакового пространства в пространство ответов, все параметры которой могут настраиваться одновременно и взаимосвязанно (то есть сеть может обучаться end-to-end). В частном (и наиболее частом) случае представляет собой последовательность (дифференцируемых) параметрических преобразований.\n\nВнимательный читатель может заметить, что под указанное выше определение нейронной сети подходят и логистическая, и линейная регрессия. Это верное замечание: и линейная, и логистическая регрессии могут рассматриваться как нейронные сети, задающие отображения в пространство ответов и логитов соответственно.\n\nСложную функцию удобно представлять в виде суперпозиции простых, и нейронные сети обычно предстают перед программистом в виде конструктора, состоящего из более-менее простых блоков (слоёв, layers). Вот две простейшие их разновидности:\n\nЛинейный слой (linear layer, dense layer) — линейное преобразование над входящими данными (его обучаемые параметры — это матрица $W$ и вектор $b$): $x \\mapsto xW + b$ ($W \\in \\mathbb{R}^{d \\times k}, x \\in \\mathbb{R}^{d}, b \\in \\mathbb{R}^{k}$). Такой слой преобразует $d$-мерные векторы в $k$-мерные.\n\nФункция активации (activation function) — нелинейное преобразование, поэлементно применяющееся к пришедшим на вход данным. Благодаря функциям активации нейронные сети способны порождать более информативные признаковые описания, преобразуя данные нелинейным образом. Может использоваться, например, ReLU (rectified linear unit) $\\text{ReLU}(x) = \\text{max}(0, x)$ или уже знакомая вам из логистической регрессии сигмоида $\\sigma(x) = \\frac1{1 + e^{-x}}$. К более глубокому рассмотрению разновидностей и свойств различных функций активации вернёмся позднее.\n\nДаже самые сложные нейронные сети обычно собираются из относительно простых блоков, подобных этим. Таким образом, их можно представить в виде вычислительного графа (computational graph), где вершинам промежуточным соответствуют преобразования. На иллюстрации ниже приведён вычислительный граф для логистической регрессии.\n\nНе правда ли, похоже на слоёный пирог из преобразований? Отсюда и слои.\n\nГрафы могут быть и более сложными, в том числе нелинейными:\n\nДавайте разберёмся, что тут происходит.\n\nInput — это вход нейросети, который получает исходные данные. Обычно требуется, чтобы они имели вид матрицы («объекты-признаки») или тензора (многомерной матрицы). Вообще говоря, входов может быть несколько: например, мы можем подавать в нейросеть картинку и какие-нибудь ещё сведения о ней — преобразовывать их мы будем по-разному, поэтому логично предусмотреть два входа в графе.\n\nДальше к исходным данным $X^0$ применяются два линейных слоя, которые превращают их в промежуточные (внутренние, скрытые) представления $X^1$ и $X^2$. В литературе они также называются активациями (не путайте с функциями активации).\n\nКаждое из представлений $X^1$ и $X^2$ подвергается нелинейному преобразованию, превращаясь в новые промежуточные представления $X^3$ и $X^4$ соответственно. Переход от $X^0$ к двум новым матрицам (или тензорам) $X^3$ и $X^4$ можно рассматривать как построение двух новых (возможно, более информативных) признаковых описаний исходных данных.\n\nЗатем представления $X^3$ и $X^4$ конкатенируются (то есть признаковые описания всех объектов объединяются).\n\nДальше следует ещё один линейный слой и ещё одна активация, и полученный результат попадает на выход сети, то есть отдаётся обратно пользователю.\n\nНейросеть, в которой есть только линейные слои и различные функции активации, называю полносвязной (fully connected) нейронной сетью или многослойным перцептроном (multilayer perceptron, MLP).\n\nПосмотрим, что происходит с размерностями, если на вход подаётся матрица $N\\times d$:\n\n{% include details.html summary="Примечание о терминологии" details="В литературе, увы, нет единства терминологии.\n\nТак, например, никто не мешает нам объявить «единым и неделимым слоем» композицию линейного слоя и активации (в ознаменование того, что мы почти никогда не используем просто линейный слой без нелинейности). Например, в фреймворке keras активацию можно указать в линейном слое в качестве параметра.\n\nТакже в ряде источников слоями называется то, что мы называем промежуточными представлениями. Нам, впрочем, кажется, что промежуточные результаты $X^i$ правильнее называть именно представлениями: ведь это новые признаковые описания, представляющие исходные объекты. Кроме того, во всех нейросетевых фреймворках слои — это именно преобразования, поэтому и нам кажется правильным объявлять слоями именно преобразования, связывающие промежуточные представления. " %}\n\nА вот и настоящий пример из реальной жизни. GoogLeNet (она же Inception-v1), показавшая SotA-результат на ILSVRC 2014 (ImageNet challenge), выглядит так:\n\nЗдесь каждый кирпичик — это некоторое относительно простое преобразование, а белым помечены входы и выходы вычислительного графа.\n\nСовременные же сети часто выглядят и ещё сложней, но всё равно они собираются из достаточно простых кирпичиков-слоёв.\n\n{% include details.html summary="Примечание" details="Стоит отметить, впрочем, что в общем случае нейронная сеть представляет собой просто некоторую сложную функцию (или, что эквивалентно, граф вычислений), и в некоторых (очень нетривиальных) случаях её нет смысла разбивать на слои. В качестве иллюстрации ниже приведены структуры агностических нейронных сетей WANN, представленных в работе Weight Agnostic Neural Networks, NeurIPS 2019.\n\n" %}\n\nForward & backward propagation\n\nИнформация может течь по графу в двух направлениях.\n\nПрименение нейронной сети к данным (вычисление выхода по заданному входу) часто называют прямым проходом, или же forward propagation (forward pass). На этом этапе происходит преобразование исходного представления данных в целевое и последовательно строятся промежуточные (внутренние) представления данных — результаты применения слоёв к предыдущим представлениям. Именно поэтому проход называют прямым.\n\nПри обратном проходе, или же backward propagation (backward pass), информация (обычно об ошибке предсказания целевого представления) движется от финального представления (а чаще даже от функции потерь) к исходному через все преобразования. Механизм обратного распространения ошибки, играющий важнейшую роль в обучении нейронных сетей, как раз предполагает обратное движение по вычислительному графу сети (с ним вы познакомитесь в следующей главе).\n\nАрхитектуры для простейших задач\n\nКак мы уже упоминали выше, нейросети — это универсальный конструктор, который из простых блоков позволяет собрать орудия для решения самых разных задач. Давайте посмотрим на конкретные примеры. Безусловно, мир намного разнообразнее того, что мы покажем вам в этой главе, но с чего-то ведь надо начинать, не так ли?\n\nВ тех несложных ситуациях, которые мы сейчас рассмотрим, архитектура будет отличаться лишь самыми последними этапами вычисления (у сетей будут разные «головы»). Для иллюстрации приведём примеры нескольких игрушечных архитектур для решения игрушечных задач классификации и регрессии на двумерных данных:\n\nБинарная классификация\n\nДля решения задачи бинарной классификации подойдёт любая архитектура, на выходе у которой одно число от $0$ до $1$, интерпретируемое как «вероятность класса 1». Обычно этого добиваются, взяв\n\n$$\\widehat{y} = \\sigma(f(X^m)),$$\n\nгде $f$ — некоторая функция, превращающая представление $X^m$ в число (если $X^m$ — матрица, то подойдёт $f(X^m) = X^mw + b$, где $w$ — вектор-столбец), а $\\sigma$ — наша любимая сигмоида. При этом $X^m$ может получаться как угодно, лишь бы хватало оперативной памяти и не было переобучения.\n\nВ качестве функции потерь удобно брать уже знакомый нам log loss.\n\nМногоклассовая классификация\n\nРаботая с другими моделями, мы порой вынуждены были выдумывать сложные стратегии многоклассовой классификации; нейросети позволяют это делать легко и элегантно. Достаточно построить сеть, которая будет выдавать $K$ неотрицательных чисел, суммирующихся в 1 (где $K$ — число классов); тогда им можно придать смысл вероятностей классов и предсказывать тот класс, «вероятность» которого максимальна (в главе про вероятностные модели мы обсудим, почему это вовсе не обязаны быть настоящие вероятности). Превратить произвольный набор из $K$ чисел в набор из неотрицательных чисел, суммирующихся в 1, позволяет, к примеру, функция\n\n$$\\text{softmax}(x_1,\\ldots,x_K) = \\left( \\frac{e^{x_1}}{\\sum_ie^{x_i}},\\ldots,\\frac{e^{x_K}}{\\sum_ie^{x_i}} \\right)$$\n\nНаиболее популярные архитектуры для многоклассовой классификации имеют вид\n\n$$\\widehat{y} = \\text{softmax}(f(X^m)),$$\n\nгде $f$ — функция, превращающая $X^m$ в матрицу $B\\times K$ (где $B$ — размер батча), а $X^m$ может быть получен любым приятным вам образом.\n\nНо какой будет функция потерь для такой сети? Мы должны научиться сравнивать «распределение вероятностей классов» с истинным (в котором на месте истинного класса стоит 1, а в остальных местах 0). Сделать это позволяет кросс-энтропия (она же negative log-likelihood), которая является некоторым аналогом расстояния между распределениями:\n\n$$\\mathcal{L}(\\widehat{y}, y) = -\\frac1B\\sum_{i=1}^B\\sum_{k=1}^Ky_{ik}\\log{\\widehat{y}_{ik}},$$\n\nгде снова $B$ — размер батча, а $K$ — число классов. Легко видеть, что при $k = 2$ получается та самая функция потерь, которую мы использовали для обучения бинарной классификации.\n\n(Множественная) регрессия\n\nС помощью нейросетей легко создать модель, которая предсказывает не одно число, а сразу несколько (например, координаты ключевых точек лица — кончика носа, уголков рта и так далее — по фотографии). Достаточно сделать, чтобы последнее представление было матрицей $B\\times M$, где $B$ — размер батча, а $M$ — количество предсказываемых чисел. Особенностью большинства моделей регрессии является то, что после последнего слоя (часто линейного) не ставят функций активации. Вы тоже этого не делайте, если только чётко не понимаете, зачем вам это. В качестве функции потерь можно брать, например, $MSE$ по всей матрице $B\\times M$.\n\nВсё вместе\n\nЕсли вы используете нейросети, то ваши таргеты могут иметь и различную природу. Например, можно соорудить одну-единственную сеть, которая по фотографии нескольких котиков определяет их количество (регрессия) и породу каждого из них (многоклассовая классификация). Лосс для такой модели может быть равен (взвешенной) сумме лоссов для каждой из задач (правда, не факт, что это хорошая идея). Так что, по крайней мере в теории, сетям подвластны любые задачи. На практике, конечно, всё гораздо хитрей: для обучения слишком сложной сети у вас может не хватить данных или вычислительных мощностей.\n\nПопулярные функции активации\n\nReLU, Rectified linear unit\n\n$$ \\text{ReLU}(x) = \\max(0, x), $$\n\n$$ \\text{ReLU}: \\mathbb{R} \\to [0, +\\infty). $$\n\nReLU представляет собой простую кусочно-линейную функцию. Одна из наиболее популярных функций активации. В нуле производная доопределяется нулевым значением.\n\nМинусы: * область значений является смещённой относительно нуля; * для отрицательных значений производная равна нулю, что может привести к затуханию градиента.\n\nПлюсы: * простота вычисления активации и производной.\n\nReLU и её производная очень просты для вычисления: достаточно лишь сравнить значение с нулём. Благодаря этому использование ReLU позволяет достигать прироста в скорости до четырёх-шести раз относительно сигмоиды.\n\nLeaky ReLU\n\n$$ \\text{Leaky ReLU}(x) = \\max(\\alpha x, x), \\quad \\alpha = \\text{const}, 0 < \\alpha \\ll 1 $$\n\n$$ \\text{Leaky ReLU}: \\mathbb{R} \\to (-\\infty, +\\infty). $$\n\nГиперпараметр $\\alpha$ обеспечивает небольшой уклон слева от нуля, что позволяет получить более симметричную относительно нуля область значений. Также меньше провоцирует затухание градиента благодаря наличию ненулевого градиента и слева, и справа от нуля.\n\nPReLU, Parametric ReLU\n\n$$ \\text{PReLU}(x) = \\max(\\alpha x, x), \\quad 0 < \\alpha \\ll 1 $$\n\n$$ \\text{PReLU}: \\mathbb{R} \\to (-\\infty, +\\infty). $$\n\nАналогична Leaky ReLU, но параметр $\\alpha$ настраивается градиентными методами.\n\nELU — гладкая аппроксимация ReLU. Обладает более высокой вычислительной сложностью, достаточно редко используется на практике.\n\nSigmoid, сигмоида\n\n$$ \\sigma(x) = \\frac{1}{1 + \\exp(-x)}, $$\n\n$$ \\sigma: \\mathbb{R} \\to (0, 1). $$ Исторически одна из первых функций активации. Рассматривалась в том числе и как гладкая аппроксимация порогового правила, эмулирующая активацию естественного нейрона.\n\nК минусам сигмоиды можно отнести: * область значений смещена относительно нуля; * сигмоида (как и её производная) требует вычисления экспоненты, что является достаточно сложной вычислительной операцией. Её приближённое значение вычисляется на основе ряда Тейлора или с помощью полиномов, Stack Overflow question 1, question 2; * на «хвостах» обладает практически нулевой производной, что может привести к затуханию градиента; * максимальное значение производной составляет $0.25$, что также приводит к затуханию градиента.\n\nНа практике редко используется внутри сетей, чаще всего в случаях, когда внутри модели решается задача бинарной классификации (например, вероятность забывания информации в LSTM).\n\nTanh, гиперболический тангенс\n\n$$ \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}, $$\n\n$$ \\tanh: \\mathbb{R} \\to (-1, 1). $$\n\nПлюсы: * как и сигмоида, имеет ограниченную область значений; * в отличие от сигмоиды, область значений симметрична.\n\nМинусы: * требует вычисления экспоненты, что является достаточно сложной вычислительной операцией; * на «хвостах» обладает практически нулевой производной, что может привести к затуханию градиента.\n\nВопрос на подумать. А почему симметричность области значений может быть ценным свойством?\n\n{% include details.html summary="Ответ (попобуйте подумать сами, прежде чем открывать)" details="Разберём на примере сигмоиды. Пусть оказалось так, что все веса линейных слоёв инициализированы положительными числами. Сигмоида от положительного числа даёт нечто, большее $\\frac12$, и с каждым дальнейшим слоем ситуация может усугубляться, приводя к тому, что сигмоиды будут выдавать всё более близкие к единице значения. Это покарает нас, когда мы начнём считать градиенты: они начнут «затухать»." %}\n\n{% include details.html summary="Более подробно о затухании градиента" details="Рассмотрим нейронную сеть из нескольких линейных слоёв с сигмоидой в качестве функции активации. Пусть\n\n$$ X^1 = \\sigma(Z^1) = \\sigma(W^1 X^0), $$\n\nгде в качестве $Z^1$ обозначен результат применения линейного слоя. Свободный член, как и ранее, опущен для упрощения выкладок.\n\nРассмотрим график сигмоиды и её производной:\n\nНа «хвостах» её производная практически равна нулю (ведь сигмоида представляет собой почти константу). То есть если какое-то значение $Z^1$ было достаточно велико по абсолютной величине (например, $\\vert z^1_k \\vert = 7.4$), то градиент функции потерь для этой компоненты будет домножен на очень малое число и фактически станет равным нулю:\n\n$$ \\frac{\\partial l}{\\partial W^0_{k, :}} = \\frac{\\partial l}{\\partial X^1_k}\\frac{\\partial X^1_k}{\\partial Z^1_k} \\frac{\\partial Z^1_k}{\\partial W^0_{k,:}} \\approx \\frac{\\partial l}{\\partial X^1_k} \\cdot 0 \\cdot \\frac{\\partial Z^1_k}{\\partial W^0_{k,:}} \\approx 0. $$\n\nПолучается, что $k$-ая строка матрицы $W^0$ не будет обновлена: ведь градиент равен нулю. Это может привести к «отмиранию» части весов: неудачное значение параметров приведёт к невозможности их обновления.\n\nПримечание. Это одна из причин необходимости нормировки данных и выбора правильной инициализации для начальных значений параметров нейронной сети.\n\nНо помимо явного обнуления градиента есть и вторая проблема: максимальное значение производной сигмоиды составляет $0.25$. То есть она всегда уменьшает значение градиента не менее чем в четыре раза. Если же представить себе глубокую сеть с 20+ слоями, то для каждого следующего (если считать с конца) слоя градиент будет домножаться на число, не превосходящее $0.25$. Так, для переменных из первого слоя коэффициент составит $4^{-20}$. " %}\n\nЗачем нужны функции активации?\n\nКазалось бы, можно последовательно выстраивать лишь линейные слои, но так не делают: после каждого линейного слоя обязательно вставляют функцию активации. Но зачем? Попробуем разобраться. Рассмотрим нейронную сеть из двух линейных слоёв. Что произойдёт, если между ними будет отсутствовать нелинейная функция активации?\n\n$$\\widehat{y} = X^{out} = X^{1}W^{2} + b^2 = (X^0W^1 + b^1)W^2 + b^2 = $$\n\n$$= X^0\\color{blue}{W^1W^2} + \\color{green}{b^1W^2 + b^2} = X^0\\color{blue}{\\widetilde{W}} + \\color{green}{\\widetilde{b}}$$\n\nЛинейная комбинация линейных отображений есть линейное отображение, то есть два последовательных линейных слоя эквивалентны одному линейному слою.\n\nДобавление функций активации после линейного слоя позволяет получить нелинейное преобразование, и подобной проблемы уже не возникает. Вдобавок правильный выбор функции активации позволяет получить преобразование, обладающее подходящими свойствами.\n\nВ качестве функции активации может использоваться, например, уже знакомая вам из логистической регрессии сигмоида $\\sigma(x) = \\frac1{1 + \\exp(-x)}$ или ReLU (Rectified linear unit) $\\text{ReLU}(x) = \\text{max}(0, x)$. К более глубокому рассмотрению разновидностей и свойств различных функций активации вернёмся позднее.\n\n{% include details.html summary="Примечание" details="На самом деле бывают ситуации, когда два линейных слоя подряд — это полезно. Например, если вы понимаете, что у вас очень много параметров, а информации в данных не так много, вы можете заменить линейный слой, превращающий $m$-мерные векторы в $n$-мерные, на два, вставив посередине $k$-мерное представление, где $k \\ll m, n$:\n\nС точки зрения линейной алгебры это примерно то же самое, что потребовать, чтобы матрица исходного линейного слоя имела ранг не выше $k$. И с точки зрения сужения «информационного канала» это иногда может сработать. Но в любом случае вы должны понимать, что два линейных слоя подряд стоит ставить, только если вы хорошо понимаете, чего хотите добиться." %}\n\nНемного о мощи нейросетей\n\nРассмотрим для начала задачу регрессии. Ясно, что линейная модель (то есть однослойная нейросеть) может приблизить только линейную функцию, но уже двуслойная нейросеть может приблизить почти что угодно. Есть ряд теорем на эту тему, мы упомянем одну из них (обратите внимание на год: как мы уже упоминали, нейросети начали серьёзно изучать задолго до того, как они начали превращаться в state of the art).\n\nТеорема Цыбенко (1989). Для любой непрерывной функции $f(x):\\mathbb{R}^m\\rightarrow\\mathbb{R}$ и для любого $\\varepsilon > 0$ найдётся число $N$, а также числа $w_1\\ldots,w_N$, $b_1,\\ldots,b_N$ $\\alpha_1,\\ldots,\\alpha_N$, для которых\n\n$$\\left|f(x) - \\sum_{i=1}^N\\alpha_i\\sigma(\\langle x, w_i\\rangle + b_i)\\right| < \\varepsilon$$\n\nдля любых $x$ из единичного куба $[0,1]^m$ в $\\mathbb{R}^m$.\n\nВ сумме из теоремы Цыбенко легко опознать двуслойную нейросеть с сигмоидной функцией активации. В самом деле, сперва мы превращаем $x$ в $\\langle x, w_i\\rangle + b_i$ — это можно представить в виде одной матричной операции (линейный слой!):\n\n$$x\\mapsto x^{(1)} = x\\cdot\\begin{pmatrix} \\phantom{\\frac12}w_1 & \\ldots & w_N \\end{pmatrix} + \\begin{pmatrix} \\phantom{\\frac12}b_1 & \\ldots & b_N \\end{pmatrix},$$\n\nгде $w_i$ — вектор-столбцы, а каждое из $b_i$ прибавляется к $i$-му столбцу, после чего поэлементно берём от $x^{(1)}$ сигмоиду (активация) $x^{(2)} = \\sigma(x^{(1)})$, после чего вычисляем\n\n$$\\sum_{i=1}^N\\alpha_ix^{(2)}_i = \\left(\\alpha_1,\\ldots,\\alpha_N\\right)\\cdot x,$$\n\nи это второй линейный слой (без свободного члена).\n\nПравда, теорема не очень помогает находить такие функции, но это уже другое дело. В любом случае — если дать нейросети достаточно данных, она действительно может выучить почти что угодно.\n\nУпражнение. Мы не будем приводить результатов, касающихся классификации, но рекомендуем воспользоваться замечательной «песочницей» и убедиться, что, используя один скрытый слой из двух нейронов и сигмоиду в качестве функции активации, можно неплохо классифицировать данные со сложной, совсем даже не линейной границей между классами. Вы также можете поиграть с разными функциями активации.\n\nА для получения данного решения нам необходим метод автоматической настройки всех параметров нейронной сети — метод обратного распространения ошибки, или же error backpropagation. Следующая глава будет посвящён внимательному рассмотрению данного метода.'), Document(metadata={'source': '/Users/eugenekillevsky/PycharmProjects/LLM-Interviewer/data/RAG/ml-handbook-master/chapters/prob_genclass/intro.md'}, page_content='title: Генеративный подход к классификации author: michail_artemiev toc: true\n\nЭтот список будет заменен оглавлением, за вычетом заголовка "Contents", к которому добавлен класс no_toc. {:toc}\n\nГенеративный и дискриминативный подходы к обучению\n\nКлассификационные модели, которые мы рассматривали в предыдущих главах, нацелены непосредственно на оценку $P(Y \\vert X)$. Такие модели называются дискриминативными. К ним относится, например, логистическая регрессия: она предлагает оценку $$ \\hat P(y=1 \\vert x) = \\sigma(w^Tx) $$. В процессе обучения дискриминативные модели подбирают разделяющую поверность (гиперплоскость в случае логистической регрессии). Новые объекты дискриминативная модель классифицирует в зависимости от того, по какую сторону от разделяющей поверности они лежат. Например, обучившись на изображениях домашних кошек (y=0) и рысей (y=1), дискриминативная модель будет определять, новое изображение больше похоже на кошку или на рысь. При этом, если на вход такой модели дать изображение собаки (объект класса, которого не было в обучении, выброс), дискриминативная модель заведомо не сможет обнаружить, что это и не кошка, и не рысь, и отнесёт такой объект к одному из \'\'знакомых\'\' ей классов.\n\nВ этой главе мы поговорим о другой группе моделей, которые нацелены на оценку $P(X, Y) = P(X \\vert Y)P(Y)$. Такая модель описала бы, как обычно выглядят кошки, как они могут выглядеть, а каких кошек точно не бывает. Так же она описала бы и рысей. Она также определила бы по обучающим данным, насколько изображения кошек встречаются чаще, чем изображения рысей, т.е. оценила бы $P(Y)$. Если модель позволила точно оценить распределение $P(X \\vert Y)$, с её помощью можно генерировать объекты из этого условного распределения, в нашем примере -- изображения кошек и рысей соответственно. А вместе распределение $P(X, Y)$ дало бы нам возможность генерировать изображения и кошек, и рысей, причём именно в той пропорции, в которой они встречаются в реальном мире. Поэтому модели, оценивающие $P(X, Y)$, называют генеративными. Ещё одно достоинство генеративных моделей -- их способность находить выбросы в данных: объект $x$ можно считать выбросом, если $P(x \\vert y)$ мало для каждого класса $y$.\n\nЗаметим, что находить выбросы с помощью генеративной модели можно и когда класс всего один (т.е. никакие метки классов не доступны). Такая задача называется одноклассовой классификацией. Например, если у нас есть не размеченный датасет с аудиозаписями речи людей, то, обучив на нём генеративную модель, оценивающую в данном случае $P(X \\vert Y)=P(X)$, мы сможем для нового аудио $x$ определить, похоже ли оно на аудиозапись человеческой речи (значение $P(x)$ велико), или это что-то другое: синтезированная речь, посторонний шум и т.п. ($P(x)$ мало). Тем не менее, если мы знаем, что "выбросы", с которыми модели предстоит сталкиваться, -- как правило, синтезированная речь, то, дополнив датасет вторым классов, состоящим из синтезированной речи и смоделировав также распределение этого класса, мы можем существенно увеличить качество детектирования таких выбросов.\n\nЧтобы использовать генеративную модель для классификации, необходимо выразить $P(Y \\vert X)$ через $P(X \\vert Y)$ и $P(Y)$. Сделать это позволяет формула Байеса:\n\n$$ P(y \\vert x) = \\frac{P(x, y)}{\\sum\\limits_{y\'\\in Y} P(y\')P(x \\vert y\')} = \\frac{P(y)P(x \\vert y)}{\\sum\\limits_{y\'\\in Y} P(y\')P(x \\vert y\')} $$\n\nКлассификация в генеративных моделях осуществляется с помощью $$ \\textit{байесовского классификатора} $$:\n\n$$a(x) = \\arg\\max\\limits_{y\\in Y} P(y \\vert x) = \\arg\\max\\limits_{y\\in Y} \\frac{P(y)P(x \\vert y)}{\\sum\\limits_{y\'\\in Y} P(y\')P(x \\vert y\')} = \\arg\\max\\limits_{y\\in Y} P(y)P(x \\vert y)$$\n\nОценить $P(Y)$, как правило, несложно. Для этого используют частотные оценки, полученные обучающей выборке:\n\n$$\\hat P(Y=y) = \\frac{#(Y=y)}{N} \\label{eq:class_proba_estimation} \\tag{1}$$\n\nОтметим ещё раз, что использование генеративного подхода позволяет внедрять в модель априорные знания о $P(y)$. Это не очень впечатляет, когда речь идёт о бинарной классификации, но всё меняется, если рассмотреть задачу ASR (автоматического распознавания речи), в которой по записи голоса восстанавливается произносимый текст. Таргетами здесь могут быть любые предложения или даже более развёрнутые тексты. При этом размеченных данных (запись, текст) обычно намного меньше, чем доступных текстов, и обученная на большом чисто текстовом корпусе языковая модель, которая будет оценивать вероятность того или иного предложения, может стать большим подспорьем, позволив из нескольких фонетически корректных наборов слов выбрать тот, который в большей степени похож на настоящее предложение.\n\nНо как смоделировать распределение $P(X, Y)$? Пространство всех возможных функций распределения $P(X, Y)$ бесконечномерно, из-за чего оценить произвольное распределение с помощью конечной выборки невозможно. Поэтому перед оценкой $P(X, Y)$ на это распределение накладывают дополнительные ограничения. Некоторые простые примеры таких ограничений мы рассмотрим в следующих разделах.\n\nGaussian discriminant analysis\n\nМодель гауссовского (или квадратичного) дискриминантного анализа (GDA) строится в предположении, что распределение объектов каждого класса $y$ подчиняется многомерному нормальному закону со средним $\\mu_y$ и ковариационной матрицей $\\Sigma_y$:\n\n$$p(x \\vert y) = \\frac{1}{(2\\pi)^{n/2} \\vert \\Sigma_y \\vert ^{1/2}}\\exp\\left(-\\frac{1}{2}(x-\\mu_y)^T\\Sigma_y^{-1} (x-\\mu_y)\\right) \\label{eq:multivariate_normal}$$\n\nТогда функция правдоподобия $$\\mathcal L(P(Y), \\mu, \\Sigma) = \\prod_{i=1}^N p(x_i \\vert y_i; \\mu_{y_i}, \\Sigma_{y_i})P(y_i)$$ достигает максимума при\n\n$$ \\hat\\mu_y = \\frac{\\sum\\limits_{i=1}^N{x_i\\unicode{x1D7D9}{y_i=y}}}{\\sum\\limits{i=1}^N\\unicode{x1D7D9}{y_i=y}},\\hspace{5mm} \\hat\\Sigma_y = \\frac{\\sum\\limits{i=1}^N{(x_i - \\hat\\mu_{y})(x_i - \\hat\\mu_{y})^T \\unicode{x1D7D9}{y_i=y}}}{\\sum\\limits{i=1}^N\\unicode{x1D7D9}_{y_i=y}} $$\n\nИ $\\hat P(Y)$, представленной выше см. выражение $(1)$.\n\nРассмотрим, как выглядит разделяющая поверхность в модели GDA. На поверхности, разделяющей классы $y_i$ и $y_j$ выполняется\n\n$$P(y_i \\vert x)=P(y_j \\vert x) \\Leftrightarrow$$ $$p(x \\vert y_i)P(y_i) = p(x \\vert y_j)P(y_j)\\Leftrightarrow$$ $$\\log p(x \\vert y_i) + \\log P(y_i) - \\log p(x \\vert y_j) - \\log P(y_j) = 0\\Leftrightarrow$$\n\n\\begin{equation} -\\frac{1}{2}(x-\\mu_{y_i})^T\\Sigma_{y_i}^{-1} (x-\\mu_{y_i})-\\log (2\\pi)^{n/2}|\\Sigma_{y_i}|^{1/2} + \\log P(y_i) + \\frac{1}{2}(x-\\mu_{y_j})^T\\Sigma_{y_j}^{-1} (x-\\mu_{y_j}) + \\log (2\\pi)^{n/2}|\\Sigma_{y_j}|^{1/2} - \\log P(y_j) = 0 \\label{eq:GDA_boundary} \\tag{2} \\end{equation}\n\nПоскольку левая часть уравнения $ (2) $ квадратична по $x$, разделяющая поверность между двумя классами будет представлять из себя гиперповерность порядка 2. Пример разделяющей поверхности многоклассовой модели GDA приведён на рис.\n\n{: style="width:30vw" #fig:GDA_boundary}\n\nПлотность классов и разделяющая поверхность в многоклассовой модели LDA см. рисунок.\n\n{: style="width:30vw" #fig:LDA_boundary}\n\nLinear Discriminant Analysis\n\nВ выражении $ (2) $ член второго порядка $x^T (\\Sigma_{y_j}^{-1} - \\Sigma_{y_i}^{-1})x$ зануляется при $\\Sigma_{y_i}=\\Sigma_{y_j}$. Таким образом, если дополнительно предположить, что все классы имеют общую ковариационную матрицу $\\Sigma$, разделяющая поверхность между любыми двумя классами будет линейной (см. рисунок). Поэтому такая модель называется линейным дискриминантным анализом (LDA).\n\nНа этапе обучения единственное отличие модели LDA от GDA состоит в оценке ковариационной матрицы: $$\\hat \\Sigma = \\frac{1}{N}\\sum\\limits_{i=1}^N{(x_i - \\hat\\mu_{y_i})(x_i - \\hat\\mu_{y_i})^T}$$\n\nЗаметим, что в модели GDA для каждого класса требовалось оценить порядка $d^2$ параметров. Это может привести к переобучению в случае, если размерность пространства признаков велика, а некоторые классы представлены в обучающей выборке малым количеством объектов. В LDA для каждого класса требуется оценить лишь порядка $d$ параметров (значение $P(y)$ и элементы вектора $\\mu_y$), и ещё $d^2$ общих для всех классов параметров (элементы матрицы $\\Sigma$). Таким образом, основное преимущество модели LDA перед GDA -- её меньшая склонность к переобучению, недостаток -- линейная разделяющая поверхность.\n\nМетод наивного байеса\n\nПредположим, что признаки $X$ объектов каждого класса $y$ -- независимые случайные величины:\n\n$$\\forall y\\in Y \\hspace{2mm} \\forall U, V: U\\sqcup V = {1, ... d}, \\hspace{2mm} \\forall x^u\\subset \\mathbb R^{|U|}, x^v\\subset \\mathbb R^{|V|}$$\n\n$$P(X^U\\in x^u, X^V\\in x^v|Y=y) = P(X^U\\in x^u|Y=y)P(X^V\\in x^u|Y=y).$$\n\nВ таком случае говорят, что величины $X$ условно независимы отностиельно $Y$. Тогда справедливо\n\n\\begin{equation}\\label{eq:cond_independent} P(X \\vert Y) = P(X^1, X^2, ..., X^d \\vert Y) = P(X^1 \\vert Y)P(X^2, ..., X^d \\vert Y) = ... = P(X^1 \\vert Y)P(X^2 \\vert Y)...P(X^d \\vert Y) \\tag{3} \\end{equation}\n\nТо есть для того, чтобы оценить плотность многомерного распределения $P(X \\vert Y)$ достаточно оценить плотности одномерных распределений $P(X^i \\vert Y)$, см. рисунок.\n\n{: #fig:blobs_density}\n\nНа рисунке приведён пример условно независимых относительно $Y$ случайных величин $X^1, X^2$. Для оценки плотности двумерных распределений объектов классов достаточно оценить плотности маргинальных распределений, изображённые графиками вдоль осей.\n\nРассмотрим пример. Пусть решается задача классификации отзывов об интернет-магазине на 2 категории: $Y=0$ -- отрицательный отзыв, клиент остался не доволен, и $Y=1$ -- положительный отзыв. Пусть признак $X^w$ равен 1, если слово $w$ присутствует в отзыве, и 0 иначе. Тогда условие $ (3) $ означает, что, в частности, наличие или отсутствие слова \'\'дозвониться\'\' в отрицательном отзыве не влияет на вероятность наличия в этом отзыве слова \'\'телефон\'\'.\n\nНа практике в процессе feature engineering почти всегда создаётся много похожих признаков, и условно независимые признаки можно встретить очень редко. Поэтому генеративную модель, построенную в предположении условия $ (3) $, называют наивным байесовским классификатором (Naive Bayes classifier, NB).\n\nОбучение модели NB заключается в оценке распределений $P(Y)$ и $P(X^i \\vert Y)$. Для $P(Y)$ можно использовать частотную оценку $ (1) $. $P(X^i \\vert y)$ -- одномерное распределение. Рассмотрим несколько способов оценки одномерного распределения.\n\nОценка одномерного распределения\n\nПусть мы хотим оценить одномерное распределение $P(X)$.\n\nЕсли распределение $P(X)$ дискретное, требуется оценить его функцию массы, т.е. вероятность того, что величина $X$ примет значение $x_j$. Метод максимума правдоподобия приводит к частотной оценке:\n\n$$ \\hat P(X = x_j) = \\frac{#(X = x_j)}{N} \\tag{4} $$\n\nГде $N$ -- размер выборки, по которой оцениватеся распределение $X$ (количество объектов класса $y$ в случае оценки плотности класса $y$).\n\nПри этом может оказаться, что некоторое значение $x_j$ ни разу не встречается в обучающей выборке. Например, в случае классификации отзывов методом Наивного Байеса, слово \'\'амбивалентно\'\' не встретилось ни в одном положительном отзыве, но встретилось в отрицательных. Тогда использование оценки приведёт к тому, что все отзывы с этим словом будут определяться NB как отрицательные с вероятностью 1. Чтобы избежать принятия таких радикальных решений при недостатке статистики, используют сглаживание Лапласа:\n\n$$\\hat P(X = x_j) = \\frac{#(X = x_j) + \\alpha}{N + m\\alpha},$$\n\nгде $m$ -- количество различных значений, принимаемых случайной величиной $X$, $\\alpha$ -- гиперпараметр.\n\nДля оценки плотности $p$ абсолютно непрерывного распределения в точке $a$ можно разделить количество объектов обучающей выборки в окрестности точки $a$ на размер этой окрестности:\n\n$$\\hat p(a) = \\frac{\\sum\\limits_{j}\\unicode{x1D7D9}{a - h < X_j < a + h}}{2h} = \\frac{\\sum\\limits{j}\\unicode{x1D7D9}_{-h < X_j - a < h}}{2h}.$$\n\nОбычно объекты, лежащие дальше от точки $a$, учитывают с меньшим весом. Таким образом, оценка плотности приобретает вид\n\n$$\\hat p(a) = \\frac{\\sum\\limits_{j}K_h(X_j - a)}{2h},$$\n\nгде функция $K_h$, называемая ядром, обычно имеет носитель $(-h, h)$ (см. рисунок). Такой способ оценки плотности называют непарамерическим.\n\n{: #fig:kernls}\n\nРезультат оценки плотности с разными ядрами. Использованы изображения из:\n\n{: #fig:KDE}\n\nПри параметрической оценке плотности предполагают, что искомое распределение лежит в параметризованном классе, и подбирают значения параметров при помощи метода максимума правдоподобия. Например, предположим, что искомое распределение нормальное. Тогда функция его плотности имеет вид\n\n$$p(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\n\nТаким образом, чтобы оценить плотность $p(x)$, достаточно оценить параметры $\\mu, \\sigma$. Метод максимума правдоподобия в этом случае даст такие оценки:\n\n$\\hat\\mu = \\overline X$ -- выборочное среднее, $\\hat\\sigma = \\sqrt{\\frac{1}{N}\\sum_{j=1}^N (X_j - \\overline X)}$ -- выборочное стандартное отклонение.\n\nЕсли в модели NB распределения всех признаков объектов каждого класса нормальные, оценив параметры этих распределений, мы сможем каждый класс $y$ описать нормальным распределением со средним $\\mu_p$ и диагональной ковариационной матрицей, значения на диагонали которой обозначим $\\sigma_p$. Таким образом, полученная модель (Gaussian Naive Bayes, GNB) эквивалентна модели GDA с дополнительным ограничением на диагональность ковариационных матриц.\n\nНаивный байесовский подход и логистическая регрессия\n\nПредположим теперь, что в модели GNB класса всего 2, причём соответствующие им ковариационные матрицы совпадают, как это было в модели LDA. Таким образом $\\sigma_0 = \\sigma_1 = \\sigma$.\n\nПосмотрим, как будет выглядеть $P(Y \\vert X)$ в этом случае. По теореме Байеса имеем\n\n$$P(Y=1 \\vert X) = \\frac{P(Y = 1)P(X \\vert Y = 1)}{P(Y = 1)P(X \\vert Y = 1) + P(Y = 0)P(X \\vert Y = 0)}$$\n\nРазделим числитель и знаменатель полученного выражения на числитель:\n\n$$P(Y=1 \\vert X) = \\frac{1}{1 + \\frac{P(Y = 0)P(X \\vert Y = 0)}{P(Y = 1)P(X \\vert Y = 1)}} = \\frac{1}{1 + \\exp\\left(\\ln\\frac{P(Y=0)P(X \\vert Y=0)}{P(Y=1)P(X \\vert Y=1)}\\right)}$$\n\nИз условной независимости $X^i$ относительно $Y$ получаем\n\n\\begin{equation}\\label{eq:posterior} P(Y=1 \\vert X) = \\frac{1}{1 + \\exp\\left(\\ln\\frac{P(Y=0)}{P(Y=1)} + \\sum\\limits_{i=1}^d \\ln\\frac{P(X^i \\vert Y=0)}{P(X^i \\vert Y=1)}\\right)} \\tag{5} \\end{equation}\n\nПерепишем сумму в знаменателе, воспользовавшись формулой плотности нормального распределения\n\n$$\\sum\\limits_{i=1}^d \\ln\\frac{P(X^i \\vert Y=0)}{P(X^i \\vert Y=1)} = \\sum\\limits_{i=1}^d \\ln\\frac{\\frac{1}{\\sqrt{2\\pi\\sigma_i^2}}\\exp \\left(\\frac{-(X^i - \\mu_{0,i})^2}{2\\sigma_i^2}\\right)}{\\frac{1}{\\sqrt{2\\pi\\sigma_i^2}}\\exp \\left(\\frac{-(X^i - \\mu_{1,i})^2}{2\\sigma_i^2}\\right)} $$\n\n$$= \\sum\\limits_{i=1}^d \\frac{\\left(X^i - \\mu_{1, i}\\right)^2 - \\left(X^i - \\mu_{0, i}\\right)^2}{2\\sigma_i^2}= \\sum\\limits_{i=1}^d \\left(\\frac{\\mu_{0, i} - \\mu_{1, i}}{\\sigma_i^2}X^i + \\frac{\\mu_{1, i} ^ 2 - \\mu_{0, i} ^ 2}{2\\sigma_i^2}\\right)$$\n\nПодставляя это выражение в формулу $ (5) $, получаем\n\n$$P(Y=1 \\vert X) = \\frac{1}{1 + \\exp\\left(\\ln\\frac{P(Y=0)}{P(Y=1)} + \\sum\\limits_{i=1}^d \\left(\\frac{\\mu_{0, i} - \\mu_{1, i}}{\\sigma_i^2}X^i + \\frac{\\mu_{1, i} ^ 2 - \\mu_{0, i} ^ 2}{2\\sigma_i^2}\\right)\\right)}$$\n\nТаким образом, $P(Y=1 \\vert X)$ представляется в GNB с общей ковариационной матрицей в таком же виде, как в модели логистической регрессии:\n\n\\begin{equation} \\label{eq:logreg} P(Y=1 \\vert X) = \\frac{1}{1 + \\exp\\left(w_0 + \\sum\\limits_{i=1}^d w_i X^i\\right)} \\tag{6} \\end{equation}\n\nгде в случае GNB\n\n$$w_0 = \\ln\\frac{P(Y=1)}{P(Y=0)} +\\sum\\limits_{i=1}^d\\frac{\\mu_{1, i} ^ 2 - \\mu_{0, i} ^ 2}{2\\sigma_i^2}, \\quad w_i = \\frac{\\mu_{0, i} - \\mu_{1, i}}{\\sigma_i^2} \\hspace{1cm} i=1, \\dots, l$$\n\nОднако это не значит, что модели эквивалентны: модель логистической регрессии накладывает менее строгие ограничения на распределение $P(X, Y)$, чем GNB. Так, $X^i$ могут не являться условно независимыми относительно $Y$, а распределения $P(X \\vert Y=y)$ могут не удовлетворять нормальному закону, но $P(y \\vert X)$ может при этом всё равно представляться в виде $ (6) $. В этом случае использование метода логистической регрессии предпочтительнее. С другой стороны, если есть основания полагать, что требования GNB выполняются, то от GNB можно ожидать более высокого качества классификации по сравнению с логистической регрессией.')]