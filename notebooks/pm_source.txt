[Document(metadata={'source': '/Users/eugenekillevsky/PycharmProjects/LLM-Interviewer/data/RAG/ml-handbook-master/chapters/prob_glm/glm.md'}, page_content='title: Обобщённые линейные модели author: michail_artemiev, stanislav_fedotov toc: true\n\nЭтот список будет заменен оглавлением, за вычетом заголовка "Contents", к которому добавлен класс no_toc. {:toc}\n\nМотивация\n\nДо сих пор мы рассматривали в основном модели вида\n\n$$y\\sim f(x) + \\varepsilon$$\n\nс шумом $$\\varepsilon$$ из того или иного распределения. Но у этих моделей (а) шум не зависит от $$x$$ и (б) $$y$$ может принимать любые значения. А что, если мы захотим предсказывать время ожидания доставки? Казалось бы, чем дольше время потенциального ожидания, тем больше его дисперсия. А как корректно предсказывать таргет, который принимает только целые значения?\n\nОдин из подходов мы обсудим в этой главе. Грубо говоря, вместо того, чтобы прибавлять один и тот же шум, мы зафиксируем семейство распределений $$p(y\\vert\\mu(x))$$, в котором изменяемым параметром будет зависящее от $$x$$ математическое ожидание $$\\mu(x)$$.\n\nВот как могут выглядеть такие модели для случаев, если $$p$$ нормальное с фиксированной дисперсией, экспоненциальное или пуассоновское соответственно:\n\n{: .center}\n\nКак видим, такой подход позволяет получать и модели с меняющейся дисперсией шума, и модели с целочисленным таргетом.\n\nОпределение\n\nВ этом разделе мы рассмотрим достаточно широкий класс моделей – обобщённые линейные модели (generalized linear models, GLM). К ним относятся, в частности, линейная и логистическая регрессии. В итоге мы научимся подбирать подходящую регрессионную модель для самых разных типов данных.\n\nВспомним, что вероятностную модель линейной регрессии можно записать как\n\n$$y \\vert x \\sim\\color{red}{\\mathcal N}(\\langle x, w\\rangle, \\tau^2),$$\n\nа вероятностную модель логистической регрессии – как\n\n$$y \\vert x \\sim \\color{red}{Bern}(\\color{blue}{\\sigma}(\\langle x, w\\rangle)),$$\n\nгде $\\color{red}{Bern}(p)$ – распределение Бернулли с параметром $p$, а $\\color{blue}{\\sigma}(u) = \\frac{1}{1+e^{-u}}$.\n\nИтак, чем в этих терминах отличаются вероятностные модели линейной и логистической регрессии? 1. Параметризованное семейство распределений для $y \\vert x$, а именно, $\\color{red}{\\mathcal N}(*, \\sigma^2)$ в случае линейной регрессии и $\\color{red}{Bern}$ в случае логистической.\n\nВ обоих случаях математическое ожидание условного распределения $y\\vert x$ является функцией от $\\langle x, w\\rangle$. На это можно посмотреть и по-другому: для каждой из задач выбрана функция $g$ такая, что $g(\\mathbb E(y \\vert x)) = \\langle x, w\\rangle$. Эта функция называется функцией связи (link function). В случае линейной регрессии $g(u) = u$. В самом деле, $\\mathbb E(y \\vert x) = \\mathbb E{\\mathcal N}(\\langle x, w\\rangle, \\tau^2) = \\langle x, w\\rangle$. В случае логистической регрессии $$g(u) = \\sigma^{-1}(u) = \\text{logit}(u) = \\log\\frac{u}{1-u}$$. Давайте это тоже проверим. В модели логистической регрессии условное распределение $y \\vert x$ – это распределение Бернулли с вероятностью успеха $\\sigma(\\langle x, w\\rangle)$, и этой же вероятности равно его математическое ожидание. Следовательно, $$g(\\sigma(\\langle x, w\\rangle)) = \\sigma^{-1}(\\sigma(\\langle x, w\\rangle)) = \\langle x, w\\rangle$$.\n\nОбобщая, можно сказать, что, если данные таковы, что $\\mathbb E(Y \\vert X)$ не является линейной функцией от $x$, мы линеаризуем $\\mathbb E(Y \\vert X)$ с помощью функции связи $g$.\n\nЗамечание: Вообще говоря, нормальное распределение определяется не только своим математическим ожиданием, но и стандартным отклонением. То есть, в отличие от логистической регрессии, модель линейной регрессии не позволяет для данного $x$ оценить все параметры распределения $y \\vert x$, и дисперсию приходится фиксировать изначально. К счастью, выбор её значения в нормальном распределении не влияет ни на оптимальный вектор весов $w$, ни на итоговые предсказания $\\mathbb E(Y \\vert X)$, которые выдаёт обученная модель.\n\nЗадав эти две составляющие – параметризованное семейство распределений и функцию связи – мы получим обобщённую линейную модель (GLM). Для нового объекта $x$ она выдаст предсказание $\\widehat{y} = \\mathbb{E}(y\\vert x) = g^{-1}(\\langle x, w\\rangle)$, а выбор класса распределений $y \\vert x$ потребуется нам для подбора весов $w$. В принципе, можно выбрать любой класс распределений $y \\vert x$ и любую монотонную функцию связи $g$, получив некоторую вероятностную модель. Однако обычно для упрощения поиска оптимальных весов $w$ в GLM предполагают, что $y \\vert x$ принадлежит одному из достаточно простых семейств экспоненциального класса.\n\nЧто даёт нам принадлежность экспоненциальному классу?\n\nВ контексте GLM обычно рассматривают подкласс экспоненциального класса, состоящий из семейств, представимых в виде\n\n$$\\color{#348FEA}{p(y \\vert \\theta, \\phi) = \\exp\\left(\\frac{y\\theta - a(\\theta)}{\\phi} + b(y, \\phi)\\right)}$$\n\nгде $\\theta$ и $\\phi$ – скалярные параметры, причём $\\phi$ – нечто фиксированное, чаще всего дисперсия, которая чаще всего полагается равной $1$, а значения $\\theta$ параметризуют распределения из семейства. Нетрудно переписать плотность в более привычном для нас виде, чтобы стало очевидно, что это семейство действительно из экспоненциального класса:\n\n$$p(y \\vert \\theta, \\phi) = \\frac1{\\exp\\left(\\frac{a(\\theta)}{\\phi}\\right)}\\exp(b(y,\\phi))\\exp\\left(\\frac{y\\theta}{\\phi}\\right)$$\n\nДействительно, если вспомнить, что $\\varphi$ – это константа, а не параметр, то получается очень похоже на\n\n$$p(y\\vert\\nu) = \\frac1{h(\\nu)}g(y)\\cdot\\exp\\left(\\nu^Tu(y)\\right)$$\n\nВ частности, мы видим, что $u(y)$ состоит из единственной компоненты $u_1(y)$, равной $\\frac{y}{\\phi}$. По доказанной в предыдущем разделе лемме имеем тогда, что математическое ожидание $\\mu$ такой случайной величины равно\n\n$$\\mu = \\phi\\mathbb{E}u_1(y) = \\phi\\frac{\\partial}{\\partial\\theta}\\left(\\frac{a(\\theta)}{\\phi}\\right) = a\'(\\theta)$$\n\nДо сих пор мы рассуждали о распределении $p(y)$ без $x$ в условии. Что будет, если его добавить? Параметр $\\phi$ мы договорились сохранять постоянным, тогда от $x$ должен зависеть единственный оставшийся параметр $\\theta$. Самый естественный в нашей ситуации вариант – это положить $\\theta = \\langle x, w\\rangle$. В GLM мы вводили функцию $g$, для которой $g(\\mathbb E(y \\vert x)) = \\langle x, w\\rangle$, то есть $\\mathbb{E}(y\\vert x) = g^{-1}(\\langle x, w\\rangle)$. Но ведь матожидание $y$ равно $a\'(\\theta)$, то есть $a\'(\\langle x, w\\rangle)$. Это позволяет нам однозначно определить функцию связи $g = (a\')^{-1}$. Такая функция связи называется канонической функцией связи (canonical link function).\n\nПримеры\n\nПоговорим немного о том, как на практике подбирать $\\phi, a, b$, чтобы по классу распределений $y \\vert x$ определить каноническую функцию связи. Чтобы разобраться, рассмотрим несколько примеров.\n\nПример 1 Пусть мы решили применить к данным линейную регрессию. Тогда\n\n$$p(y \\vert x, w, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(y-\\langle x, w\\rangle)^2}{2\\sigma^2}\\right)$$\n\nОбозначим для краткости $\\mu = \\langle x, w\\rangle$ и будем рассматривать $p(y\\vert \\mu, \\sigma^2)$.\n\nМы уже знаем, что семейство нормальных распределений относится к экспоненциальному классу, но давайте выразим эту плотность в описанном выше более частном виде:\n\n$$p(y \\vert \\mu, \\sigma^2) = \\exp\\left(-\\frac{(y-\\mu)^2 }{2\\sigma^2}- \\log(2\\pi\\sigma^2)\\right)$$\n\nВ формуле экспоненциального семейства распределений единственная часть, не зависящая от $\\theta$, – это функция $b$. Поскольку $\\mu=a\'(\\theta)$, функция $b$ также не должна зависеть от $\\mu$. Так что внутри экспоненты выделим в качестве функции $b$ всё, что не зависит от $\\mu$:\n\n$$p(y \\vert \\mu, \\sigma^2) = \\exp\\left(\\frac{\\overbrace{y\\mu}^{=y\\theta} - \\overbrace{\\mu^2/2}^{=a(\\theta)}}{\\underbrace{\\sigma^2}{=\\varphi}}\\underbrace{ - \\left(\\frac{y_i^2}{2\\sigma^2} + \\log(2\\pi\\sigma^2)\\right)}{=b(y,\\varphi)}\\right)$$\n\nЭта формула уже похожа на формулу экспоненциального семейства распределений и видно, что $\\phi=\\sigma^2$, $\\theta=g(\\mu)=\\mu$ (коэффициент при $y$), $a(\\theta) = \\mu^2/2 = \\theta^2/2$, $b(y, \\phi) = -\\frac{y^2}{2\\sigma^2} - \\log(2\\pi\\sigma^2)$.\n\nКаноническая функция связи является обратной к $a\'(\\theta) = \\theta$, то есть $\\langle x, w\\rangle = g(\\mu) = \\mu$, как мы и привыкли.\n\nПример 2 Проделаем то же самое, но теперь для распределения Бернулли.\n\nПример 3 Хорошо, про линейную и логистическую регрессию мы и так знали. Давайте попробуем решить с помощью GLM новую задачу. Пусть мы хотим по каким-то признакам $X$ предсказать количество <<лайков>>, которое пользователи поставят посту в социальной сети за первые 10 минут после публикации. Конечно, можно использовать для этого линейную регрессию. Однако предположение линейной регрессии, что $Y \\vert X\\sim\\mathcal N$, в данном случае странное по нескольким причинам. Во-первых, количество лайков заведомо не может быть отрицательным, а нормальное распределение всегда будет допускать ненулевую вероятность отрицательного значения. Во-вторых, количество лайков – всегда целое число. В-третьих, у распределения количества лайков, скорее всего, положительный коэффициент асимметрии (skewness). То есть, если модель предсказывает, что под постом будет 100 лайков, мы скорее можем ожидать, что под ним окажется 200 лайков, чем 0. Нормальное распределение симметрично и не может описать такие данные. С другой стороны, если мы предположим, что в первые 10 минут после публикации есть какая-то постоянная частота (своя для каждого поста, зависящая от $x$), с которой пользователи ставят лайк, мы получим, что количество лайков имеет распределение Пуассона. Распределение Пуассона не имеет описанных выше проблем:\n\nНо какая будет каноническая функция связи, если мы считаем, что $Y \\vert X\\sim\\text{Poisson}$? Аналогично примерам 1, 2:\n\n$$p(y \\vert \\mu) = \\frac{e^{-\\mu}\\mu^y}{y!} = \\exp\\left(y\\log\\mu - \\mu - \\log y!\\right)$$\n\nОткуда $\\phi=1$, $\\theta = g(\\mu) = \\log\\mu$, $a(\\theta) = \\mu = \\exp(\\theta)$, $b(y, \\phi) = -\\log y!$\n\nЗначит, эта модель, называемая пуассоновской регрессией, будет предсказывать с помощью формулы $\\mathbb E(y \\vert x) = g^{-1}(\\langle x, w\\rangle)= \\exp(\\langle x, w\\rangle)$.\n\nВопрос на подумать. В каких ситуациях была бы полезной функция связи complementary log-log link (cloglog)\n\n$$g(x) = \\log(-log(1 - x))?$$'), Document(metadata={'source': '/Users/eugenekillevsky/PycharmProjects/LLM-Interviewer/data/RAG/ml-handbook-master/chapters/prob_intro/intro.md'}, page_content='title: Вероятностный подход в ML author: stanislav_fedotov\n\nЭтот список будет заменен оглавлением, за вычетом заголовка "Contents", к которому добавлен класс no_toc. {:toc}\n\nВ этой главе мы посмотрим на, казалось бы, те же самые модели машинного обучения с другой стороны, проинтерпретировав их, как вероятностные. В первом разделе мы расскажем, как обращаться с вероятностными моделями, и покажем, что привычный вам подбор параметров модели с помощью минимизации функции потерь соответствует их подбору методом максимального правдоподобия, что даст возможность транслировать в мир ML известные результаты о свойствах оценок максимального правдоподобия, но в то же время и обнажит их недостатки. Это позволит нам по-новому взглянуть на логистическую регрессию и с новым пониманием сформулировать её обобщение – generalized linear model (GLM). По ходу дела выяснится, что большинство классификаторов, хоть и делают вид, что предсказывают корректные вероятности, на самом деле вводят в заблуждение, и в третьем разделе мы поговорим о том, как проверить отклонение предсказанных значений от истинных вероятностей и как поправить ситуацию. Далее, мы обсудим генеративный подход к классификации и разберём несколько примеров генеративных моделей, после чего перейдём к байесовскому подходу оценивания параметров, который, хоть зачастую и трудно осуществим вычислительно, однако обладает большей теоретической стройностью, позволяет оценивать распределение параметров и предсказаний – то есть, например, уверенность в нашей оценке – а, кроме того, дает нам возможность измерить качество модели, не прибегая к проверке на тестовой выборке.\n\nСлучайность как источник несовершенства модели\n\nПрактически любая модель, которую мы строим, несовершенна. Но объяснять это несовершенство можно по-разному.\n\nПредставим, что мы решаем задачу регрессии $$y\\simeq \\langle x, w\\rangle$$: например, пытаемся по университетским оценкам выпускника предсказать его годовую зарплату. Ясно, что точная зависимость у нас не получится как минимум потому, что мы многого не знаем о выпускнике: куда он пошёл работать, насколько он усерден, как у него с soft skills и так далее – как же нам быть?\n\nПервый вариант – просто признать, что мы не получим идеальную модель, но постараться выучить насколько это возможно оптимальную, то есть приблизить таргет предсказаниями наилучшим образом с точки зрения какой-то меры близости, которую мы подберём из экспертных соображений. Так мы получаем простой инженерный подход к машинному обучению: есть формула, в которой присутствуют некоторые параметры ($$w$$), есть формализация того, что такое <<приблизить>> (функция потерь) – и мы бодро решаем задачу оптимизации по параметрам.\n\nВторой вариант – свалить вину за неточности наших предсказаний на случайность. В самом деле: если мы что-то не можем измерить, то для нас это всё равно что случайный фактор. В постановке задачи мы заменяем приближённое равенство $$y\\simeq \\langle x, w\\rangle$$ на точное\n\n$$y = \\left(\\langle x, w\\rangle, \\mbox{искажённое шумом $\\varepsilon$}\\right)$$\n\nНапример, это может быть аддитивный шум (чаще всего так и делают):\n\n$$y = \\langle x, w\\rangle + \\varepsilon$$\n\nгде $$\\varepsilon$$ – некоторая случайная величина, которая представляет этот самый случайный шум. Тогда получается, что для каждого конкретного объекта $$x_i$$ соответствующий ему истинный таргет – это сумма $$\\langle x_i, w\\rangle$$ и конкретной реализации шума $$\\varepsilon$$.\n\nПри построении такой модели мы можем выбирать различные распределения шума, кодируя тем самым, какой может быть ошибка. Чаще всего выбирают гауссовский шум: $$\\varepsilon\\sim\\mathcal{N}(0,\\sigma^2)$$ с некоторой фиксированной дисперсией $$\\sigma^2$$ – но могут быть и другие варианты.\n\nПроиллюстрируем, как ведут себя данные, подчиняющиеся закону $y = ax + b + \\varepsilon$, $\\varepsilon\\sim\\mathcal{N}(0, \\sigma^2)$:\n\n{: .center}\n\nВопрос на подумать. Зачем человеку может прийти в голову в модели линейной регрессии $y\\sim Xw + \\varepsilon$ предположить, что шум $\\varepsilon$ имеет распределение Лапласа? А распределение Коши? Чем свойства таких моделей будут отличаться от свойств модели с нормальным шумом?\n\nКак вы могли заметить, в каждом из подходов после того, как мы зафиксировали признаки (то есть координаты $x_i$), остаётся своя степень свободы: в инженерном это выбор функции потерь, а в вероятностном – выбор распределения шума. Дальше в этой главе мы увидим, что на самом деле эти два подхода глубинным образом связаны между собой, причём выбор функции потерь – это в некотором смысле то же самое, что выбор распределения шума.\n\nУсловное распределение на таргет, непрерывный случай\n\nДопустим, что мы исследуем вероятностную модель таргета с аддитивным шумом\n\n$$y = f_w(x) + \\varepsilon,$$\n\nгде $$f_w$$ – некоторая функция, не обязательно линейная с (неизвестными пока) параметрами $$w$$, а $$\\varepsilon$$ – случайный шум с плотностью распределения $$\\varepsilon\\sim p_{\\varepsilon}(t)$$. Для каждого конкретного объекта $$x_i$$ значение $$f_w(x_i)$$ является просто константой, но $$y_i$$ превращается в случайную величину, зависящую от $$x_i$$ (и ещё от $$w$$, на самом деле). Таким образом, можно говорить об условном распределении\n\n$$p_y(y \\vert x, w)$$\n\nДля каждого конкретного $$x_i$$ и $$w$$ распределение соответствующего $$y_i$$ – это просто $$p_{\\varepsilon}(y - f_{w}(x_i))$$, ведь $$y - f_w(X) = \\varepsilon$$.\n\nПример. Рассмотрим вероятностную модель $$y = \\langle x, w\\rangle + \\varepsilon$$, где $\\varepsilon\\sim\\mathcal{N}(0, \\sigma^2)$. Тогда для фиксированного $x_i$ имеем $y_i = \\langle x_i, w\\rangle + \\varepsilon$. Поскольку $\\langle x_i, w\\rangle$ – константа, мы получаем\n\n$$y_i\\sim\\mathcal{N}(\\langle x_i, w\\rangle, \\sigma^2).$$\n\nЭто можно записать и так:\n\n$$p(y_i\\vert x_i, w)\\sim\\mathcal{N}(y_i\\vert\\langle x_i, w\\rangle, \\sigma^2),$$\n\nгде выражение справа – это значение функции плотности нормального распределения с параметрами $\\langle x_i, w\\rangle, \\sigma^2$ в точке $y_i$. В частности, $$\\langle x_i, w\\rangle = \\mathbb{E}(y_i\\vert x_i)$$.\n\nБолее сложные вероятностные модели\n\nНа самом деле, мы можем для нашей задачи придумывать любую вероятностную модель $$p_y(y \\vert x, w)$$, не обязательно вида $$y = f_w(X) + \\varepsilon$$. Представьте, что мы хотим предсказывать точку в плоскости штанг, в которую попадает мячом бьющий по воротам футболист. Можно предположить, что она имеет нормальное распределение со средним (цель удара), которое определяется ситуацией на поле и состянием игрока, и некоторой дисперсией (т.е. скалярной ковариационной матрицей), которая тоже зависит от состояния игрока и ещё разных сложных факторов, которые мы объявим случайными. Состояние игрока – это сложное понятие, но, вероятно, мы можем выразить его, зная пульс, давление и другие физические показатели. В свою очередь, ситуацию на поле можно описать, как функцию от позиций и движений других игроков, судьи и зрителей – но всего не перечислишь, поэтому нам снова придётся привлекать случайность. Таким образом, мы получаем то, что называется графической моделью:\n\n{: .center}\n\nЗдесь стрелки означают статистические зависимости, а отсутствие стрелок – допущение о статистической независимости. Конечно же, это лишь допущение, принятое нами для ограничения сложности модели: ведь пульс человека и давление взаимосвязаны, равно как и поведение различных игроков на поле. Но мы уже обсуждали, что каждая модель, в том числе и вероятностная, является лишь приблизительным отражением бесконечно сложного мира. Впрочем, если у нас много вычислительных ресурсов, то никто не мешает нам попробовать учесть и все пропущенные сейчас зависимости.\n\nРасписав всё по определению условной вероятности, мы получаем следующую вероятностную модель:\n\n{: .center}\n\nв которой, конечно же, мы должны все вероятности расписать через какие-то понятные и логически обоснованные распределения – но пока воздержимся от этого.\n\nОценка максимального правдоподобия = оптимизация функции потерь\n\nМы хотим подобрать такие значения параметров $$w$$, для которых модель $$p_y(y \\vert x, w)$$ была бы наиболее адекватна обучающим данным. Суть метода максимального правдоподобия (maximum likelihood estimation) состоит в том, чтобы найти такое $$w$$, для которого вероятность (а в данном, непрерывном, случае плотность вероятности) появления выборки $$y = {y_1, \\ldots, y_N}$$ была бы максимальной, то есть\n\n$$\\widehat{w}_{MLE} = \\underset{w}{\\operatorname{argmax}}p(y \\vert X, w)$$\n\nВеличина $$p(y \\vert X, w)$$ называется функцией правдоподобия (likelihood). Если мы считаем, что все объекты независимы, то функция правдоподобия распадается в произведение:\n\n$$p(y \\vert X, w) = p(y_1 \\vert x_1, w) \\cdot\\ldots\\cdot p(y_i \\vert x_i, w)$$\n\nТеперь, поскольку перемножать сложно, а складывать легко (и ещё поскольку мы надеемся, что, раз наши объекты всё-таки наблюдаются в природе, их правдоподобие отлично от нуля), мы переходим к логарифму функции правдоподобия:\n\n$$l(y \\vert X,w) = \\log{p(y_1 \\vert x_1, w)} + \\ldots + \\log{p(y_i \\vert x_i, w)}$$\n\nэту функцию мы так или иначе максимизируем по $$w$$, находя оценку максимального правдоподобия $$\\hat{w}$$.\n\nКак мы уже обсуждали выше, $p(y_i \\vert x_i, w) = p_{\\varepsilon}(y - f_{w}(x_i))$, то есть\n\n$$l(y \\vert X,w) = \\sum\\limits_{i=1}^N\\log{p_{\\varepsilon}(y_i - f_w(x_i))}$$\n\nМаксимизация функции правдоподобия соответствует минимизации\n\n$$\\sum\\limits_{i=1}^N\\left[-\\log{p_{\\varepsilon}(y_i - f_w(x_i))}\\right]$$\n\nа это выражением можно интерпретировать, как функцию потерь. Вот и оказывается, что подбор параметров вероятностей модели с помощью метода максимального правдоподобия – это то же самое, что <<инженерная>> оптимизация функции потерь. Давайте посмотрим, как это выглядит в нескольких простых случаях.\n\nПример. Давайте предположим, что наш таргет связан с данными вот так:\n\n$$y_i = \\langle x_i, w \\rangle + \\varepsilon$$\n\nгде $$\\varepsilon\\sim\\mathcal{N}(0, \\sigma^2)$$, то есть\n\n$$p(\\varepsilon) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{\\varepsilon^2}{2\\sigma^2}\\right)$$\n\nСлучайная величина $$y_i$$ получается из шума $$\\varepsilon$$ сдвигом на постоянный вектор $$\\langle x_i, w \\rangle$$, так что она тоже распределена нормально с той же дисперсией $$\\sigma^2$$ и со средним $$\\langle x_i, w \\rangle$$\n\n$$p(y\\vert x, w) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(y - \\langle x_i, w \\rangle)^2}{2\\sigma^2}\\right)$$\n\nПравдоподобие выборки имеет вид\n\n$$p(y\\vert X, w) = \\prod_{i=1}^N p(y_i, \\vert x_i, w) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(y_i-\\langle w,x_i\\rangle)^2}{2\\sigma^2}\\right)$$\n\nЛогарифм правдоподобия можно переписать в виде\n\n$$ l(y \\vert X,w) = \\sum_{i=1}^N \\left(-\\log({\\sqrt{2 \\pi \\sigma^2}}) -\\frac{(y_i-\\langle w,x_i\\rangle)^2}{2\\sigma^2}\\right)$$\n\nПостоянными слагаемыми можно пренебречь, и тогда оказывается, что максимизация этой величины равносильна минимизации\n\n$$ \\sum_{i=1}^N (y_i-\\langle w,x_i\\rangle\\rangle)^2$$\n\nМы получили обычную квадратичную функцию потерь. Итак, обучать вероятностную модель линейной регрессии с нормальным шумом – это то же самое, что учить <<инженерную>> модель с функцией потерь MSE.\n\nВопрос на подумать. Какая вероятностная модель соответствует обучению линейной регрессии с функцией потерь MAE\n\n$$ \\sum_{i=1}^N \\vert y_i-\\langle w,x_i\\rangle\\vert?$$\n\nПредсказание в вероятностных моделях\n\nТеперь представим, что параметры подобраны, и подумаем о том, как же теперь делать предсказания.\n\nРассмотрим модель линейной регрессии\n\n$$y = \\langle x, w\\rangle + \\varepsilon,\\quad\\varepsilon\\sim\\mathcal{N}(0,\\sigma^2)$$\n\nЕсли $w$ известен, то для нового объекта $x_0$ соответствующий таргет имеет вид\n\n$$y_0 = \\langle x_0, w\\rangle + \\varepsilon\\sim\\mathcal{N}(\\langle x_0, w\\rangle, \\sigma^2)$$\n\nТаким образом, $y_0$ дан нам не точно, а в виде распределения (и логично: ведь мы оговорились выше, что ответы у нас искажены погрешностью, проинтерпретированной, как нормальный шум). Но что делать, если требуют назвать конкретное число? Кажется логичным выдать условное матожидание $$\\mathbb{E}(y_0\\vert x_0) = \\langle x_0, w\\rangle$$, тем более что оно совпадает с условной медианой и условной модой этого распределения.\n\nЕсли же медиана, мода и математическое ожидание различаются, то можно выбрать что-то из них с учётом особенностей задачи. Но на практике в схеме $$y\\sim f(x) + \\varepsilon$$ чаще всего рассматривают именно симметричные распределения с нулевым матожиданием, потому что для них $$f(x)$$ совпадает с условным матожиданием $$\\mathbb{E}(y\\vert x)$$ и является логичным точечным предсказанием.\n\nПриведём пример. Допустим шум $$\\varepsilon$$ был бы из экспоненциального распределения. Тогда $$f(x)$$ была бы условным минимумом распределения. В принципе, можно придумать задачу, для которой такая постановка (предсказание минимума) была бы логичной. Но это всё же довольно экзотическая ситуация. Приводим для сравнения модели с нормальным, лапласовским и экспоненциальным шумом:\n\n{: .center}\n\nУсловное распределение на таргет, дискретный случай\n\nДопустим, мы имеем дело с задачей классификации с $$K$$ классами. Как мы можем её решать? Самый наивный вариант – научиться по каждому объекту $$x_i$$ предсказывать некоторый некоторое число для каждого класса, и у кого число больше – тот класс и выбираем! Наверное, так можно сделать, если мы придумаем хорошую функцию потерь. Но сразу в голову приходит мысль: почему бы не начать предсказывать не просто число, а вероятность?\n\nТаким образом, задача классификации сводится к предсказанию\n\n$$P(y_i = k \\vert x_i)$$\n\nи как будто бы выбору класса с наибольшей вероятностью (как мы увидим дальше, всё не всегда работает так просто).\n\nОдну такую модель – правда, только для бинарной классификации – вы уже знаете. Это логистическая регрессия:\n\n$$P(y_i = 1 \\vert x_i,w) = \\frac{1}{1+e^{-\\langle x_i, w\\rangle}},\\quad P(y_i = 0 \\vert x_i,w) = \\frac{e^{-(x_i, w)}}{1+e^{-\\langle x_i, w\\rangle}} = \\frac{1}{1+e^{\\langle x_i, w\\rangle}}$$\n\nкоторую также можно записать в виде\n\n$$y_i \\vert x_i \\sim \\color{red}{Bern}\\left(\\frac{1}{1+e^{-\\langle x_i, w\\rangle}}\\right)$$ где $\\color{red}{Bern}(p)$ – распределение Бернулли с параметром $p$.\n\nНахождение вероятностей классов можно разделить на два этапа:\n\n$$x_i\\rightarrow{\\begin{smallmatrix}\\mbox{Находим}\\\\mbox{логиты}\\end{smallmatrix}} \\left(-\\langle x_i, w\\rangle, \\langle x_i, w\\rangle\\right)\\xrightarrow{\\ \\sigma\\ }\\left(\\sigma(-\\langle x_i, w\\rangle), \\sigma(\\langle x_i, w\\rangle)\\right)$$\n\nгде, напомним, $$\\sigma$$ – это сигмоида:\n\n$$\\sigma(t) = \\frac{1}{1+e^{-t}}$$\n\nСигмоида тут не просто так. Она обладает теми счастливыми свойствами, что\n\nмонотонно возрастает;\n\nотображает всю числовую прямую на интервал $$(0,1)$$;\n\n$$\\sigma(-x) = 1 - \\sigma(x)$$.\n\nВот такой вид имеет её график:\n\n{: .center }\n\nИными словами, с помощью сигмоиды можно делать «вероятности» из чего угодно, то есть более или менее для любого отображения $$f_w$$ (из признакового пространства в $$\\mathbb{R}$$) с параметрами $$w$$ построить модель бинарной классификации:\n\n$$P(y_i = 0 \\vert x_i, w) = \\sigma(f_w(-x_i)),\\quad P(y_i = 1 \\vert x_i, w) = \\sigma(f_w(x_i)).$$\n\nКак и в случае логистической регрессии, такая модель равносильна утверждению о том, что\n\n$$f_w(x_i) = \\log{\\frac{p(y = 1 \\vert x_i,w)}{p(y = 0 \\vert x_i, w)}}.$$\n\nПохожим способом можно строить и модели для многоклассовой классификации; в этом нам поможет обобщение сигмоиды, которое называется softmax:\n\n$$softmax(t_1,\\ldots,t_K) = \\left(\\frac{e^{t_1}}{\\sum_{k=1}^Ke^{t_k}},\\ldots,\\frac{e^{t_K}}{\\sum_{k=1}^Ke^{t_k}}\\right)$$\n\nА именно, для любого отображения $$f_w$$ из пространства признаков в $$\\mathbb{R}^K$$ мы можем взять модель\n\n$$\\left(P(y_i = k \\vert x_i, w)\\right)^K_{k=1} = softmax(f_w(x_i))$$\n\nЕсли все наши признаки – вещественные числа, а $$f_w(x_i) = x_iW$$ – просто линейное отображение, то мы получаем однослойную нейронную сеть\n\n$$\\left(P(y_i = k \\vert x_i, w)\\right)^K_{k=1} = softmax(x_iW)$$\n\n{: .center }\n\nПредостережение. Всё то, что мы описали выше, вполне работает на практике (собственно, классификационные нейросети зачастую так и устроены), но корректным не является. В самом деле, мы говорим, что строим оценки вероятностей $$P(y_i = k \\vert x_i, w)$$, но для подбора параметров используем не эмпирические вероятности, а только лишь значения $$\\underset{k}{\\operatorname{argmax}} \\ P(y_i = k \\vert x_i, w)$$, то есть метки предсказываемых классов. Таким образом, при обучении мы не будем различать следующие две ситуации:\n\n{: .center}\n\nЭто говорит нам о некоторой неполноценности такого подхода.\n\nЗаметим ещё вот что. В случае бинарной классификации выбор предсказываемого класса как $$\\underset{k}{\\operatorname{argmax}} P(y_i=k \\vert x_i,w)$$ равносилен выбору того класса, для которого $$P(y_i=k \\vert x_i,w) > \\frac{1}{2}$$. Но если наши оценки вероятностей неадекватны, то этот вариант проваливается, и мы встаём перед проблемой выбора порога: каким должно быть значение $$\\widehat{t}$$, чтобы мы могли приписать класс 1 тем объектам $$x_i$$, для которых $$\\sigma(f_w(x_i)) > \\widehat{t}$$?\n\nВ одном из следующих разделов текущей главы мы обсудим, как всё-таки правильно предсказывать вероятности.'), Document(metadata={'source': '/Users/eugenekillevsky/PycharmProjects/LLM-Interviewer/data/RAG/ml-handbook-master/chapters/prob_bayes/bayes.md'}, page_content='title: Байесовский подход к оцениванию author: stanislav_fedotov toc: true\n\nЭтот список будет заменен оглавлением, за вычетом заголовка "Contents", к которому добавлен класс no_toc. {:toc}\n\nНачнём с простого вопроса: как нам внести в модель априорные знания.\n\nА зачем, собственно?\n\nПредставьте, что мы обучаем модель линейной регрессии $$y\\sim \\langle x, w\\rangle + \\varepsilon$$, $$\\varepsilon\\sim\\mathcal{N}(0,\\sigma^2)$$. С помощью MLE мы получили некоторую оценку $$\\widehat{w}$$ на веса $$w$$ – всякие ли их значения мы встретим с покорностью и смирением? Наверное, мы удивимся, если какие-то компоненты вектора $$\\widehat{w}$$ будут очень большими по сравнению с элементами $$X$$: пожалуй, наша физическая интуиция будет бунтовать против этого, мы задумаемся о том, что из-за потенциальных ошибок сокращения вычисление предсказаний $$(x_i, \\widehat{w})$$ окажутся неточным – в общем, хотелось бы по возможности избежать этого. Но как?\n\nБудь мы приверженцами чисто инженерного подхода, мы бы сделали просто: прибавили бы к функции потерь слагаемое $$+\\alpha|w|^2$$, или $$+\\alpha \\vert w \\vert^2$$, или ещё что-то такое – тогда процедура обучения стала бы компромиссом между минимизацией исходного лосса и этой добавки, что попортило бы слегка близость $$y\\sim \\langle x, w \\rangle$$, но зато позволило бы лучше контролировать масштаб $$\\widehat{w}$$. Надо думать, вы узнали в этой конструкции старую добрую регуляризацию.\n\nНо наша цель – зашить наше априорное знание о том, что компоненты $$w$$ не слишком велики по модулю, в вероятностную модель. Введение в модель априорного знания соответствует введению априорного распределения на $$w$$. Какое распределение выбрать? Ну, наверное, компоненты $$w$$ будут независимыми (ещё нам не хватало задавать взаимосвязи между ними!), а каждая из них будет иметь какое-то непрерывное распределение, в котором небольшие по модулю значения более правдоподобны, а совсем большие очень неправдоподобны. Мы знаем такие распределения? Да, и сразу несколько. Например, нормальное. Логично было бы определить\n\n$$p(w) = \\prod_{i=1}^D\\mathcal{N}(w_i \\vert 0,\\tau^2)$$\n\nгде $$\\tau^2$$ – какая-то дисперсия, которую мы возьмём с потолка или подберём по валидационной выборке. Отметим, что выбор нормального распределение следует и из принципа максимальной энтропии: ведь у него наибольшая энтропия среди распределений на всей числовой оси с нулевым матожиданием и дисперсией $\\tau^2$.\n\nКонтроль масштаба весов – это, вообще говоря, не единственное, что мы можем потребовать. Например, мы можем из каких-то физических соображений знать, что тот или иной вес в линейной модели непременно должен быть неотрицательным. Тогда в качестве априорного на этот вес мы можем взять, например, показательное распределение (которое, напомним, обладает максимальной энтропией среди распределений на положительных числах с данным матожиданием).\n\nОцениваем не значение параметра, а его распределение\n\nРаз уж мы начали говорить о распределении на веса $$w$$, то почему бы не пойти дальше. Решая задачу классификации, мы уже столкнулись с тем, что может быть важна не только предсказанная метка класса, но и вероятности. Аналогичное верно и для задачи регрессии. Давайте рассмотрим две следующих ситуации, в каждой из коорых мы пытаемся построить регрессию $$y\\sim ax + b$$:\n\n{: .center}\n\nНесмотря на то, что в каждом из случаев \'\'точная формула\'\' или градиентный спуск выдадут нам что-то, степень нашей уверенности в ответе совершенно различная. Один из способов выразить (не)уверенность – оценить распределение параметров. Так, для примеров выше распределения на параметр $$a$$ могли бы иметь какой-то такой вид:\n\n{: .center}\n\nДальше мы постараемся формализовать процесс получения таких оценок.\n\nПостроение апостериорного распределения\n\nДавайте ненадолго забудем про линейную регрессию и представим, что мы подобрали с полу монету, которая выпадает орлом с некоторой неизвестной пока вероятностью $\\theta$. До тех пор, пока мы не начали её подкидывать, мы совершенно ничего не знаем о $\\theta$, эта вероятность может быть совершенно любой – то есть априорное распределение на $\\theta$ является равномерным (на отрезке $[0,1]$):\n\n$$p(\\theta) = \\mathbb{I}_{[0;1]}(\\theta)$$\n\nТеперь представим, что мы подкинули её $n$ раз, получив результаты $Y = (y_1,\\ldots,y_n)$ ($0$ – решка, $1$ – орёл), среди которых $n_0 = n - \\sum_{i=1}^ny_i$ решек и $n_1=\\sum_{i=1}^ny_i$ орлов. Определённо наши познания о числе $p$ стали точнее: так, если $n_1$ мало, то можно заподозрить, что и $p$ невелико (уже чувствуете, запахло распределением!). Распределение мы посчитаем с помощью формулы Байеса:\n\n$$\\color{#348FEA}{p(\\theta \\vert Y) = \\frac{p(\\theta , Y)}{p(Y)} = \\frac{p(Y \\vert \\theta)p(\\theta)}{ \\int p(Y \\vert \\psi)p(\\psi)d\\psi }}$$\n\nв нашем случае:\n\n$$p(\\theta \\vert Y) = \\frac{\\prod_{i=1}^n\\theta^{x_i}(1 - \\theta)^{1 - x_i}\\mathbb{I}{[0,1]}(\\theta)}{ \\int_0^1\\prod{i=1}^n\\psi^{x_i}(1 - \\psi)^{1 - x_i}d\\psi} = $$\n\n$$=\\frac{\\theta^{n_1}(1 - \\theta)^{n_0}\\mathbb{I}_{[0,1]}(\\theta)}{ \\int_0^1\\psi^{n_1}(1 - \\psi)^{n_0}d\\psi} $$\n\nВ этом выражении нетрудно узнать бета-распределение: $$\\text{Beta}(n_1 + 1, n_0 + 1)$$. Давайте нарисует графики его плотности для нескольких конкретных значений $n_0$ и $n_1$:\n\n{: style="width:45vw" }\n\nКак можно заметить, с ростом $n$ мы всё лучше понимаем, каким может быть $\\theta$, при этом если орёл выпадал редко, то пик оказывается ближе к нулю, и наоборот. Ширина пика в каком-то смысле отражает нашу уверенность в том, какими могут быть значения параметра, и не случайно чем больше у нас данных – тем уже будет пик, то есть тем больше уверенности.\n\nРаспределение $p(\\theta\\vert Y)$ параметра, полученное с учётом данных, называется апостериорным. Переход от априорного распределения к апостериорному отражает обновление нашего представления о параметрах распределения с учётом полученной информации, и этот процесс является сердцем байесовского подхода. Отметим, что если нам придут новые данные $Y\' = (y_1\',\\ldots,y_m\')$, в которых $m_0$ решек и $m_1$ орлов, мы сможем ещё раз обновить распределение по той же формуле Байеса:\n\n$$p(\\theta \\vert Y\\cup Y\') = p([\\theta\\vert Y]\\vert Y\') = \\frac{p(Y\'\\vert\\theta)p(\\theta\\vert Y)}{p(Y\')}= $$\n\n$$=\\frac{\\theta^{m_1}(1 - \\theta)^{m_0}\\frac{\\theta^{n_1}(1 - \\theta)^{n_0}}{B(n_1 + 1, n_0 + 1)}\\mathbb{I}_{[0,1]}(\\theta)}{ \\text{злой интеграл}} = $$\n\n$$=\\frac{\\theta^{n_1 + m_1}(1 - \\theta)^{n_0 + m_0}}{\\text{константа}}\\sim\\text{Beta}(n_1 + m_1 + 1, n_0 + m_0 + 1)$$\n\nВопрос на подумать. Пусть $p(y\\vert\\mu) = \\mathcal{N}(y\\vert\\mu,\\sigma^2)$ – нормальное распределение с фиксированной дисперсией $\\sigma^2$, а для параметра $\\mu$ в качестве априорного выбрано также нормальное распределение $\\mathcal{N}(y\\vert \\lambda,\\theta^2)$. Каким будет апостериорное распределение при условии данных $Y = (y_1,\\ldots,y_n)$?\n\nСопряжённые распределения\n\nВ двух предыдущих примерах нам очень сильно повезло, что апостериорные распределения оказались нашими добрыми знакомыми. Если же взять случайную пару распределений $p(y\\vert\\theta)$ и $p(\\theta)$, результат может оказаться совсем не таким приятным. В самом деле, нет никакой проблемы в том, чтобы посчитать числитель формулы Байеса, но вот интеграл в знаменателе может и не найтись. Поэтому выбирать распределения нужно с умом. Более того, поскольку апостериорное распределение само станет априорным, когда придут новые данные, хочется, чтобы априорное и апостериорное распределения были из одного семейства; пары (семейств) распределений $p(y\\vert\\theta)$ и $p(\\theta)$, для которых это выполняется, называются сопряжёнными ($p\\theta)$ называется сопряжённым к $p(y\\vert\\theta)$). Полезно помнить несколько наиболее распространённых пар сопряжённых распределений:\n\n$p(y\\vert\\theta)$ – распределение Бернулли с вероятностью успеха $\\theta$, $p(\\theta)$ – бета распределение;\n\n$p(y\\vert\\mu)$ – нормальное с матожиданием $\\mu$ и фиксированной дисперсией $\\sigma^2$, $p(\\theta)$ также нормальное;\n\n$p(y\\vert\\lambda)$ – показательное с параметром $\\lambda$, $p(\\lambda)$ – гамма распределение;\n\n$p(y\\vert\\lambda)$ – пуассоновское с параметром $\\lambda$, $p(\\lambda)$ – гамма распределение;\n\n$p(y\\vert\\theta)$ – равномерное на отрезке $[0,\\theta]$, $p(\\theta)$ – Парето;\n\nВозможно, вы заметили, что почти все указанные выше семейства распределений (кроме равномерного и Парето) относятся к экспоненциальному классу. И это не случайность! Экспоненциальный класс и тут лучше всех: оказывается, что для $p(y\\vert\\theta)$ из экспоненциального класса можно легко подобрать сопряжённое $p(\\theta)$. Давайте же это сделаем.\n\nПусть $p(y\\vert\\theta)$ имеет вид\n\n$$p(y\\vert\\theta) = \\frac1{h(\\theta)}g(y)\\exp(\\theta^Tu(y))$$\n\nПоложим\n\n$$p(\\theta) = \\frac1{h^{\\nu}(\\theta)}\\exp(\\eta^T\\theta)\\cdot f(\\eta, \\nu)$$\n\nгде $$f(\\eta, \\nu)$$ – множитель, обеспечивающий равенство единице интеграла от этой функции. Найдём апостериорное распределение:\n\n$$p(\\theta\\vert Y) = \\frac{\\left[\\prod_{i=1}^n\\frac1{h(\\theta)}g(y_i)\\exp(\\theta^Tu(y_i))\\right]\\frac1{h^{\\nu}(\\theta)}\\exp(\\eta^T\\theta)\\cdot f(\\eta, \\nu)}{\\text{злой интеграл}} = $$\n\n$$= \\frac{\\frac1{h^{\\nu + n}(\\theta)}\\exp\\left(\\theta^T\\left[\\eta + \\sum_{i=1}^nu(y_i)\\right]\\right)}{\\text{что-то, где нет }\\theta}$$\n\nЭто распределение действительно из того же семейства, что и $p(\\theta)$, только с новыми параметрами:\n\n$$\\eta_{new} = \\eta + \\sum_{i=1}^nu(y_i),\\quad\\nu_{new} = \\nu + n$$\n\nПример. Пусть $p(y\\vert q) = q^y(1 - q)^{1 - y}$ подчиняется распределению Бернулли. Напомним, что оно следующим образом представляется в привычном для экспоненциального класса виде:\n\n$$p(y\\vert q) = \\underbrace{(1 - q)}{=\\frac1{h(q)}}\\exp\\left(\\underbrace{y}{=u_1(y)}\\underbrace{\\log{\\frac{q}{1 - q}}}_{=\\theta}\\right)$$\n\nПредлагается брать априорное распределение вида\n\n$$p(q) = \\frac{(1 - q)^{\\nu}\\exp\\left(\\eta\\log{\\frac{q}{1-q}}\\right)}{\\text{что-то, где нет}q}$$\n\nТогда апостериорное распределение будет иметь вид (проверьте, посчитав по формуле Байеса!)\n\n$$p(q\\vert Y) = \\frac{(1 - q)^{\\nu + n}\\exp\\left(\\left[\\eta + \\sum_{i=1}^ny_i\\right]\\log{\\frac{q}{1-q}}\\right)}{\\text{что-то, где нет}q}$$\n\nПревратив логарифм частного в сумму, а экспоненту суммы в произведение, легко убедиться, что получается то самое бета распределение, которое мы уже получали выше.\n\nОценка апостериорного максимума (MAP)\n\nАпостериорное распределение – это очень тонкий инструмент анализа данных, но иногда надо просто сказать число (или же интеграл в знаменателе не берётся и мы не можем толком посчитать распределение). В качестве точечной оценки логично выдать самое вероятное значение $$\\theta\\vert Y$$ (интеграл в знаменателе от $$\\theta$$ не зависит, поэтому на максимизацию не влияет):\n\n$$\\color{blue}{\\widehat{\\theta}_{MAP} = \\underset{\\theta}{\\operatorname{argmax}}{p(\\theta \\vert Y)} = \\underset{\\theta}{\\operatorname{argmax}}{p(Y \\vert \\theta)p(\\theta)}}$$\n\nЭто число называется оценкой апостериорного максимума (MAP).\n\nЕсли же в формуле выше перейти к логарифмам, то мы получим кое-что, до боли напоминающее старую добрую регуляризацию (и не просто так, как мы вскоре убедимся!):\n\n$$\\underset{\\theta}{\\operatorname{argmax}}{p(Y \\vert \\theta)p(\\theta)} = \\underset{\\theta}{\\operatorname{argmax}}\\log(p(Y \\vert \\theta)p(\\theta)) = $$\n\n$$=\\underset{\\theta}{\\operatorname{argmax}}\\left(\\vphantom{\\frac12}\\log{p(Y \\vert \\theta)} + \\log{p(\\theta)}\\right)$$\n\nПример. Рассмотрим снова распределение Бернулли $p(y\\vert q)$ и априорное распределение $p(q)\\sim\\text{Beta}(q\\vert a, b)$. Тогда MAP-оценка будет равна\n\n$$\\underset{q}{\\operatorname{argmax}}{p(Y \\vert q)p(q)} = \\underset{q}{\\operatorname{argmax}}{q^{\\sum_{i=1}^ny_i}(1 - q)^{n - \\sum_{i=1}^ny_i}\\cdot q^{a - 1}(1 - q)^{b - 1}} = $$\n\n$$\\underset{q}{\\operatorname{argmax}}\\left((a - 1 + \\sum_{i=1}^ny_i)\\log{q} + (b - 1 + n - \\sum_{i=1}^ny_i)\\log(1 - q)\\right)$$\n\nДифференцируя по $q$ и приравнивая производную к нулю, мы получаем\n\n$$q = \\frac{a + \\sum_{i=1}^ny_i - 1}{a + b + n - 2}$$\n\nВ отличие от оценки максимального правдоподобия $\\frac{\\sum_{i=1}^ny_i}{n}$ мы здесь используем априорное знание: параметры $(a - 1)$ и $(b - 1)$ работают как <<память о воображаемых испытаниях>>, как будто бы до того, как получить данные $y_i$, мы уже имели $(a - 1)$ успехов и $(b - 1)$ неудач.\n\nСвязь MAP- и MLE-оценок\n\nОценка максимального правдоподобия является частным случаем апостериорной оценки. В самом деле, если априорное распределение является равномерным, то есть $$p(\\theta)$$ не зависит $$\\theta$$ (если веса $$\\theta$$ вещественные, могут потребоваться дополнительные усилия, чтобы понять, как такое вообще получается), и тогда\n\n$$\\widehat{\\theta}{MAP} = \\underset{\\theta}{\\operatorname{argmax}}\\log{p(Y \\vert \\theta)p(\\theta)} = \\underset{\\theta}{\\operatorname{argmax}}\\left(\\log{p(Y \\vert \\theta)} + \\underbrace{\\log{p(\\theta)}}{=const}\\right) =$$\n\n$$= \\underset{\\theta}{\\operatorname{argmax}}\\log{p(y \\vert \\theta)} = \\widehat{\\theta}_{MLE}$$\n\nБайесовские оценки для условных распределений\n\nВ предыдущих разделах мы разобрали, как байесовский подход работает для обычных, не условных распределений. Теперь вернёмся к чему-то более близкому к машинному обучению, а именно к распределениям вида $y\\vert x,w$, и убедимся, что для них байесовских подход работает точно так же, как и для обычных распределений.\n\nИмея некоторое распределение $p(y\\vert x, w)$, мы подбираем для него априорное распределение на веса $p(w)$ (и да, оно не зависит от $x$: ведь априорное распределение существует ещё до появления данных) и вычисляем апостериорное распределение на веса:\n\n$$p(w \\vert X, y)$$\n\nВычислять его мы будем по уже привычной формуле Байеса:\n\n$$\\color{blue}{p(w \\vert X, y) = \\frac{p(y, w \\vert X)}{p(y)} = \\frac{p(y \\vert X, w)p(w)}{p(y)}}$$\n\nПовторим ещё разок, в чём суть байесовского подхода: у нас было некоторое априорное представление $$\\color{blue}{p(w)}$$ о распределении весов $$\\color{blue}{w}$$, а теперь, посмотрев на данные $(x_i, y_i)_{i=1}^n$, мы уточняем своё понимание, формулируя апостериорное представление $$p(w \\vert X, y)$$.\n\nЕсли же нам нужна только точечная оценка, мы можем ограничиться оценкой апостериорного максимума (MAP): $$\\color{blue}{\\widehat{w}_{MAP} = \\underset{w}{\\operatorname{argmax}}{p(w \\vert X,y)} = \\underset{w}{\\operatorname{argmax}}{p(y \\vert X, w)p(w)}} = $$\n\n$$=\\underset{w}{\\operatorname{argmax}}\\left(\\vphantom{\\frac12}\\log{p(y \\vert X, w)} + \\log{p(w)}\\right)$$\n\nчто уже до неприличия напоминает регуляризованную модель\n\nПример: линейная регрессия с $$L^2$$-регуляризацией как модель с гауссовским априорным распределением на веса\n\nВ модели линейной регрессии $$y = \\langle x, w\\rangle + \\varepsilon$$, $$\\varepsilon\\sim\\mathcal{N}(0, \\sigma^2)$$ введём априорное распределение на веса вида\n\n$$\\color{blue}{p(w) = \\mathcal{N}(w \\vert 0, \\tau^2I) = \\prod_{j=1}^D \\mathcal{N}(w_j \\vert 0, \\tau^2) = \\prod_{j=1}^D p(w_j)}$$\n\nТогда $$\\widehat{w}_{MAP}$$ является точкой минимума следующего выражения:\n\n$$-\\log{p(y \\vert X, w)} - \\log{p(w)} =-\\sum_{i=1}^Np(y_i \\vert x_i, w) - \\sum_{j=1}^Dp(w_j) =$$\n\n$$=-\\sum_{i=1}^N\\left(-\\frac12\\log(2\\pi\\sigma^2) - \\frac{(y_i - (w, x_i))^2}{2\\sigma^2}\\right) -\\sum_{j=1}^D\\left(-\\frac12\\log(2\\pi\\tau^2) - \\frac{w_j^2}{2\\tau^2}\\right)=$$\n\n$$= \\frac1{2\\sigma^2}\\sum_{i=1}^N(y_i - (w, x_i))^2 + \\frac1{2\\tau^2}\\sum_{j=1}^D w_j^2+\\mbox{ не зависящие от $w$ члены}$$\n\nПолучается, что\n\n$$\\color{blue}{\\widehat{w}{MAP} = \\underset{w}{\\operatorname{argmin}}\\left(\\vphantom{\\frac12}\\sum{i=1}^N(y_i - (w, x_i))^2 + \\frac{\\sigma^2}{\\tau^2}|w|^2\\right)}$$\n\nа это же функция потерь для линейной регрессии с $$L^2$$-регуляризацией! Напомним на всякий случай, что у этой задачи есть \'\'точное\'\' решение\n\n$$\\color{blue}{\\widehat{w}_{MAP} = \\left(X^TX + \\frac{\\sigma^2}{\\tau^2}I\\right)^{-1}X^Ty}$$\n\nДля этого примера мы можем вычислить и апостериорное распределение $$p(w \\vert X, y)$$. В самом деле, из написанного выше мы можем заключить, что\n\n$$\\log{p(w \\vert X, y)} = \\log(p(y \\vert X, w)p(w)) - \\log{p(y)} = $$\n\n$$ =\\frac1{2\\sigma^2}(y - Xw)^T(y - Xw) + \\frac1{2\\tau^2}w^Tw+\\mbox{ не зависящие от $w$ члены}$$\n\nТаким образом, $$\\log{p(w \\vert X, y)}$$ – это квадратичная функция от $$w$$, откуда следует, что апостериорное распределение является нормальным. Чтобы найти его параметры, нужно немного преобразовать полученное выражение:\n\n$$ \\ldots=\\frac1{2\\sigma^2}(y^Ty - w^TX^Ty - y^TWx + w^TX^TXw) + \\frac1{2\\tau^2}w^Tw+\\mathrm{const}(w) =$$\n\n$$=w^T\\left(\\frac1{2\\sigma^2}X^TX + \\frac1{2\\tau^2}I\\right)w - \\frac{1}{2\\sigma^2}w^TX^Ty - \\frac1{2\\sigma^2}y^TWx + \\mathrm{const}(w) =$$\n\n$$=\\frac12\\left(w - \\widehat{w}{MAP}\\right)^T\\left(\\frac1{\\sigma^2}X^TX + \\frac1{\\tau^2}I\\right)\\left(w - \\widehat{w}{MAP}\\right) + \\mathrm{const}(w)=$$\n\nТаким образом,\n\n$$\\color{blue}{p(w \\vert X,y) = \\mathcal{N}\\left(\\widehat{w}_{MAP}, \\left(\\frac1{\\sigma^2}X^TX + \\frac1{\\tau^2}I\\right)^{-1} \\right)}$$\n\nКак видим, от априорного распределения оно отличается корректировкой как матожидания $$0\\mapsto\\widehat{w}_{MAP}$$, так и ковариационной матрицы $$\\left(\\frac1{\\tau^2}I\\right)^{-1}\\mapsto\\left(\\frac1{\\sigma^2}X^TX + \\frac1{\\tau^2}I\\right)^{-1}$$. Отметим, что $$X^TX$$ – это, с точностью до численного множителя, оценка ковариационной матрицы признаков нашего датасета (элементы матрицы $$X^TX$$ – это скалярные произведения столбцов $$X$$, то есть столбцов значений признаков).\n\nИллюстрация. Давайте на простом примере (датасет с двумя признаками) посмотрим, как меняется апостериорное распределение $$w$$ с ростом размера обучающей выборки:\n\nКак видим, не только мода распределения, то есть $$\\widehat{w}_{MAP}$$ приближается к своему истинному значению, но и дисперсия распределения постепенно уменьшается.\n\nЕщё иллюстрация. Теперь рассмотрим задачу аппроксимации неизвестной функции одной переменной (чьи значения в обучающей выборке искажены нормальным шумом) многочленом третьей степени. Её, разумеется, тоже можно решать, как задачу линейной регрессии на коэффициенты многочлена. Давайте нарисуем, как будут выглядеть функции, сгенерированные из распределения $${p(w \\vert X,y)}$$ для разного объёма обучающей выборки:\n\n{: style="width:50vw"}\n\nТут тоже видим, что функции не только становятся ближе к истинной, но и разброс их уменьшается.\n\nПример: линейная регрессия с $$L^1$$-регуляризацией как модель с лапласовским априорным распределением на веса\n\nДругим распределением, которое тоже может кодировать наше желание, чтобы небольшие по модулю значения $$w_j$$ были правдоподобными, а большие не очень, является распределение Лапласа. Посмотрим, что будет, если его взять в качестве априорного распределения на веса.\n\n$$\\color{blue}{p(w) = \\prod_{j=1}^D p(w_j) = \\prod_{j=1}^D\\frac{\\lambda}{2}\\exp(-\\lambda|w_m|)}$$\n\nПроводя такое же вычисление, получаем, что\n\n$$\\color{blue}{\\widehat{w}{MAP} = \\underset{w}{\\operatorname{argmin}}\\left(\\vphantom{\\frac12}\\sum{i=1}^N(y_i - (w, x_i))^2 + \\lambda\\sum_{j=1}^D|w_j|\\right)}$$\n\nа это же функция потерь для линейной регрессии с $$L^1$$-регуляризацией!\n\nЕсли мы нашли распределение $$w$$, как делать предсказания?\n\nВсе изложенные выше рассуждения проводились в ситуации, когда $$X = X_{train}$$ – обучающая выборка. Для неё мы можем посчитать\n\n$$p(w \\vert X_{train}, y_{train}) = \\frac{(y \\vert X,w)p(w)}{p(y)}$$\n\nи точечную апостериорную оценку $$\\widehat{w}_{MAP} = \\underset{w}{\\operatorname{argmax}}{p(y \\vert X,w)p(y)}$$. А теперь пусть нам дан новый объект $$x_0\\in\\mathbb{X}$$. Какой таргет $$y_0$$ мы для него предскажем?\n\nБыло бы естественным, раз уж мы предсказываем распределение для $$w$$, и для $$y_0$$ тоже предсказывать распределение. Делается это следующим образом:\n\n$$p(y_0 \\vert x_0, X_{train}, y_{train}) = \\int{p(y_0 \\vert x_0,w)p(w \\vert X_{train}, y_{train})}dw$$\n\nНадо признать, что вычисление этого интеграла не всегда является посильной задачей, поэтому зачастую приходится \'\'просто подставлять $$\\widehat{w}{MAP}$$\'\'. В вероятностных терминах это можно описать так: вместо сложного апостериорного распределения $$p(w \\vert X{train}, y_{train})$$ мы берём самое грубое на свете приближение\n\n$$p(w \\vert X_{train}, y_{train})\\approx\\delta(w - \\widehat{w}_{MAP}),$$\n\nгде $$\\delta(t)$$ – дельта-функция, которая не является честной функцией (а является тем, что математики называют обобщёнными функциями), которая определяется тем свойством, что $$\\int f(t)\\delta(t)dt = f(0)$$ для достаточно разумных функций $$f$$. Если не мудрствовать лукаво, то это всё значит, что\n\n$$p(y_0 \\vert x_0,X_{train}, y_{train})\\approx p(y_0 \\vert x_0,\\widehat{w}_{MAP})$$\n\nПример. Пусть $$y\\sim Xw + \\varepsilon$$, $$\\varepsilon\\sim\\mathcal{N}(0,\\sigma)^2$$ – модель линейной регрессии с априорным распределением $$p(w) = \\mathcal{N}(0,\\tau^2)$$ на параметры. Тогда, как мы уже видели раньше,\n\n$$p(w \\vert X,y) = \\mathcal{N}\\left(w \\vert \\widehat{w}_{MAP}, \\left(\\frac1{\\sigma^2}X^TX + \\frac1{\\tau^2}I\\right)^{-1} \\right)$$\n\nПопробуем для новой точки $$x_0$$ посчитать распределение на $$y_0$$. Рекомендуем читателю попробовать самостоятельно посчитать интеграл или же обратиться к пункту 7.6.2 книжки \'\'Machine Learning A Probabilistic Perspective\'\' автора Kevin P. Murphy, убедившись, что\n\n$$p(y_0 \\vert x_0, X_{train}, y_{train}) = \\mathcal{N}\\left(y_0 \\vert x_0\\widehat{w}_{MAP}, \\sigma^2 + \\sigma^2x_0^T\\left(X^TX + \\frac{\\sigma^2}{\\tau^2}I\\right)^{-1}x_0\\right)$$\n\nчто, очевидно, более содержательно, чем оценка, полученная с помощью приближения $$p(w \\vert X_{train}, y_{train})\\approx\\delta(w - \\widehat{w}_{MAP})$$:\n\n$$p(y_0 \\vert x_0, \\widehat{w}{MAP}) = \\mathcal{N}\\left(y_0\\left \\vert x_0\\widehat{w}{MAP}, \\sigma^2\\right.\\right)$$\n\nСобственно, видно, что в этом случае\n\nПример в примере. Рассмотрим полюбившуюся уже нам задачу приближения функции многочленом степени не выше $$3$$ (в которой мы строим модели с $$\\sigma^2 = \\tau^2 = 1$$). Для $$N = 8$$ мы получали такую картинку:\n\n{: style="width:30vw"}\n\nЕсли оценить по приведённым выше формулам $$p(y_0 \\vert x_0, X_{train}, y_{train})$$ для разных $$x_0$$, то можно убедиться, что модель в большей степени уверена в предсказаниях для точек из областей, где было больше точек из обучающей выборки:\n\n{: style="width:45vw"}\n\nБайесовский подход и дообучение моделей\n\nДо сих пор мы в основном рассуждали о моделях машинного обучения как о чём-то, что один раз обучается и дальше навсегда застывает в таком виде, но в жизни такое скорее редкость. Мы пока не будем обсуждать изменчивость истинных зависимостей во времени, но даже если истина неизменна, к нам могут поступать новые данные, которые очень хотелось бы использовать для дообучения модели.\n\nОбычные, не байесовские вероятностные модели не предоставляют таких инструментов. Оценку максимального правдоподобия придётся пересчитывать заново (хотя, конечно, можно схитрить, использовав старое значение в качестве начального приближения при итеративной оптимизации). Байесовский же подход позволяет оформить дообучения в виде простой и элегантной формулы: при добавлении новых данных $(x_{N+1}, y_{N+1}),\\ldots,(x_M, y_M)$ имеем\n\n$$p\\left(w\\vert (x_i, y_i){i=1}^M\\right) = \\frac{p\\left((y_i){i=N+1}^M\\vert (x_i){i=N+1}^M\\right) p\\left(w\\vert (x_i, y_i){i=1}^N\\right)}{p\\left( (y_i)_{i=N+1}^M \\right)}$$\n\nБайесовский подход к выбору модели: мотивация\n\nНам часто приходится выбирать: дерево или случайный лес, линейная модель или метод ближайших соседей; да, собственно, и внутри наших вероятностных моделей есть параметры (скажем, дисперсия шума $$\\sigma^2$$ и $$\\tau^2$$), которые надо бы подбирать. Но как?\n\nВ обычной ситуации мы выбираем модель, обученную на выборке $$(X_{train}, y_{train})$$ в зависимости от того, как она себя ведёт на валидационной выборке $$(X_{val}, y_{val})$$ (сравниваем правдоподобие или более сложные метрики) – или же делаем кросс-валидацию. Но как сравнивать модели, выдающие распределение?\n\nОтветим вопросом на вопрос: а как вообще сравнивать модели? Назначение любой модели – объяснять мир вокруг нас, и её качество определяется именно тем, насколько хорошо она справляется с этой задачей. Тестовая выборка – это хороший способ оценки, потому что она показывает, насколько вписываются в модель новые данные. Но могут быть и другие соображения, помогающие оценить качество модели.\n\nПример\n\nАналитик Василий опоздал на работу. Своему руководителю он может предложить самые разные объяснения – и это будет выработанная на одном обучающем примере модель, описывающая причины опоздания (и потенциально позволяющая руководителю принять решение о том, карать ли Василия). Конечно, руководитель мог бы принять изложенную Василием модель к сведению, подождать, пока появятся другие опоздавшие, и оценить её, так скажем, на тестовой выборке, но стоит ли? Давайте рассмотрим несколько конкретных примеров:\n\nМодель \'\'Василий опоздал, потому что так получилось\'\', то есть факт опоздания – это просто ни от чего не зависящая случайная величина. Такая модель плоха тем, что (а) не предлагает, на самом деле, никакого объяснения тому факту, что Василий опоздал, а его коллега Надежда не опоздала и (б) совершенно не помогает решить, наказывать ли за опоздание. Наверное, такое не удовлетворит руководителя.\n\nМодель \'\'Василий опоздал, потому что рядом с его домом открылся портал в другой мир, где шла великая битва орков с эльфами, и он почувствовал, что посто обязан принять в ней участие на стороне орков, которых привёл к победе, завоевав руку и сердце орочьей принцессы, после чего был перенсён обратно в наш скучный мир завистливым шаманом\'\'. Чем же она плоха? Битва с эльфами – это, безусловно, важное и нужное дело, и на месте руководителя мы бы дружно согласились, что причина уважительная. Но заметим, что в рамках этой модели можно объяснить множество потенциальных исходов, среди которых довольно маловероятным представляется наблюдаемый: тот, в котором Василий не погиб в бою, не остался со своей принцессой и не был порабощён каким-нибудь завистливым шаманом. Отметим и другой недостаток этой модели: её невозможно провалидировать. Если в совершенно случайной модели можно оценить вероятность опоздания и впоследствии, когда накопятся ещё примеры, проверить, правильно ли мы её посчитали, то в мире, где открываются порталы и любой аналитик может завоевать сердце орочьей принцессы, возможно всё, и даже если больше никто не попадёт в такую ситуацию, Василий всё равно сможет бить себя в грудь кулаком и говорить, что он избранный. Так что, наверное, это тоже не очень хорошая модель.\n\nМодель \'\'Василий опоздал, потому что проспал\'\' достаточно проста, чтобы в неё поверить, и в то же время даёт руководителю возможность принять решение, что делать с Василием.\n\nПример\n\nОбратимся к примеру из машинного обучения. Сравним три модели линейной регрессии:\n\nДаже и не заправшивая тестовую выборку, мы можем сделать определённые выводы о качестве этих моделей. Средняя (квадратичная) явно лучше левой (линейной), потому что она лучше объясняет то, что мы видим: тот факт, что облако точек обучающей выборки выглядит вогнутым вниз.\n\nА что с правым, почему мы можем утверждать, что он хуже? Есть много причин критиковать его. Остановимся вот на какой. На средней картинке у нас приближение квадратичной функцией, а на правой – многочленом довольно большой степени (на самом деле, десятой). А ради интереса: как выглядит график квадратичной функции и как – многочлена десятой степени со случайно сгенерированными коэффициентами? Давайте сгенерируем несколько и отметим их значения в точках обучающей выборки:\n\n{: style="width:40vw" }\n\nОбратите внимание на масштаб на графиках справа. И какова вероятность, что нам достался именно тот многочлен десятой степени, у которого значения в обучающих точках по модулю в пределах сотни? Очевидно, она очень мала. Поэтому мы можем сказать, что выбор в качестве модели многочлена десятой степени не очень обоснован.\n\nПопробуем резюмировать\n\nСлишком простая модель плохо объясняет наблюдаемые нами данные, тогда как слишком сложная делает это хорошо, но при этом описывает слишком многообразный мир, в котором имеющиеся у нас данные оказываются уже слишком частным случаем. В каком-то смысле наш способ выбора модели оказывается переформулировкой бритвы Оккама: из моделей, пристойно описывающих наблюдаемые явления, следует выбирать наиболее минималистичную.\n\nБайесовский подход к выбору модели: формализация\n\nПусть у нас есть некоторое семейство моделей $$\\mathcal{J}$$ и для каждого $$j\\in\\mathcal{J}$$ задана какая-то своя вероятностная модель. В духе байесовского подхода было бы оценить условное распределение моделей\n\n$$p(j \\vert y, X) = \\frac{p(y \\vert X,j)p(j)}{\\sum\\limits_{j\\in\\mathcal{J}}p(j, y \\vert X)}$$\n\nи в качестве наилучшей модели взять её моду. Если же считать все модели равновероятными, то мы сводим всё к максимизации только лишь $$p(y \\vert X,j) = p_j(y \\vert X)$$:\n\n$$\\color{blue}{\\widehat{\\jmath} = \\underset{j}{\\operatorname{argmax}}\\int{p_j(y \\vert X,w)p_j(w)}dw =\\underset{j}{\\operatorname{argmax}}p_j(y \\vert X)}$$\n\nВеличина $$p_j(y \\vert X)$$ называется обоснованностью (evidence, marginal likelihood) модели.\n\nОтметим, что такое определение вполне согласуется с мотивацией из предыдущего подраздела. Слишком простая модель плохо описывает наблюдаемые данные, и потому будет отвергнута. В свою очередь, слишком сложная модель способна описывать гораздо большее многообразие явлений, чем нам было бы достаточно. Таким образом, компромисс между качеством описания и сложностью и даёт нам оптимальную модель.\n\nПример\n\nВернёмся к нашей любимой задаче аппроксимации функции одной переменной многочленом небольшой степени по нескольким точкам, значение в которых было искажено нормальным шумом. Построим несколько моделей, приближающих многочленом степени не выше некоторого $$\\mathrm{deg}$$ (будет принимать значения 1, 3 и 6), положив в вероятностной модели $$\\sigma^2 = \\tau^2 = 1$$.\n\nМы не будем приводить полный вывод обоснованности для задачи регрессии $$p(y \\vert X,w) = \\mathcal{N}(y \\vert Xw,\\sigma^2I)p(w \\vert \\tau^2I)$$, а сразу выпишем ответ:\n\n$$p(y \\vert X) = \\mathcal{N}\\left(0, \\sigma^2I + \\tau^2XX^T\\right)$$\n\nПосмотрим, какой будет обоснованность для разного числа обучающих точек:\n\nМожно убедиться, что для регрессии по двум точкам наиболее обоснованной является линейная модель (и неудивительно), тогда как с ростом числа точек более обоснованной становится модель с многочленом третьей степени; слишком сложная же модель шестой степени всегда плетётся в хвосте.\n\nАппроксимация обоснованности и байесовский информационный критерий\n\nТочно вычислить обоснованность может быть трудной задачей (попробуйте проделать это сами хотя бы для линейной регрессии!). Есть разные способы посчитать её приближённо; мы рассмотрим самый простой. Напомним, что\n\n$$p(y \\vert X) = \\int{p(y \\vert X,w)p(w)}dw $$\n\nВоспользуемся приближением Лапласа, то есть разложим $$p(y \\vert X,w)$$ (как функцию от $$w$$) вблизи своего максимума, то есть вблизи $$\\widehat{w} := \\widehat{w}_{MLE}$$ в ряд Тейлора:\n\n$$\\log{p(y \\vert X,w)} \\approx \\log{p(y \\vert X,\\widehat{w})} - \\frac12(w - \\widehat{w})^TI_N(\\widehat{w})(w - \\widehat{w}),$$\n\nгде линейный член отсутствует, поскольку разложение делается в точке локального экстремума, а $$I(\\widehat{w})$$ – знакомая нам матрица Фишера $$I_N(\\widehat{w}) = -\\mathbb{E}\\nabla^2_w\\log{p(y \\vert X,w)}\\vert_{\\widehat{w}} = NI_1(\\widehat{w})$$.\n\nДалее, $$p(w)$$ мы можем с точностью до второго порядка приблизить $$p(\\widehat{w}_{MAP})$$. Получается, что\n\n$$p(y \\vert X)\\approx\\int e^{\\log{p(y \\vert X,\\widehat{w})} - \\frac{N}2(w - \\widehat{w})^TI_1(\\widehat{w})(w - \\widehat{w})}p(\\widehat{w}_{MAP})dw =$$\n\n$$= e^{\\log{p(y \\vert X,\\widehat{w})}}p(\\widehat{w}_{MAP})\\int e^{ - \\frac{N}{2}(w - \\widehat{w})^TI_1(\\widehat{w})(w - \\widehat{w})}dw = $$\n\n$$= e^{\\log{p(y \\vert X,\\widehat{w})}}p(\\widehat{w}_{MAP})\\cdot (2\\pi)^{D/2}\\frac{|I_1(\\widehat{w})|^{-\\frac12}}{N^{D/2}} =$$\n\n$$=\\exp\\left(\\log{p(y \\vert X,\\widehat{w})} - \\frac{D}2\\log{N} + \\mbox{всякие штуки}\\right)$$\n\nНесмотря на то, что $$p(\\widehat{w}_{MAP})$$ и $$\\vert I_1(\\widehat{w})\\vert^{-\\frac12}$$, сгруппированные нами во \'\'всякие штуки\'\', существенным образом зависят от модели, при больших $$N$$ они вносят в показатель гораздо меньше вклада, чем первые два слагаемых. Таким образом, мы можем себе позволить вместо трудновычисляемых $$p(y \\vert X)$$ использовать для сравнения модели $$ \\color{blue} { \\mbox{байесовский информационный критерий (BIC)}:} $$\n\n$$\\color{blue}{BIC = D\\log{N} - 2\\log{p(y \\vert X,\\widehat{w})}}$$\n\nФреквентисты против байесиан: кто кого?\n\nМы с вами познакомились с двумя парадигмами оценивания:\n\nфреквентистской (frequentist, от слова "frequency", частота) – в которой считается, что данные являются случайным (настоящая случайность!) семплом из некоторого фиксированного распределения, которое мы стараемся оценить по этому семплу, и\n\nбайесовской – в которой данные считаются данностью и в которой мы используем данные для обновления наших априорных представлений о распределении параметров (здесь случайности нет, а есть лишь нехватка знания).\n\nУ обеих есть свои достоинства и недостатки, поборники и гонители. К недостаткам байесовской относится, безусловно, её вычислительная сложность: возможно, вы помните, в пучину вычислений сколь мрачных нас низвергла банальная задача линейной регрессии, и дальше становиться только ещё трудней. Если мы захотим байесовский подход применять к более сложным моделям, например, нейросетям, нам придётся прибегать к упрощениям, огрублениям, приближениям, что, разумеется, ухудшает наши оценки. Но, если простить ему эту вынужденную неточность, он логичнее и честней, и мы продемонстрируем это на следующем примере.\n\nОдним известным свойством оценки максимального правдоподобия является асимптотическая нормальность. Если оценивать наши веса $$w$$ по различным наборам из $$N$$ обучающих примеров, причём считать, что наборы выбираются случайно (не будем уточнять, как именно), то оценка $$\\widehat{w}_{MLE}$$ тоже превращается в случайную величину, которая как-то распределена. Теория утверждает, что при $N\\rightarrow\\infty$\n\n$$ \\quad \\widehat{w}_{MLE}\\sim\\mathcal{N}\\left(w^{\\ast}, I_N({w}^{\\ast})^{-1}\\right)$$\n\nгде $$w^{\\ast}$$ – истинное значение весов, а $$I_N({w}^{\\ast})$$ – матрица информации Фишера, которая определяется как\n\n$$I_N({w}^{\\ast}) = \\mathbb{E}\\left[\\left(\\left.\\frac{\\partial}{\\partial w_i}\\log{p(y \\vert X,w)}\\right|{w^{\\ast}}\\right)\\left(\\left.\\frac{\\partial}{\\partial w_j}\\log{p(y \\vert X,w)}\\right|{w^{\\ast}}\\right)\\right]$$\n\nчто при некоторых не слишком обременительных ограничениях равно\n\n$$I_N({w}^{\\ast}) = -\\mathbb{E}\\left[\\left.\\frac{\\partial^2}{\\partial w_i\\partial w_j}\\log{p(y \\vert X,w)}\\right|_{w^{\\ast}}\\right]$$\n\nПри этом поскольку $$\\log{p(y \\vert X,w)} = \\sum_{i=1}^N\\log{p(y \\vert X,w)}$$, матрица тоже распадается в сумму, и получается, что $$I_N({w}^{\\ast}) = NI_1(w^{\\ast})$$, то есть с ростом $$N$$ ковариация $$(NI_1(w^{\\ast}))^{-1}$$ оценки максимального правдоподобия стремится к нулю.\n\nНа интуитивном уровне можно сказать, что матрица информации Фишера показывает, сколько информации о весах $$w$$ содержится в $$X$$.\n\nПоговорим о проблемах. В реальной ситуации мы не знаем $$w^{\\ast}$$ и тем более не можем посчитать матрицу Фишера, то есть мы с самого начала вынуждены лукавить. Ясно, что вместо $$w^{\\ast}$$ можно взять просто $$\\widehat{w}$$, а вместо $$I_N(w^{\\ast})$$ – матрицу $$I_N(\\widehat{w})$$, которую можно даже при желании определить как\n\n$$-\\left(\\left.\\frac{\\partial^2}{\\partial w_i\\partial w_j}\\log(p(y \\vert X, w))\\right|_{w^{\\ast}}\\right)$$\n\nбезо всякого математического ожидания. Итак, хотя мы можем теперь построить доверительный интервал для оцениваемых параметров, по ходу нами было сделано много упрощений: мы предположили, что асимптотическая оценка распределения уже достигнута, от $$w^{\\ast}$$ перешли к $$\\widehat{w}$$, а для полноты чувств ещё и избавились от математического ожидания. В байесовском подходе мы такого себе не позволяем.'), Document(metadata={'source': '/Users/eugenekillevsky/PycharmProjects/LLM-Interviewer/data/RAG/ml-handbook-master/chapters/linear_models/intro.md'}, page_content='title: Линейные модели author: filipp_sinicin, evgenii_sokolov\n\nМы начнем с самых простых и понятных моделей машинного обучения: линейных. В этой главе мы разберёмся, что это такое, почему они работают и в каких случаях их стоит использовать. Так как это первый класс моделей, с которым вы столкнётесь, мы постараемся подробно проговорить все важные моменты. Заодно объясним, как работает машинное обучение, на сравнительно простых примерах.\n\nПочему модели линейные?\n\nПредставьте, что у вас есть множество объектов $\\mathbb{X}$, а вы хотели бы каждому объекту сопоставить какое-то значение. К примеру, у вас есть набор операций по банковской карте, а вы бы хотели, понять, какие из этих операций сделали мошенники. Если вы разделите все операции на два класса и нулём обозначите законные действия, а единицей мошеннические, то у вас получится простейшая задача классификации. Представьте другую ситуацию: у вас есть данные геологоразведки, по которым вы хотели бы оценить перспективы разных месторождений. В данном случае по набору геологических данных ваша модель будет, к примеру, оценивать потенциальную годовую доходность шахты. Это пример задачи регрессии. Числа, которым мы хотим сопоставить объекты из нашего множества иногда называют таргетами (от английского target).\n\nТаким образом, задачи классификации и регрессии можно сформулировать как поиск отображения из множества объектов $\\mathbb{X}$ в множество возможных таргетов.\n\nМатематически задачи можно описать так: - классификация: $$\\mathbb{X} \\to {0, 1, \\ldots, K}$$, где $0, \\ldots, K$ – номера классов, - регрессия: $\\mathbb{X} \\to \\mathbb{R}$.\n\nОчевидно, что просто сопоставить какие-то объекты каким-то числам — дело довольно бессмысленное. Мы же хотим быстро обнаруживать мошенников или принимать решение, где строить шахту. Значит нам нужен какой-то критерий качества. Мы бы хотели найти такое отображение, которое лучше всего приближает истинное соответствие между объектами и таргетами. Что значит <<лучше всего>> – вопрос сложный. Мы к нему будем много раз возвращаться. Однако, есть более простой вопрос: среди каких отображений мы будем искать самое лучшее? Возможных отображений может быть много, но мы можем упростить себе задачу и договориться, что хотим искать решение только в каком-то заранее заданном параметризированном семействе функций. Вся эта глава будет посвящена самому простому такому семейству — линейным функциям вида\n\n$$ y = w_1 x_1 + \\ldots + w_D x_D + w_0, $$\n\nгде $y$ – целевая переменная (таргет), $(x_1, \\ldots, x_D)$ – вектор, соответствующий объекту выборки (вектор признаков), а $w_1, \\ldots, w_D, w_0$ – параметры модели. Признаки ещё называют фичами (от английского features). Вектор $w = (w_1,\\ldots,w_D)$ часто называют вектором весов, так как на предсказание модели можно смотреть как на взвешенную сумму признаков объекта, а число $w_0$ – свободным коэффициентом, или сдвигом (bias). Более компактно линейную модель можно записать в виде\n\n$$y = \\langle x, w\\rangle + w_0$$\n\nТеперь, когда мы выбрали семейство функций, в котором будем искать решение, задача стала существенно проще. Мы теперь ищем не какое-то абстрактное отображение, а конкретный вектор $(w_0,w_1,\\ldots,w_D)\\in\\mathbb{R}^{D+1}$.\n\nЗамечание. Чтобы применять линейную модель, нужно, чтобы каждый объект уже был представлен вектором численных признаков $x_1,\\ldots,x_D$. Конечно, просто текст или граф в линейную модель не положить, придётся сначала придумать для него численные фичи. Модель называют линейной, если она является линейной по этим численным признакам.\n\nРазберёмся, как будет работать такая модель в случае, если $$D = 1$$. То есть у наших объектов есть ровно один численный признак, по которому они отличаются. Теперь наша линейная модель будет выглядеть совсем просто: $$y = w_1 x_1 + w_0$$. Для задачи регрессии мы теперь пытаемся приблизить значение игрек какой-то линейной функцией от переменной икс. А что будет значить линейность для задачи классификации? Давайте вспомним про пример с поиском мошеннических транзакций по каратам. Допустим, нам известна ровно одна численная переменная — объём транзакции. Для бинарной классификации транзакций на законные и потенциально мошеннические мы будем искать так называемое разделяющее правило: там, где значение функции положительно, мы будем предсказывать один класс, где отрицательно – другой. В нашем примере простейшим правилом будет какое-то пороговое значение объёма транзакций, после которого есть смысл пометить транзакцию как подозрительную.\n\n{: .left}\n\nВ случае более высоких размерностей вместо прямой будет гиперплоскость с аналогичным смыслом.\n\nВопрос на подумать. Если вы посмотрите содержание учебника, то не найдёте в нём ни <<полиномиальных>> моделей, ни каких-нибудь <<логарифмических>>, хотя, казалось бы, зависимости бывают довольно сложными. Почему так?\n\n{% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details="Линейные зависимости не так просты, как кажется. Пусть мы решаем задачу регрессии. Если мы подозреваем, что целевая переменная $$y$$ не выражается через $$x_1, x_2$$ как линейная функция, а зависит ещё от логарифма $$x_1$$ и ещё как-нибудь от того, разные ли знаки у признаков, то мы можем ввести дополнительные слагаемые в нашу линейную зависимость, просто объявим эти слагаемые новыми переменными, и добавив перед ними соответствующие регрессионные коэффициенты\n\n$$y \\approx w_1 x_1 + w_2 x_2 + w_3\\log{x_1} + w_4\\text{sgn}(x_1x_2) + w_0,$$\n\nи в итоге из двумерной нелинейной задачи мы получили четырёхмерную линейную регрессию." %}\n\nВопрос на подумать. А как быть, если одна из фичей является категориальной, то есть принимает значения из (обычно конечного числа) значений, не являющихся числами? Например, это может быть время года, уровень образования, марка машины и так далее. Как правило, с такими значениями невозможно производить арифметические операции или же результаты их применения не имеют смысла.\n\n{% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details=" В линейную модель можно подать только численные признаки, так что категориальную фичу придётся как-то закодировать. Рассмотрим для примера вот такой датасет\n\n{: .left}\n\nЗдесь два категориальных признака – pet_type и color. Первый принимает четыре различных значения, второй – пять.\n\nСамый простой способ – использовать one-hot кодирование (one-hot encoding). Пусть исходный признак мог принимать $M$ значений $c_1,\\ldots, c_M$. Давайте заменим категориальный признак на $M$ признаков, которые принимают значения $0$ и $1$: $i$-й будет отвечать на вопрос <<принимает ли признак значение $c_i$?>>. Иными словами, вместо ячейки со значением $c_i$ у объекта появляется строка нулей и единиц, в которой единица стоит только на $i$-м месте.\n\nВ нашем примере получится вот такая табличка:\n\n{: .left}\n\nМожно было бы на этом остановится, но добавленные признаки обладают одним неприятным свойством: в каждом из них ровно одна единица, так что сумма соответствующих столбцов равна столбцу из единиц. А это уже плохо. Представьте, что у нас есть линейная модель\n\n$$y \\sim w_1x_1 + \\ldots + w_{D-1}x_{d-1} + w_{c_1}x_{c_1} + \\ldots + w_{c_M}x_{c_M} + w_0$$\n\nПреобразуем немного правую часть:\n\n$$y\\sim w_1x_1 + \\ldots + w_{D-1}x_{d-1} + \\underbrace{(w_{c_1} - w_{c_N})}{=:w\'{c_1}}x_{c_1} + \\ldots + \\underbrace{(w_{c_{M-1}} - w_{c_N})}{=:w\'{C_{N-1}}}x_{c_{M-1}} + w_{c_N}\\underbrace{(x_{c_1} + \\ldots + x_{c_M})}_{=1} + w_0 = $$\n\n$$ = w_1x_1 + \\ldots + w_{D-1}x_{d-1} + w\'{c_1}x{c_1} + \\ldots + w\'{c{M-1}}x_{c_{M-1}} + (w_{c_N} + w_0){=w\'{0}}$$\n\nКак видим, от одного из новых признаков можно избавиться, не меняя модель. Больше того, это стоит сделать, потому что наличие <<лишних>> признаков ведёт к переобучению или вовсе ломает модель – подробнее об этом мы поговорим в разделе про регуляризацию. Поэтому при использовании one-hot-encoding обычно выкидывают признак, соответствующий одному из значений. Например, в нашем примере итоговая матрица объекты-признаки будет иметь вид:\n\n{: .left}\n\nКонечно, one-hot кодирование – это самый наивный способ работы с категориальными признаками, и для более сложных фичей или фичей с большим количеством значений оно плохо подходит. С рядом более продвинутых техник вы познакомитесь в разделе про обучение представлений. " %}\n\nПомимо простоты, у линейных моделей есть несколько других достоинств. К примеру, мы можем достаточно легко судить, как влияют на результат те или иные признаки. Скажем, если вес $w_i$ положителен, то с ростом $i$-го признака таргет в случае регрессии будет увеличиваться, а в случае классификации наш выбор будет сдвигаться в пользу одного из классов. Значение весов тоже имеет прозрачную интерпретацию: чем вес $w_i$ больше, тем <<важнее>> $i$-й признак для итогового предсказания. То есть, если вы построили линейную модель, вы неплохо можете объяснить заказчику те или иные её результаты. Это качество моделей называют интерпретируемостью. Оно особенно ценится в индустриальных задачах, цена ошибки в которых высока. Если от работы вашей модели может зависеть жизнь человека, то очень важно понимать, как модель принимает те или иные решения и какими принципами руководствуется. При этом, не все методы машинного обучения хорошо интерпретируемы, к примеру, поведение искусственных нейронных сетей или градиентного бустинга интерпретировать довольно сложно.\n\nВ то же время слепо доверять весам линейных моделей тоже не стоит по целому ряду причин:\n\nЛинейные модели всё-таки довольно узкий класс функций, они неплохо работают для небольших датасетов и простых задач. Однако, если вы решаете линейной моделью более сложную задачу, то вам, скорее всего, придётся выдумывать дополнительные признаки, являющиеся сложными функциями от исходных. Поиск таких дополнительных признаков называется feature engineering, технически он устроен примерно так, как мы описали в вопросе про "полиномиальные модели". Вот только поиском таких искусственных фичей можно сильно увлечься, так что осмысленность интерпретации будет сильно зависеть от здравого смысла эксперта, строившего модель.\n\nЕсли между признаками есть приближённая линейная зависимость, коэффициенты в линейной модели могут совершенно потерять физический смысл (об этой проблеме и о том, как с ней бороться, мы поговорим дальше, когда будем обсуждать регуляризацию).\n\nОсобенно осторожно стоит верить в утверждения вида <<этот коэффициент маленький, значит, этот признак не важен>>. Во-первых, всё зависит от масштаба признака: вдруг коэффициент мал, чтобы скомпенсировать его. Во-вторых, зависимость действительно может быть слабой, но кто знает, в какой ситуации она окажется важна. Такие решения принимаются на основе данных, например, путём проверки статистического критерия (об этом мы коротко упомянем в разделе по вероятностные модели).\n\nКонкретные значения весов могут меняться в зависимости от обучающей выборки, хотя с ростом её размера они будут потихоньку сходиться к весам <<наилучшей>> линейной модели, которую можно было бы построить по всем-всем-всем данным на свете.\n\nОбсудив немного общие свойства линейных моделей, перейдём к тому, как их всё-таки обучать. Сначала разберёмся с регрессией, а затем настанет черёд классификации.\n\nЛинейная регрессия и метод наименьших квадратов (МНК)\n\nМы начнём с использования линейных моделей для решения задачи регрессии. Простейшим примером постановки задачи линейной регрессии является метод наименьших квадратов (Ordinary least squares).\n\nПусть у нас задан датасет $(X, y)$, где $$y=(y_i){i=1}^N \\in \\mathbb{R}^N$$ – вектор значений целевой переменной, а $$X=(x_i){i = 1}^N \\in \\mathbb{R}^{N \\times D}, x_i \\in \\mathbb{R}^D$$ – матрица объекты-признаки, в которой $i$-я строка – это вектор признаков $i$-го объекта выборки. Мы хотим моделировать зависимость $$y_i$$ от $$x_i$$ как линейную функцию со свободным членом. Общий вид такой функции из $$\\mathbb{R}^D$$ в $$\\mathbb{R}$$ выглядит следующим образом:\n\n$$\\color{#348FEA}{f_w(x_i) = \\langle w, x_i \\rangle + w_0}$$\n\nСвободный член $$w_0$$ часто опускают, потому что такого же результата можно добиться, добавив ко всем $$x_i$$ признак, тождественно равный единице; тогда роль свободного члена будет играть соответствующий ему вес:\n\n$$\\begin{pmatrix}x_{i1} & \\ldots & x_{iD} \\end{pmatrix}\\cdot\\begin{pmatrix}w_1\\ \\vdots \\ w_D\\end{pmatrix} + w_0 = \\begin{pmatrix}1 & x_{i1} & \\ldots & x_{iD} \\end{pmatrix}\\cdot\\begin{pmatrix}w_0 \\ w_1\\ \\vdots \\ w_D \\end{pmatrix}$$\n\nПоскольку это сильно упрощает запись, в дальнейшем мы будем считать, что это уже сделано и зависимость имеет вид просто $$f_w(x_i) = \\langle w, x_i \\rangle$$.\n\nСведение к задаче оптимизации\n\nМы хотим, чтобы на нашем датасете (то есть на парах $$(x_i, y_i)$$ из обучающей выборки) функция $$f_w$$ как можно лучше приближала нашу зависимость.\n\n{: .left}\n\nДля того, чтобы чётко сформулировать задачу, нам осталось только одно: на математическом языке выразить желание <<приблизить $$f_w(x)$$ к $$y$$>>. Говоря простым языком, мы должны научиться измерять качество модели и минимизировать её ошибку, как-то меняя обучаемые параметры. В нашем примере обучаемые параметры — это веса $$w$$. Функция, оценивающая то, как часто модель ошибается, традиционно называется функцией потерь, функционалом качества или просто лоссом (loss function). Важно, чтобы её было легко оптимизировать: скажем, гладкая функция потерь – это хорошо, а кусочно постоянная – просто ужасно.\n\nФункции потерь бывают разными. От их выбора зависит то, насколько задачу в дальнейшем легко решать, и то, в каком смысле у нас получится приблизить предсказание модели к целевым значениям. Интуитивно понятно, что для нашей текущей задачи нам нужно взять вектор $$y$$ и вектор предсказаний модели и как-то сравнить, насколько они похожи. Так как эти вектора <<живут>> в одном векторном пространстве, расстояние между ними вполне может быть функцией потерь. Более того, положительная непрерывная функция от этого расстояния тоже подойдёт в качестве функции потерь. При этом способов задать расстояние между векторами тоже довольно много. От всего этого разнообразия глаза разбегаются, но мы обязательно поговорим про это позже. Сейчас давайте в качестве лосса возьмём квадрат $$L^2$$-нормы вектора разницы предсказаний модели и $$y$$. Во-первых, как мы увидим дальше, так задачу будет нетрудно решить, а во-вторых, у этого лосса есть ещё несколько дополнительных свойств:\n\n$$L^2$$-норма разницы – это евклидово расстояние $$|y - f_w(x)|_2$$ между вектором таргетов и вектором ответов модели, то есть мы их приближаем в смысле самого простого и понятного <<расстояния>>.\n\nКак мы увидим в разделе про вероятностные модели, с точки зрения статистики это соответствует гипотезе о том, что наши данные состоят из линейного <<сигнала>> и нормально распределенного <<шума>>.\n\nТак вот, наша функция потерь выглядит так:\n\n$$L(f, X, y) = |y - f(X)|_2^2 = $$\n\n$$= |y - Xw|2^2 = \\sum{i=1}^N(y_i - \\langle x_i, w \\rangle)^2$$\n\nТакой функционал ошибки не очень хорош для сравнения поведения моделей на выборках разного размера. Представьте, что вы хотите понять, насколько качество модели на тестовой выборке из $2500$ объектов хуже, чем на обучающей из $5000$ объектов. Вы измерили $$L^2$$-норму ошибки и получили в одном случае $300$, а в другом $500$. Эти числа не очень интерпретируемы. Гораздо лучше посмотреть на среднеквадратичное отклонение\n\n$$L(f, X, y) = \\frac1N\\sum_{i=1}^N(y_i - \\langle x_i, w \\rangle)^2$$\n\nПо этой метрике на тестовой выборке получаем $$0,12$$, а на обучающей $$0,1$$.\n\nФункция потерь $$\\frac1N\\sum_{i=1}^N(y_i - \\langle x_i, w \\rangle)^2$$ называется Mean Squared Error, MSE или среднеквадратическим отклонением. Разница с $$L^2$$-нормой чисто косметическая, на алгоритм решения задачи она не влияет:\n\n$$\\color{#348FEA}{\\text{MSE}(f, X, y) = \\frac{1}{N}|y - X w|_2^2}$$\n\nВ самом широком смысле, функции работают с объектами множеств: берут какой-то входящий объект из одного множества и выдают на выходе соответствующий ему объект из другого. Если мы имеем дело с отображением, которое на вход принимает функции, а на выходе выдаёт число, то такое отображение называют функционалом. Если вы посмотрите на нашу функцию потерь, то увидите, что это именно функционал. Для каждой конкретной линейной функции, которую задают веса $$w_i$$, мы получаем число, которое оценивает, насколько точно эта функция приближает наши значения $$y$$. Чем меньше это число, тем точнее наше решение, значит для того, чтобы найти лучшую модель, этот функционал нам надо минимизировать по $$w$$:\n\n$$\\color{#348FEA}{|y - Xw|_2^2 \\longrightarrow \\min_w}$$\n\nЭту задачу можно решать разными способами. В этой главе мы сначала решим эту задачу аналитически, а потом приближенно. Сравнение двух этих решений позволит нам проиллюстрировать преимущества того подхода, которому посвящена эта книга. На наш взгляд, это самый простой способ "на пальцах" показать суть машинного обучения.\n\nМНК: точный аналитический метод\n\nТочку минимума можно найти разными способами. Если вам интересно аналитическое решение, вы можете найти его в главе про матричные дифференцирования (раздел <<Примеры вычисления производных сложных функций>>). Здесь же мы воспользуемся геометрическим подходом.\n\nПусть $$x^{(1)},\\ldots,x^{(D)}$$ – столбцы матрицы $$X$$, то есть столбцы признаков. Тогда\n\n$$Xw = w_1x^{(1)}+\\ldots+w_Dx^{(D)},$$\n\nи задачу регрессии можно сформулировать следующим образом: найти линейную комбинацию столбцов $$x^{(1)},\\ldots,x^{(D)}$$, которая наилучшим способом приближает столбец $$y$$ по евклидовой норме – то есть найти проекцию вектора $$y$$ на подпространство, образованное векторами $$x^{(1)},\\ldots,x^{(D)}$$.\n\nРазложим $$y = y_{\\parallel} + y_{\\perp}$$, где $$y_{\\parallel} = Xw$$ – та самая проекция, а $$y_{\\perp}$$ – ортогональная составляющая, то есть $$y_{\\perp} = y - Xw\\perp x^{(1)},\\ldots,x^{(D)}$$. Как это можно выразить в матричном виде? Оказывается, очень просто:\n\n$$X^T(y - Xw) = 0$$\n\nВ самом деле, каждый элемент столбца $$X^T(y - Xw)$$ – это скалярное произведение строки $$X^T$$ (=столбца $$X$$ = одного из $$x^{(i)}$$) на $$y - Xw$$. Из уравнения $$X^T(y - Xw) = 0$$ уже очень легко выразить $$w$$:\n\n$$w = (X^TX)^{-1}X^Ty$$\n\nВопрос на подумать Для вычисления $w_{\\ast}$ нам приходится обращать (квадратную) матрицу $X^TX$, что возможно, только если она невырожденна. Что это значит с точки зрения анализа данных? Почему мы верим, что это выполняется во всех разумных ситуациях?\n\n{% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details="Как известно из линейной алгебры, для вещественной матрицы $X$ ранги матриц $X$ и $X^TX$ совпадают. Матрица $X^TX$ невырожденна тогда и только тогда, когда её ранг равен числу её столбцов, что равно числу столбцов матрицы $X$. Иными словами, формула регрессии поломается, только если столбцы матрицы $X$ линейно зависимы. Столбцы матрицы $X$ – это признаки. А если наши признаки линейно зависимы, то, наверное, что-то идёт не так и мы должны выкинуть часть из них, чтобы остались только линейно независимые.\n\nДругое дело, что зачастую признаки могут быть приближённо линейно зависимы, особенно если их много. Тогда матрица $X^TX$ будет близка к вырожденной, и это, как мы дальше увидим, будет вести к разным, в том числе вычислительным проблемам." %}\n\nВычислительная сложность аналитического решения – $$O(ND^2 + D^3)$$, где, $N$ – длина выборки, $D$ – число признаков у одного объекта. Слагаемое $$ND^2$$ отвечает за сложность перемножения матриц $$X^T$$ и $$X$$, а слагаемое $$D^3$$ – за сложность обращения их произведения. Перемножать матрицы $$(X^TX)^{-1}$$ и $$X^T$$ не стоит. Гораздо лучше сначала умножить $$y$$ на $$X^T$$, а затем полученный вектор на $$(X^TX)^{-1}$$: так будет быстрее и, кроме того, не нужно будет хранить матрицу $$(X^TX)^{-1}X^T$$.\n\nВычисление можно ускорить, используя продвинутые алгоритмы перемножения матриц или итерационные методы поиска обратной матрицы.\n\nПроблемы <<точного>> решения\n\nЗаметим, что для получения ответа нам нужно обратить матрицу $$X^TX$$. Это создает множество проблем: 1. Основная проблема в обращении матрицы — это то, что вычислительно обращать большие матрицы дело сложное, а мы бы хотели работать с датасетами, в которых у нас могут быть миллионы точек, 2. Матрица $$X^TX$$, хотя почти всегда обратима в разумных задачах машинного обучения, зачастую плохо обусловлена. Особенно если признаков много, между ними может появляться приближённая линейная зависимость, которую мы можем упустить на этапе формулировки задачи. В подобных случаях погрешность нахождения $w$ будет зависеть от квадрата числа обусловленности матрицы $X$, что очень плохо. Это делает полученное таким образом решение численно неустойчивым: малые возмущения $$y$$ могут приводить к катастрофическим изменениям $$w$$.\n\n{% include details.html summary="Пара слов про число обусловленности." details=" Пожертвовав математической строгостью, мы можем считать, что число обусловленности матрицы $X$ – это корень из отношения наибольшего и наименьшего из собственных чисел матрицы $X^TX$. Грубо говоря, оно показывает, насколько разного масштаба бывают собственные значения $X^TX$. Если рассмотреть $L^2$-норму ошибки предсказания, как функцию от $w$, то её линии уровня будут эллипсоидами, форма которых определяется квадратичной формой с матрицей $X^TX$ (проверьте это!). Таким образом, число обусловленности говорит о том, насколько вытянутыми являются эти эллипсоиды. " %}\n\n{% include details.html summary="Данные проблемы не являются поводом выбросить решение на помойку. Существует как минимум два способа улучшить его численные свойства, однако если вы не знаете про сингулярное разложение, то лучше вернитесь сюда, когда узнаете." details=" 1. Построим $$QR$$-разложение матрицы $$X$$. Напомним, что это разложение, в котором матрица $$Q$$ ортогональна по столбцам (то есть её столбцы ортогональны и имеют длину 1; в частности, $$Q^TQ=E$$), а $$R$$ квадратная и верхнетреугольная. Подставив его в формулу, получим\n\n  $$w = ((QR)^TQR)^{-1}(QR)^T y = (R^T\\underbrace{Q^TQ}_{=E}R)^{-1}R^TQ^Ty = R^{-1}R^{-T}R^TQ^Ty = R^{-1}Q^Ty$$\n\n  Отметим, что написать $$(R^TR)^{-1} = R^{-1}R^{-T}$$ мы имеем право благодаря тому, что $$R$$ квадратная. Полученная формула намного проще, обращение верхнетреугольной матрицы (=решение системы с верхнетреугольной левой частью) производится быстро и хорошо, погрешность вычисления $w$ будет зависеть просто от числа обусловленности матрицы $X$, а поскольку нахождение $QR$-разложения является достаточно стабильной операцией, мы получаем решение с более хорошими, чем у исходной формулы, численными свойствами.\n\nТакже можно использовать псевдообратную матрицу, построенную с помощью сингулярного разложения, о котором подробно написано в разделе про матричные разложения. А именно, пусть\n\n$$A = U\\underbrace{\\mathrm{diag}(\\sigma_1,\\ldots,\\sigma_r)}_{=\\Sigma}V^T$$\n\n– это усечённое сингулярное разложение, где $r$ – это ранг $A$. В таком случае диагональная матрица посередине является квадратной, $U$ и $V$ ортогональны по столбцам: $U^TU = E$, $V^TV = E$. Тогда\n\n$$w = (V\\Sigma \\underbrace{U^TU}_{=E}\\Sigma V^T)^{-1}V\\Sigma U^Ty$$\n\nЗаметим, что $$V\\Sigma^{-2}V^T\\cdot V\\Sigma^2V^T = E = V\\Sigma^2V^T\\cdot V\\Sigma^{-2}V^T$$, так что $$(V\\Sigma^2 V^T)^{-1} = V\\Sigma^{-2}V^T$$, откуда\n\n$$w = V\\Sigma^{-2}\\underbrace{V^TV}_{=E}V^T\\cdot V\\Sigma U^Ty = V\\Sigma^{-1}Uy$$\n\nХорошие численные свойства сингулярного разложения позволяют утверждать, что и это решение ведёт себя довольно неплохо.\n\nТем не менее, вычисление всё равно остаётся довольно долгим и будет по-прежнему страдать (хоть и не так сильно) в случае плохой обусловленности матрицы $$X$$. " %}\n\nПолностью вылечить проблемы мы не сможем, но никто и не обязывает нас останавливаться на <<точном>> решении (которое всё равно никогда не будет вполне точным). Поэтому ниже мы познакомим вас с совершенно другим методом.\n\nМНК: приближенный численный метод\n\nМинимизируемый функционал является гладким и выпуклым, а это значит, что можно эффективно искать точку его минимума с помощью итеративных градиентных методов. Более подробно вы можете прочитать о них в разделе про методы оптимизации, а здесь мы лишь коротко расскажем об одном самом базовом подходе.\n\nКак известно, градиент функции в точке направлен в сторону её наискорейшего роста, а антиградиент (противоположный градиенту вектор) в сторону наискорейшего убывания. То есть имея какое-то приближение оптимального значения параметра $$w$$, мы можем его улучшить, посчитав градиент функции потерь в точке и немного сдвинув вектор весов в направлении антиградиента:\n\n$$w_j \\mapsto w_j - \\alpha \\frac{d}{d{w_j}} L(f_w, X, y) $$\n\nгде $$\\alpha$$ – это параметр алгоритма (<<темп обучения>>), который контролирует величину шага в направлении антиградиента. Описанный алгоритм называется градиентным спуском.\n\nПосмотрим, как будет выглядеть градиентный спуск для функции потерь $L(f_w, X, y) = \\frac1N\\vert\\vert Xw - y\\vert\\vert^2$. Градиент квадрата евклидовой нормы мы уже считали; соответственно,\n\n$$ \\nabla_wL = \\frac2{N} X^T (Xw - y) $$\n\nСледовательно, стартовав из какого-то начального приближения, мы можем итеративно уменьшать значение функции, пока не сойдёмся (по крайней мере в теории) к минимуму (вообще говоря, локальному, но в данном случае глобальному).\n\nАлгоритм градиентного спуска\n\npython w = random_normal() # можно пробовать и другие виды инициализации repeat S times: # другой вариант: while abs(err) > tolerance f = X.dot(w) # посчитать предсказание err = f - y # посчитать ошибку grad = 2 * X.T.dot(err) / N # посчитать градиент w -= alpha * grad # обновить веса\n\nС теоретическими результатами о скорости и гарантиях сходимости градиентного спуска вы можете познакомиться в главе про методы оптимизации. Мы позволим себе лишь несколько общих замечаний:\n\nПоскольку задача выпуклая, выбор начальной точки влияет на скорость сходимости, но не настолько сильно, чтобы на практике нельзя было стартовать всегда из нуля или из любой другой приятной вам точки;\n\nЧисло обусловленности матрицы $X$ существенно влияет на скорость сходимости градиентного спуска: чем более вытянуты эллипсоиды уровня функции потерь, тем хуже;\n\nТемп обучения $\\alpha$ тоже сильно влияет на поведение градиентного спуска; вообще говоря, он является гиперпараметром алгоритма, и его, возможно, придётся подбирать отдельно. Другими гиперпараметрами являются максимальное число итераций $S$ и/или порог tolerance.\n\n{% include details.html summary="Иллюстрация." details=" Рассмотрим три задачи регрессии, для которых матрица $X$ имеет соответственно маленькое, среднее и большое числа обусловленности. Будем строить для них модели вида $y=w_1x_1 + w_2x_2$. Раскрасим плоскость $(w_1, w_2)$ в соответствии со значениями $|X_{\\text{train}}w - y_{\\text{train}}|^2$. Тёмная область содержит минимум этой функции – оптимальное значение $w_{\\ast}$. Также запустим из из двух точек градиентный спуск с разными значениями темпа обучения $\\alpha$ и посмотрим, что получится:\n\n{: .left} Заголовки графиков (\\"Round\\", \\"Elliptic\\", \\"Stripe-like\\") относятся к форме линий уровня потерь (чем более они вытянуты, тем хуже обусловлена задача и тем хуже может вести себя градиентный спуск).\n\nИтог: при неудачном выборе $\\alpha$ алгоритм не сходится или идёт вразнос, а для плохо обусловленной задачи он сходится абы куда. " %}\n\nВычислительная сложность градиентного спуска – $O(NDS)$, где, как и выше, $N$ – длина выборки, $D$ – число признаков у одного объекта. Сравните с оценкой $$O(N^2D + D^3)$$ для <<наивного>> вычисления аналитического решения.\n\nСложность по памяти – $O(ND)$ на хранение выборки. В памяти мы держим и выборку, и градиент, но в большинстве реалистичных сценариев доминирует выборка.\n\nСтохастический градиентный спуск\n\nНа каждом шаге градиентного спуска нам требуется выполнить потенциально дорогую операцию вычисления градиента по всей выборке (сложность $$O(ND)$$). Возникает идея заменить градиент его оценкой на подвыборке (в английской литературе такую подвыборку обычно именуют batch или mini-batch; в русской разговорной терминологии тоже часто встречается слово батч или мини-батч).\n\nА именно, если функция потерь имеет вид суммы по отдельным парам объект-таргет\n\n$$L(w, X, y) = \\frac1N\\sum_{i=1}^NL(w, x_i, y_i),$$\n\nа градиент, соответственно, записывается в виде\n\n$$\\nabla_wL(w, X, y) = \\frac1N\\sum_{i=1}^N\\nabla_wL(w, x_i, y_i),$$\n\nто предлагается брать оценку\n\n$$\\nabla_wL(w, X, y) \\approx \\frac1B\\sum_{t=1}^B\\nabla_wL(w, x_{i_t}, y_{i_t})$$\n\nдля некоторого подмножества этих пар $(x_{i_t}, y_{i_t})_{t=1}^B$. Обратите внимание на множители $\\frac1N$ и $\\frac1B$ перед суммами. Почему они нужны? Полный градиент $\\nabla_wL(w, X, y)$ можно воспринимать как среднее градиентов по всем объектам, то есть как оценку матожидания $\\mathbb{E}\\nabla_wL(w, x, y)$; тогда, конечно, оценка матожидания по меньшей подвыборке тоже будет иметь вид среднего градиентов по объектам этой подвыборки.\n\nКак делить выборку на батчи? Ясно, что можно было бы случайным образом сэмплировать их из полного датасета, но даже если использовать быстрый алгоритм вроде резервуарного сэмплирования, сложность этой операции не самая оптимальная. Поэтому используют линейный проход по выборке (которую перед этим лучше всё-таки случайным образом перемешать). Давайте введём ещё один параметр нашего алгоритма: размер батча, который мы обозначим $$B$$. Теперь на $$B$$ очередных примерах вычислим градиент и обновим веса модели. При этом вместо количества шагов алгоритма обычно задают количество эпох $$E$$. Это ещё один гиперпараметр. Одна эпоха – это один полный проход нашего сэмплера по выборке. Заметим, что если выборка очень большая, а модель компактная, то даже первый проход бывает можно не заканчивать.\n\nАлгоритм: ```python w = normal(0, 1) repeat E times: for i = B, i <= n, i += B X_batch = X[i-B : i] y_batch = y[i-B : i] f = X_batch.dot(w) # посчитать предсказание err = f - y_batch # посчитать ошибку grad = 2 * X_batch.T.dot(err) / B # посчитать градиент w -= alpha * grad\n\n```\n\nСложность по времени – $$O(NDE)$$. На первый взгляд, она такая же, как и у обычного градиентного спуска, но заметим, что мы сделали в $$N / B$$ раз больше шагов, то есть веса модели претерпели намного больше обновлений.\n\nСложность по памяти можно довести до $$O(BD)$$: ведь теперь всю выборку не надо держать в памяти, а достаточно загружать лишь текущий батч (а остальная выборка может лежать на диске, что удобно, так как в реальности задачи, в которых выборка целиком не влезает в оперативную память, встречаются сплошь и рядом). Заметим, впрочем, что при этом лучше бы $$B$$ взять побольше: ведь чтение с диска – намного более затратная по времени операция, чем чтение из оперативной памяти.\n\nВ целом, разницу между алгоритмами можно представлять как-то так: {: .left}\n\nШаги стохастического градиентного спуска заметно более шумные, но считать их получается значительно быстрее. В итоге они тоже сходятся к оптимальному значению из-за того, что матожидание оценки градиента на батче равно самому градиенту. По крайней мере, сходимость можно получить при хорошо подобранных коэффициентах темпа обучения в случае выпуклого функционала качества. Подробнее мы об этом поговорим в главе про оптимизацию. Для сложных моделей и лоссов стохастический градиентный спуск может сходиться плохо или застревать в локальных минимумах, поэтому придумано множество его улучшений. О некоторых из них также рассказано в главе про оптимизацию.\n\nСуществует определённая терминологическая путаница, иногда стохастическим градиентным спуском называют версию алогоритма, в которой размер батча равен единице (то есть максимально шумная и быстрая версия алгоритма), а версии с бОльшим размером батча называют batch gradient descent. В книгах, которые, возможно, старше вас, такая процедура иногда ещё называется incremental gradient descent. Это не очень принципиально, но вы будьте готовы, если что.\n\nВопрос на подумать. Вообще говоря, если объём данных не слишком велик и позволяет это сделать, объекты лучше случайным образом перемешивать перед тем, как подавать их в алгоритм стохастического градиентного спуска. Как вам кажется, почему?\n\nТакже можно использовать различные стратегии отбора объектов. Например, чаще брать объекты, на которых ошибка больше. Какие ещё стратегии вы могли бы придумать?\n\n{% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details=" Легко представить себе ситуацию, в которой объекты как-нибудь неудачно упорядочены, скажем, по возрастанию таргета. Тогда модель будет попеременно то запоминать, что все таргеты маленькие, то – что все таргеты большие. Это может и не повлиять на качество итоговой модели, но может привести и к довольно печальным последствиям. И вообще, чем более разнообразные батчи модель увидит в процессе обучения, тем лучше.\n\nСтратегий можно придумать много. Например, не брать объекты, на которых ошибка слишком большая (возможно, это выбросы – зачем на них учиться), или вообще не брать те, на которых ошибка достаточно мала (они <<ничему не учат>>). Рекомендуем, впрочем, прибегать к этим эвристикам, только если вы понимаете, зачем они вам нужны и почему есть надежда, что они помогут. " %}\n\nНеградиентные методы\n\nПосле прочтения этой главы у вас может сложиться ощущение, что приближённые способы решения ML задач и градиентные методы – это одно и тоже, но вы будете правы в этом только на 98%. В принципе, существуют и другие способы численно решать эти задачи, но в общем случае они работают гораздо хуже, чем градиентный спуск, и не обладают таким хорошим теоретическим обоснованием. Мы не будем рассказывать про них подробно, но можете на досуге почитать, скажем, про Stepwise regression, Orthogonal matching pursuit или LARS. У LARS, кстати, есть довольно интересное свойство: он может эффективно работать на выборках, в которых число признаков больше числа примеров. С алгоритмом LARS вы можете познакомиться в главе про оптимизацию.\n\nРегуляризация\n\nВсегда ли решение задачи регрессии единственно? Вообще говоря, нет. Так, если в выборке два признака будут линейно зависимы (и следовательно, ранг матрицы будет меньше $$D$$), то гарантировано найдётся такой вектор весов $$\\nu$$ что $$\\langle\\nu, x_i\\rangle = 0\\ \\ \\forall x_i$$. В этом случае, если какой-то $$w$$ является решением оптимизационной задачи, то и $$w + \\alpha \\nu $$ тоже является решением для любого $$\\alpha$$. То есть решение не только не обязано быть уникальным, так ещё может быть сколь угодно большим по модулю. Это создаёт вычислительные трудности. Малые погрешности признаков сильно возрастают при предсказании ответа, а в градиентном спуске накапливается погрешность из-за операций со слишком большими числами.\n\nКонечно, в жизни редко бывает так, что признаки строго линейно зависимы, а вот быть приближённо линейно зависимыми они вполне могут быть. Такая ситуация называется мультиколлинеарностью. В этом случае у нас, всё равно, возникают проблемы, близкие к описанным выше. Дело в том, что $$X\\nu\\sim 0$$ для вектора $\\nu$, состоящего из коэффициентов приближённой линейной зависимости, и, соответственно, $$X^TX\\nu\\approx 0$$, то есть матрица $$X^TX$$ снова будет близка к вырожденной. Как и любая симметричная матрица, она диагонализуется в некотором ортонормированном базисе, и некоторые из собственных значений $$\\lambda_i$$ близки к нулю. Если вектор $$X^Ty$$ в выражении $$(X^TX)^{-1}X^Ty$$ будет близким к соответствующему собственному вектору, то он будет умножаться на $$1 /{\\lambda_i}$$, что опять же приведёт к появлению у $$w$$ очень больших по модулю компонент (при этом $$w$$ ещё и будет вычислен с большой погрешностью из-за деления на маленькое число). И, конечно же, все ошибки и весь шум, которые имелись в матрице $$X$$ при вычислении $$y\\sim Xw$$ будут умножаться на эти большие и неточные числа и возрастать во много-много раз, что приведёт к проблемам, от которых нас не спасёт никакое сингулярное разложение.\n\nВажно ещё отметить, что в случае, когда несколько признаков линейно зависимы, веса $w_i$ при них теряют физический смысл. Может даже оказаться, что вес признака, с ростом которого таргет, казалось бы, должен увеличиваться, станет отрицательным. Это делает модель не только неточной, но и принципиально не интерпретируемой. Вообще, неадекватность знаков или величины весов – хорошее указание на мультиколлинеарность.\n\nДля того, чтобы справиться с этой проблемой, задачу обычно регуляризуют, то есть добавляют к ней дополнительное ограничение на вектор весов. Это ограничение можно, как и исходный лосс, задавать по-разному, но, как правило, ничего сложнее, чем $$L^1$$- и $$L^2$$-нормы, не требуется.\n\nВместо исходной задачи теперь предлагается решить такую:\n\n$$\\color{#348FEA}{\\min_w L(f, X, y) = \\min_w(|X w - y|_2^2 + \\lambda |w|^k_k )}$$\n\n$$\\lambda$$ – это очередной параметр, а $$|w|^k_k $$ – это один из двух вариантов:\n\n$$\\color{#348FEA}{|w|^2_2 = w^2_1 + \\ldots + w^2_D}$$\n\nили\n\n$$\\color{#348FEA}{|w|_1^1 = \\vert w_1 \\vert + \\ldots + \\vert w_D \\vert}$$\n\nДобавка $$\\lambda|w|^k_k$$ называется регуляризационным членом или регуляризатором, а число $\\lambda$ – коэффициентом регуляризации.\n\nКоэффициент $$\\lambda$$ является гиперпараметром модели и достаточно сильно влияет на качество итогового решения. Его подбирают по логарифмической шкале (скажем, от 1e-2 до 1e+2), используя для сравнения моделей с разными значениями $\\lambda$ дополнительную валидационную выборку. При этом качество модели с подобранным коэффициентом регуляризации уже проверяют на тестовой выборке, чтобы исключить переобучение. Более подробно о том, как нужно подбирать гиперпараметры, вы можете почитать в соответствующей главе.\n\nОтдельно надо договориться о том, что вес $w_0$, соответствующий отступу от начала координат (то есть признаку из всех единичек), мы регуляризовать не будем, потому что это не имеет смысла: если даже все значения $$y$$ равномерно велики, это не должно портить качество обучения. Обычно это не отображают в формулах, но если придираться к деталям, то стоило бы написать сумму по всем весам, кроме $$w_0$$:\n\n$$|w|^2_2 = \\sum_{\\color{red}{j=1}}^{D}w_j^2,$$\n\n$$|w|1 = \\sum{\\color{red}{j=1}}^{D} \\vert w_j \\vert$$\n\nВ случае $$L^2$$-регуляризации решение задачи изменяется не очень сильно. Например, продифференцировав новый лосс по $$w$$, легко получить, что <<точное>> решение имеет вид:\n\n$$w = (X^TX + \\lambda I)^{-1}X^Ty$$\n\nОтметим, что за этой формулой стоит и понятная численная интуиция: раз матрица $$X^TX$$ близка к вырожденной и обращать её сродни самоубийству. Мы лучше слегка исказим её добавкой $$\\lambda I$$, которая увеличит все собственные значения на $$\\lambda$$, отодвинув их от нуля. Да, аналитическое решение перестаёт быть <<точным>>, но за счёт снижения численных проблем мы получим более качественное решение, чем при использовании <<точной>> формулы.\n\nВ свою очередь, градиент функции потерь\n\n$$L(f_w, X, y) = |Xw - y|^2 + \\lambda|w|^2$$\n\nпо весам теперь выглядит так:\n\n$$ \\nabla_wL(f_w, X, y) = 2X^T(Xw - y) + 2\\lambda w $$\n\nПодставив этот градиент в алгоритм стохастического градиентного спуска, мы получаем обновлённую версию приближенного алгоритма, отличающуюся от старой только наличием дополнительного слагаемого.\n\nВопрос на подумать. Рассмотрим стохастический градиентный спуск для $L^2$-регуляризованной линейной регрессии с батчами размера $$1$$. Выберите правильный вариант шага SGD:\n\n(а) $$w_i\\mapsto w_i - 2\\alpha(\\langle w, x_j\\rangle - y_j)x_{ji} - \\frac{2\\alpha\\lambda}N w_i,\\quad i=1,\\ldots,D$$;\n\n(б) $$w_i\\mapsto w_i - 2\\alpha(\\langle w, x_j\\rangle - y_j)x_{ji} - 2\\alpha\\lambda w_i,\\quad i=1,\\ldots,D$$;\n\n(в) $$w_i\\mapsto w_i - 2\\alpha(\\langle w, x_j\\rangle - y_j)x_{ji} - 2\\lambda N w_i,\\quad i=1,\\ldots D$$.\n\n{% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details="Не регуляризованная функция потерь имеет вид $$\\mathcal{L}(X, y, w) = \\frac1N\\sum_{i=1}^N\\mathcal{L}(x_i, y_i, w)$$, и её можно воспринимать, как оценку по выборке $$(x_i, y_i)_{i=1}^N$$ идеальной функции потерь\n\n$$\\mathcal{L}(w) = \\mathbb{E}_{x, y}\\mathcal{L}(x, y, w)$$\n\nРегуляризационный член не зависит от выборки и добавляется отдельно:\n\n$$\\mathcal{L}{\\text{reg}}(w) = \\mathbb{E}{x, y}\\mathcal{L}(x, y, w) + \\lambda|w|^2$$\n\nСоответственно, идеальный градиент регуляризованной функции потерь имеет вид\n\n$$\\nabla_w\\mathcal{L}{\\text{reg}}(w) = \\mathbb{E}{x, y}\\nabla_w\\mathcal{L}(x, y, w) + 2\\lambda w,$$\n\nГрадиент по батчу – это тоже оценка градиента идеальной функции потерь, только не на выборке $$(X, y)$$, а на батче $$(x_{t_i}, y_{t_i})_{i=1}^B$$ размера $$B$$. Он будет выглядеть так:\n\n$$\\nabla_w\\mathcal{L}{\\text{reg}}(w) = \\frac1B\\sum{i=1}^B\\nabla_w\\mathcal{L}(x_{t_i}, y_{t_i}, w) + 2\\lambda w.$$\n\nКак видите, коэффициентов, связанных с числом объектов в батче или в исходной выборке, во втором слагаемом нет. Так что верным является второй вариант. Кстати, обратите внимание, что в третьем ещё и нет коэффициента $\\alpha$ перед производной регуляризационного слагаемого, это тоже ошибка.\n\n" %}\n\nВопрос на подумать. Распишите процедуру стохастического градиентного спуска для $L^1$-регуляризованной линейной регрессии. Как вам кажется, почему никого не волнует, что функция потерь, строго говоря, не дифференцируема? {% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details=" Распишем для случая батча размера 1:\n\n$$w_i\\mapsto w_i - \\alpha(\\langle w, x_j\\rangle - y_j)x_{ji} - \\frac{\\lambda}N\\cdot \\text{sign}(w_i),\\quad i=1,\\ldots,D$$\n\nФункция потерь не дифференцируема лишь в одной точке. Так как в машинном обучении чаще всего мы имеем дело с данными вероятностного характера, такая не влечёт каких-то особых проблем. Дело в том, что попадание прямо в ноль очень маловероятно из-за численных погрешностей в данных, так что мы можем просто доопределить производную в одной точке, а если даже пару раз попадём в неё за время обучения это не приведёт к каким-то значительным изменениям результатов. " %}\n\nОтметим, что $L^1$- и $L^2$-регуляризацию можно определять для любой функции потерь $L(w, X, y)$ (и не только в задаче регрессии, а и, например, в задаче классификации тоже). Новая функция потерь будет соответственно равна\n\n$$\\widetilde{L}(w, X, y) = L(w, X, y) + \\lambda|w|_1$$\n\nили\n\n$$\\widetilde{L}(w, X, y) = L(w, X, y) + \\lambda|w|_2^2$$\n\nРазреживание весов в $L^1$-регуляризации\n\n$$L^2$$-регуляризация работает прекрасно и используется в большинстве случаев, но есть одна полезная особенность $$L^1$$-регуляризации: её применение приводит к тому, что у признаков, которые не оказывают большого влияния на ответ, вес в результате оптимизации получается равным $0$. Это позволяет удобным образом удалять признаки, слабо влияющие на таргет. Кроме того, это даёт возможность автоматически избавляться от признаков, которые участвуют в соотношениях приближённой линейной зависимости, соответственно, спасает от проблем, связанных с мультиколлинеарностью, о которых мы писали выше.\n\nНе очень строгим, но довольно интуитивным образом это можно объяснить так: 1. В точке оптимума линии уровня регуляризационного члена касаются линий уровня основного лосса, потому что, во-первых, и те, и другие выпуклые, а во-вторых, если они пересекаются трансверсально, то существует более оптимальная точка:\n\n{: .left}\n\nЛинии уровня $$L^1$$-нормы – это $N$-мерные октаэдры. Точки их касания с линиями уровня лосса, скорее всего, лежат на грани размерности, меньшей $$N-1$$, то есть как раз в области, где часть координат равна нулю:\n\n{: .left}\n\nЗаметим, что данное построение говорит о том, как выглядит оптимальное решение задачи, но ничего не говорит о способе, которым это решение можно найти. На самом деле, найти такой оптимум непросто: у $$L^1$$ меры довольно плохая производная. Однако, способы есть. Можете на досуге прочитать, например, вот эту статью о том, как работало предсказание CTR в google в 2012 году. Там этой теме посвящается довольно много места. Кроме того, рекомендуем посмотреть про проксимальные методы в разделе этой книги про оптимизацию в ML.\n\nЗаметим также, что вообще-то оптимизация любой нормы $$L_x, \\ 0 < x \\leq 1$$, приведёт к появлению разреженных векторов весов, просто если c $$L^1$$ ещё хоть как-то можно работать, то с остальными всё будет ещё сложнее.\n\nДругие лоссы\n\nСтохастический градиентный спуск можно очевидным образом обобщить для решения задачи линейной регрессии с любой другой функцией потерь, не только квадратичной: ведь всё, что нам нужно от неё, – это чтобы у функции потерь был градиент. На практике это делают редко, но тем не менее рассмотрим ещё пару вариантов.\n\nMAE\n\nMean absolute error, абсолютная ошибка, появляется при замене $$L^2$$ нормы в MSE на $$L^1$$:\n\n$$\\color{#348FEA}{MAE(y, \\widehat{y}) = \\frac1N\\sum_{i=1}^N \\vert y_i - \\widehat{y}_i\\vert}$$\n\nМожно заметить, что в MAE по сравнению с MSE существенно меньший вклад в ошибку будут вносить примеры, сильно удалённые от ответов модели. Дело тут в том, что в MAE мы считаем модуль расстояния, а не квадрат, соответственно, вклад больших ошибок в MSE получается существенно больше. Такая функция потерь уместна в случаях, когда вы пытаетесь обучить регрессию на данных с большим количеством выбросов в таргете.\n\nИначе на эту разницу можно посмотреть так: MSE приближает матожидание условного распределения $$y \\mid x$$, а MAE – медиану.\n\nMAPE\n\nMean absolute percentage error, относительная ошибка.\n\n$$MAPE(y, \\widehat{y}) = \\frac1N\\sum_{i=1}^N \\left|\\frac{\\widehat{y}_i-y_i}{y_i}\\right|$$\n\nЧасто используется в задачах прогнозирования (например, погоды, загруженности дорог, кассовых сборов фильмов, цен), когда ответы могут быть различными по порядку величины, и при этом мы бы хотели верно угадать порядок, то есть мы не хотим штрафовать модель за предсказание 2000 вместо 1000 в разы сильней, чем за предсказание 2 вместо 1.\n\nВопрос на подумать. Кроме описанных выше в задаче линейной регрессии можно использовать и другие функции потерь, например, Huber loss:\n\n$$\\mathcal{L}(f, X, y) = \\sum_{i=1}^Nh_{\\delta}(y_i - \\langle w_i, x\\rangle),\\mbox{ где }h_{\\delta}(z) = \\begin{cases} \\frac12z^2,\\ |z|\\leqslant\\delta,\\ \\delta(|z| - \\frac12\\delta),\\ |z| > \\delta \\end{cases}$$\n\nЧисло $$\\delta$$ является гиперпараметром. Сложная формула при $$\\vert z\\vert > \\delta$$ нужна, чтобы функция $$h_{\\delta}(z)$$ была непрерывной. Попробуйте объяснить, зачем может быть нужна такая функция потерь. {% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details="Часто требования формулируют в духе <<функция потерь должна слабее штрафовать то-то и сильней штрафовать вот это>>. Например, $L^2$-регуляризованный лосс штрафует за большие по модулю веса. В данном случае можно заметить, что при небольших значениях ошибки берётся просто MSE, а при больших мы начинаем штрафовать нашу модель менее сурово. Например, это может быть полезно для того, чтобы выбросы не так сильно влияли на результат обучения. " %}\n\nЛинейная классификация\n\nТеперь давайте поговорим про задачу классификации. Для начала будем говорить про бинарную классификацию на два класса. Обобщить эту задачу до задачи классификации на $$K$$ классов не составит большого труда. Пусть теперь наши таргеты $$y$$ кодируют принадлежность к положительному или отрицательному классу, то есть принадлежность множеству $${-1,1}$$ (в этой главе договоримся именно так обозначать классы, хотя в жизни вам будут нередко встречаться и метки $${0,1}$$), а $$x$$ – по-прежнему векторы из $$\\mathbb{R}^D$$. Мы хотим обучить линейную модель так, чтобы плоскость, которую она задаёт, как можно лучше отделяла объекты одного класса от другого.\n\n{: .left}\n\nВ идеальной ситуации найдётся плоскость, которая разделит классы: положительный окажется с одной стороны от неё, а отрицательный с другой. Выборка, для которой это возможно, называется линейно разделимой. Увы, в реальной жизни такое встречается крайне редко.\n\nКак обучить линейную модель классификации, нам ещё предстоит понять, но уже ясно, что итоговое предсказание можно будет вычислить по формуле\n\n$$y = \\text{sign} \\langle w, x_i\\rangle$$\n\n{% include details.html summary="Почему бы не решать, как задачу регрессии?" details=" Мы можем попробовать предсказывать числа $$-1$$ и $$1$$, минимизируя для этого, например, MSE с последующим взятием знака, но ничего хорошего не получится. Во-первых, регрессия почти не штрафует за ошибки на объектах, которые лежат близко к разделяющей плоскости, но не с той стороны. Во вторых, ошибкой будет считаться предсказание, например, $$5$$ вместо $$1$$, хотя нам-то на самом деле не важно, какой у числа модуль, лишь бы знак был правильным. Если визуализировать такое решение, то проблемы тоже вполне заметны:\n\n{: .left}\n\nНам нужна прямая, которая разделяет эти точки, а не проходит через них! " %}\n\nСконструируем теперь функционал ошибки так, чтобы он вышеперечисленными проблемами не обладал. Мы хотим минимизировать число ошибок классификатора, то есть\n\n$$\\sum_i \\mathbb{I}[y_i \\neq sign \\langle w, x_i\\rangle]\\longrightarrow \\min_w$$\n\nДомножим обе части на $$y_i$$ и немного упростим\n\n$$\\sum_i \\mathbb{I}[y_i \\langle w, x_i\\rangle < 0]\\longrightarrow \\min_w$$\n\nВеличина $M = y_i \\langle w, x_i\\rangle$ называется отступом (margin) классификатора. Такая фунция потерь называется misclassification loss. Легко видеть, что\n\nотступ положителен, когда $sign(y_i) = sign(\\langle w, x_i\\rangle)$, то есть класс угадан верно; при этом чем больше отступ, тем больше расстояние от $x_i$ до разделяющей гиперплоскости, то есть «уверенность классификатора»;\n\nотступ отрицателен, когда $sign(y_i) \\ne sign(\\langle w, x_i\\rangle)$, то есть класс угадан неверно; при этом чем больше по модулю отступ, тем более сокрушительно ошибается классификатор.\n\nОт каждого из отступов мы вычисляем функцию\n\n$$F(M) = \\mathbb{I}[M < 0] = \\begin{cases}1,\\ M < 0,\\ 0,\\ M\\geqslant 0\\end{cases}$$\n\nОна кусочно-постоянная, и из-за этого всю сумму невозможно оптимизировать градиентными методами: ведь её производная равна нулю во всех точках, где она существует. Но мы можем мажорировать её какой-нибудь более гладкой функцией, и тогда задачу можно будет решить. Функции можно использовать разные, у них свои достоинства и недостатки, давайте рассмотрим несколько примеров:\n\n{: .left}\n\nВопрос на подумать. Допустим, мы как-то обучили классификатор, и подавляющее большинство отступов оказались отрицательными. Правда ли нас постигла катастрофа? {% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details=" Наверное, мы что-то сделали не так, но ситуацию можно локально выправить, если предсказывать классы, противоположные тем, которые выдаёт наша модель. " %}\n\nВопрос на подумать. Предположим, что у нас есть два классификатора с примерно одинаковыми и достаточно приемлемыми значениями интересующей нас метрики. При этом одна почти всегда выдаёт предсказания с большими по модулю отступами, а вторая – с относительно маленькими. Верно ли, что первая модель лучше, чем вторая? {% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details=" На первый взгляд кажется, что первая модель действительно лучше: ведь она предсказывает <<увереннее>>, но на самом деле всё не так однозначно: во многих случаях модель, которая умеет <<честно признать, что не очень уверена в ответе>>, может быть предпочтительней модели, которая врёт с той же непотопляемой уверенностью, что и говорит правду. В некоторых случаях лучше может оказаться модель, которая, по сути, просто отказывается от классификации на каких-то объектах. " %}\n\nОшибка перцептрона\n\nРеализуем простейшую идею: давайте считать отступы только на неправильно классифицированных объектах и учитывать их не бинарно, а линейно, пропорционально их размеру. Получается такая функция:\n\n$$F(M) = \\max(0, -M)$$\n\nДавайте запишем такой лосс с $$L^2$$-регуляризацией:\n\n$$L(w, x, y) = \\lambda\\vert\\vert w\\vert\\vert^2_2 + \\sum_i \\max(0, -y_i \\langle w, x_i\\rangle)$$\n\nНайдём градиент:\n\n$$ \\nabla_w L(w, x, y) = 2 \\lambda w + \\sum_i \\begin{cases} 0, & y_i \\langle w, x_i \\rangle > 0 \\ - y_i x_i, & y_i \\langle w, x_i \\rangle \\leq 0 \\end{cases} $$\n\nИмея аналитическую формулу для градиента, мы теперь можем так же, как и раньше, применить стохастический градиентный спуск, и задача будет решена.\n\nДанная функция потерь впервые была предложена для перцептрона Розенблатта, первой вычислительной модели нейросети, которая в итоге привела к появлению глубокого обучения.\n\nОна решает задачу линейной классификации, но у неё есть одна особенность: её решение не единственно и сильно зависит от начальных параметров. Например, все изображённые ниже классификаторы имеют одинаковый нулевой лосс:\n\n{: .left}\n\nHinge loss, SVM\n\nДля таких случаев, как на картинке выше, возникает логичное желание не только найти разделяющую прямую, но и постараться провести её на одинаковом удалении от обоих классов, то есть максимизировать минимальный отступ:\n\n{: .left}\n\nЭто можно сделать, слегка поменяв функцию ошибки, а именно положив её равной:\n\n$$F(M) = \\max(0, 1-M)$$\n\n$$L(w, x, y) = \\lambda|w|^2_2 + \\sum_i \\max(0, 1-y_i \\langle w, x_i\\rangle)$$\n\n$$ \\nabla_w L(w, x, y) = 2 \\lambda w + \\sum_i \\begin{cases} 0, & 1 - y_i \\langle w, x_i \\rangle \\leq 0 \\ - y_i x_i, & 1 - y_i \\langle w, x_i \\rangle > 0 \\end{cases} $$\n\nПочему же добавленная единичка приводит к желаемому результату?\n\nИнтуитивно это можно объяснить так: объекты, которые проклассифицированы правильно, но не очень "уверенно" (то есть $$0 \\leq y_i \\langle w, x_i\\rangle < 1$$), продолжают вносить свой вклад в градиент и пытаются "отодвинуть" от себя разделяющую плоскость как можно дальше.\n\nК данному выводу можно прийти и чуть более строго; для этого надо совершенно по-другому взглянуть на выражение, которое мы минимизируем. Поможет вот эта картинка:\n\n{: .left}\n\nЕсли мы максимизируем минимальный отступ, то надо максимизировать $$\\frac{2}{|w|_2}$$, то есть ширину полосы при условии того, что большинство объектов лежат с правильной стороны, что эквивалентно решению нашей исходной задачи:\n\n$$\\lambda|w|^2_2 + \\sum_i \\max(0, 1-y_i \\langle w, x_i\\rangle) \\longrightarrow\\min\\limits_{w}$$\n\nОтметим, что первое слагаемое у нас обратно пропорционально ширине полосы, но мы и максимизацию заменили на минимизацию, так что тут всё в порядке. Второе слагаемое – это штраф за то, что некоторые объекты неправильно расположены относительно разделительной полосы. В конце концов, никто нам не обещал, что классы наши линейно разделимы, и можно провести оптимальную плоскость вообще без ошибок.\n\nИтоговое положение плоскости задаётся всего несколькими обучающими примерами. Это ближайшие к плоскости правильно классифицированные объекты, которые называют опорными векторами или support vectors. Весь метод, соответственно, зовётся методом опорных векторов, или support vector machine или сокращённо SVM. Начиная с шестидесятых годов это был сильнейший из известных методов машинного обучения. В девяностые его сменили методы, основанные на деревьях решений, которые в свою очередь недавно передали "пальму первенства" нейросетям.\n\nПочему же SVM был столь популярен? Из-за небольшого количества параметров и доказуемой оптимальности. Сейчас для нас нормально выбирать специальный алгоритм под задачу и подбирать оптимальные гиперпараметры для этого алгоритма перебором, а когда-то трава была зеленее, а компьютеры медленнее, и такой роскоши у людей не было. Поэтому им нужны были модели, которые гарантированно неплохо работали бы в любой ситуации. Такой моделью и был SVM.\n\nДругие замечательные свойства SVM: существование уникального решения и доказуемо минимальная склонность к переобучению среди всех популярных классов линейных классификаторов. Кроме того, несложная модификация алгоритма, ядровый SVM, позволяет проводить нелинейные разделяющие поверхности.\n\nСтрогий вывод постановки задачи SVM можно прочитать тут или в лекции К.В. Воронцова.\n\nЛогистическая регрессия\n\nВ этом параграфе мы будем обозначать классы нулём и единицей.\n\nЕщё один интересный метод появляется из желания посмотреть на классификацию как на задачу предсказания вероятностей. Хороший пример – предсказание кликов в интернете (например, в рекламе и поиске). Наличие клика в обучающем логе не означает, что, если повторить полностью условия эксперимента, пользователь обязательно кликнет по объекту опять. Скорее у объектов есть какая-то "кликабельность", то есть истинная вероятность клика по данному объекту. Клик на каждом обучающем примере является реализацией этой случайной величины, и мы считаем, что в пределе в каждой точке отношение положительных и отрицательных примеров должно сходиться к этой вероятности.\n\nПроблема состоит в том, что вероятность, по определению, величина от 0 до 1, а простого способа обучить линейную модель так, чтобы это ограничение соблюдалось, нет. Из этой ситуации можно выйти так: научить линейную модель правильно предсказывать какой-то объект, связанный с вероятностью, но с диапазоном значений $$(-\\infty,\\infty)$$, и преобразовать ответы модели в вероятность. Таким объектом является logit или log odds – логарифм отношения вероятности положительного события к отрицательному $$\\log\\left(\\frac{p}{1-p}\\right)$$.\n\nЕсли ответом нашей модели является $$\\log\\left(\\frac{p}{1-p}\\right)$$, то искомую вероятность посчитать не трудно:\n\n$$\\langle w, x_i\\rangle = \\log\\left(\\frac{p}{1-p}\\right)$$\n\n$$e^{\\langle w, x_i\\rangle} = \\frac{p}{1-p}$$\n\n$$p=\\frac{1}{1 + e^{-\\langle w, x_i\\rangle}}$$\n\nФункция в правой части называется сигмоидой и обозначается\n\n$$\\color{#348FEA}{\\sigma(z) = \\frac1{1 + e^{-z}}}$$\n\nТаким образом, $p = \\sigma(\\langle w, x_i\\rangle)$\n\nКак теперь научиться оптимизировать $$w$$ так, чтобы модель как можно лучше предсказывала логиты? Нужно применить метод максимума правдоподобия для распределения Бернулли. Это самое простое распределение, которое возникает, к примеру, при бросках монетки, которая орлом выпадает вероятностью $$p$$. У нас только событием будет не орёл, а то, что пользователь кликнул на объект с такой вероятностью. Если хотите больше подробностей, почитайте про распределение Бернулли в теоретическом минимуме.\n\nПравдоподобие позволяет понять, насколько вероятно получить данные значения таргета $y$ при данных $X$ и весах $w$. Оно имеет вид\n\n$$ p(y\\mid X, w) =\\prod_i p(y_i\\mid x_i, w) $$\n\nи для распределения Бернулли его можно выписать следующим образом:\n\n$$ p(y\\mid X, w) =\\prod_i p_i^{y_i} (1-p_i)^{1-y_i} $$\n\nгде $$p_i$$ – это вероятность, посчитанная из ответов модели. Оптимизировать произведение неудобно, хочется иметь дело с суммой, так что мы перейдём к логарифмическому правдоподобию и подставим формулу для вероятности, которую мы получили выше:\n\n$$ \\ell(w, X, y) = \\sum_i \\big( y_i \\log(p_i) + (1-y_i)\\log(1-p_i) \\big) =$$\n\n$$ =\\sum_i \\big( y_i \\log(\\sigma(\\langle w, x_i \\rangle)) + (1-y_i)\\log(1 - \\sigma(\\langle w, x_i \\rangle)) \\big) $$\n\nЕсли заметить, что\n\n$$ \\sigma(-z) = \\frac{1}{1 + e^z} = \\frac{e^{-z}}{e^{-z} + 1} = 1 - \\sigma(z), $$\n\nто выражение можно переписать проще:\n\n$$ \\ell(w, X, y)=\\sum_i \\big( y_i \\log(\\sigma(\\langle w, x_i \\rangle)) + (1 - y_i) \\log(\\sigma(-\\langle w, x_i \\rangle)) \\big) $$\n\nНас интересует $w$, для которого правдоподобие максимально. Чтобы получить функцию потерь, которую мы будем минимизировать, умножим его на минус один:\n\n$$\\color{#348FEA}{L(w, X, y) = -\\sum_i \\big( y_i \\log(\\sigma(\\langle w, x_i \\rangle)) + (1 - y_i) \\log(\\sigma(-\\langle w, x_i \\rangle)) \\big)}$$\n\nВ отличие от линейной регрессии, для логистической нет явной формулы решения. Деваться некуда, будем использовать градиентный спуск. К счастью, градиент устроен очень просто:\n\n$$ \\nabla_w L(y, X, w) = -\\sum_i x_i \\big( y_i - \\sigma(\\langle w, x_i \\rangle)) \\big) $$ {% include details.html summary="Вывод формулы градиента" details=" Нам окажется полезным ещё одно свойство сигмоиды::\n\n$$ \\frac{d \\log \\sigma(z)}{d z} = \\left( \\log \\left( \\frac{1}{1 + e^{-z}} \\right) \\right)\' = \\frac{e^{-z}}{1 + e^{-z}} = \\sigma(-z) \\ \\frac{d \\log \\sigma(-z)}{d z} = -\\sigma(z) $$\n\nОтсюда:\n\n$$ \\nabla_w \\log \\sigma(\\langle w, x_i \\rangle) = \\sigma(-\\langle w, x_i \\rangle) x_i \\ \\nabla_w \\log \\sigma(-\\langle w, x_i \\rangle) = -\\sigma(\\langle w, x_i \\rangle) x_i $$\n\nи градиент оказывается равным\n\n$$ \\nabla_w L(y, X, w) = -\\sum_i \\big( y_i x_i \\sigma(-\\langle w, x_i \\rangle) - (1 - y_i) x_i \\sigma(\\langle w, x_i \\rangle)) \\big) = \\ = -\\sum_i \\big( y_i x_i (1 - \\sigma(\\langle w, x_i \\rangle)) - (1 - y_i) x_i \\sigma(\\langle w, x_i \\rangle)) \\big) = \\ = -\\sum_i \\big( y_i x_i - y_i x_i \\sigma(\\langle w, x_i \\rangle) - x_i \\sigma(\\langle w, x_i \\rangle) + y_i x_i \\sigma(\\langle w, x_i \\rangle)) \\big) = \\ = -\\sum_i \\big( y_i x_i - x_i \\sigma(\\langle w, x_i \\rangle)) \\big) $$ " %}\n\nПредсказание модели будет вычисляться, как мы договаривались, следующим образом:\n\n$$p=\\sigma(\\langle w, x_i\\rangle)$$\n\nЭто вероятность положительного класса, а как от неё перейти к предсказанию самого класса? В других методах нам достаточно было посчитать знак предсказания, но теперь все наши предсказания положительные и находятся в диапазоне от 0 до 1. Что же делать? Интуитивным и не совсем (и даже совсем не) правильным является ответ <<взять порог 0.5>>. Более корректным будет подобрать этот порог отдельно, для уже построенной регрессии минимизируя нужную вам метрику на отложенной тестовой выборке. Например, сделать так, чтобы доля положительных и отрицательных классов примерно совпадала с реальной.\n\nОтдельно заметим, что метод называется логистической регрессией, а не логистической классификацией именно потому, что предсказываем мы не классы, а вещественные числа – логиты.\n\nВопрос на подумать. Проверьте, что, если метки классов – это $\\pm1$, а не $0$ и $1$, то функцию потерь для логистической регрессии можно записать в более компактном виде:\n\n$$\\mathcal{L}(w, X, y) = \\sum_{i=1}^N\\log(1 + e^{-y_i\\langle w, x_i\\rangle})$$\n\nВопрос на подумать. Правда ли разделяющая поверхность модели логистической регрессии является гиперплоскостью? {% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details=" Разделяющая поверхность отделяет множество точек, которым мы присваиваем класс $$0$$ (или $$-1$$), и множество точек, которым мы присваиваем класс $$1$$. Представляется логичным провести отсечку по какому-либо значению предсказанной вероятности. Однако, выбор этого значения — дело не очевидное. Как мы увидим в главе про калибровку классификаторов, это может быть не настоящая вероятность. Допустим, мы решили провести границу по значению $$\\frac12$$. Тогда разделяющая поверхность как раз задаётся равенством $$p = \\frac12$$, что равносильно $$\\langle w, x\\rangle = 0$$. А это гиперплоскость. " %}\n\nВопрос на подумать. Допустим, что матрица объекты-признаки $X$ имеет полный ранг по столбцам (то есть все её столбцы линейно независимы). Верно ли, что решение задачи восстановления логистической регрессии единственно? {% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details="В этот раз хорошего геометрического доказательства, как было для линейной регрессии, пожалуй, нет; нам придётся честно посчитать вторую производную и доказать, что она является положительно определённой. Сделаем это для случая, когда метки классов – это $\\pm1$. Формулы так получатся немного попроще. Напомним, что в этом случае\n\n$$L(w, X, y) = -\\sum_{i=1}^N\\log(1 + e^{-y_i\\langle w, x_i\\rangle})$$\n\nСледовательно,\n\n$$\\frac{\\partial}{\\partial w_{j}}L(w, X, y) = \\sum_{i=1}^N\\frac{y_ix_{ij}e^{-y_i\\langle w, x_i\\rangle}}{1 + e^{-y_i\\langle w, x_i\\rangle}} = \\sum_{i=1}^Ny_ix_{ij}\\left(1 - \\frac1{1 + e^{-y_i\\langle w, x_i\\rangle}}\\right)$$\n\n$$\\frac{\\partial^2L}{\\partial w_j\\partial w_k}(w, X, y) = \\sum_{i=1}^Ny^2_ix_{ij}x_{ik}\\frac{e^{-y_i\\langle w, x_i\\rangle}}{(1 + e^{-y_i\\langle w, x_i\\rangle})^2} =$$\n\n$$ = \\sum_{i=1}^Ny^2_ix_{ij}x_{ik}\\sigma(y_i\\langle w, x_i\\rangle)(1 - \\sigma(y_i\\langle w, x_i\\rangle))$$\n\nТеперь заметим, что $$y_i^2 = 1$$ и что, если обозначить через $$D$$ диагональную матрицу с элементами $$\\sigma(y_i\\langle w, x_i\\rangle)(1 - \\sigma(y_i\\langle w, x_i\\rangle))$$ на диагонали, матрицу вторых производных можно представить в виде:\n\n$$\\nabla^2L = \\left(\\frac{\\partial^2\\mathcal{L}}{\\partial w_j\\partial w_k}\\right) = X^TDX$$\n\nТак как $$0 < \\sigma(y_i\\langle w, x_i\\rangle) < 1$$, у матрицы $$D$$ на диагонали стоят положительные числа, из которых можно извлечь квадратные корни, представив $$D$$ в виде $$D = D^{1/2}D^{1/2}$$. В свою очередь, матрица $$X$$ имеет полный ранг по столбцам. Стало быть, для любого вектора приращения $$u\\ne 0$$ имеем\n\n$$u^TX^TDXu = u^TX^T(D^{1/2})^TD^{1/2}Xu = \\vert D^{1/2}Xu \\vert^2 > 0$$\n\nТаким образом, функция $$L$$ выпукла вниз как функция от $$w$$, и, соответственно, точка её экстремума непременно будет точкой минимума.\n\nА теперь – почему это не совсем правда. Дело в том, что, говоря «точка её экстремума непременно будет точкой минимума», мы уже подразумеваем существование этой самой точки экстремума. Только вот существует этот экстремум не всегда. Можно показать, что для линейно разделимой выборки функция потерь логистической регрессии не ограничена снизу, и, соответственно, никакого экстремума нет. Доказательство мы оставляем читателю. " %}\n\nВопрос на подумать. На картинке ниже представлены результаты работы на одном и том же датасете трёх моделей логистической регрессии с разными коэффициентами $L^2$-регуляризации:\n\n{: .left}\n\nНаверху показаны предсказанные вероятности положительного класса, внизу – вид разделяющей поверхности.\n\nКак вам кажется, какие картинки соответствуют самому большому коэффициенту регуляризации, а какие – самому маленькому? Почему? {% include details.html summary="Ответ (не открывайте сразу; сначала подумайте сами!)" details=" Коэффициент регуляризации максимален у левой модели. На это нас могут натолкнуть два соображения. Во-первых, разделяющая прямая проведена достаточно странно, то есть можно заподозрить, что регуляризационный член в лосс-функции перевесил функцию потерь исходной задачи. Во-вторых, модель предсказывает довольно близкие к $$\\frac12$$ вероятности – это значит, что значения $$\\langle w, x\\rangle$$ близки к нулю, то есть сам вектор $w$ близок к нулевому. Это также свидетельствует о том, что регуляризационный член играет слишком важную роль при оптимизации.\n\nНаименьший коэффициент регуляризации у правой модели. Её предсказания достаточно <<уверенные>> (цвета на верхнем графике сочные, то есть вероятности быстро приближаются к $$0$$ или $$1$$). Это может свидетельствовать о том, что числа $$\\langle w, x\\rangle$$ достаточно велики по модулю, то есть $$\\vert\\vert w \\vert\\vert$$ достаточно велик. " %}\n\nМногоклассовая классификация\n\nВ этом разделе мы будем следовать изложению из лекций Евгения Соколова.\n\nПусть каждый объект нашей выборки относится к одному из $K$ классов: $\\mathbb{Y} = {1, \\ldots, K}$. Чтобы предсказывать эти классы с помощью линейных моделей, нам придётся свести задачу многоклассовой классификации к набору бинарных, которые мы уже хорошо умеем решать. Мы разберём два самых популярных способа это сделать – one-vs-all и all-vs-all, а проиллюстрировать их нам поможет вот такой игрушечный датасет\n\n{: .left}\n\nОдин против всех (one-versus-all)\n\nОбучим $K$ линейных классификаторов $b_1(x), \\ldots, b_K(x)$, выдающих оценки принадлежности классам $1, \\ldots, K$ соответственно. В случае с линейными моделями эти классификаторы будут иметь вид\n\n$$b_k(x) = \\text{sgn}\\left(\\langle w_k, x \\rangle + w_{0k}\\right)$$\n\nКлассификатор с номером $k$ будем обучать по выборке $\\left(x_i, 2\\mathbb{I}[y_i = k] - 1\\right)_{i = 1}^{N}$; иными словами, мы учим классификатор отличать $k$-й класс от всех остальных.\n\nЛогично, чтобы итоговый классификатор выдавал класс, соответствующий самому уверенному из бинарных алгоритмов. Уверенность можно в каком-то смысле измерить с помощью значений линейных функций:\n\n$$a(x) = \\text{argmax}k \\left(\\langle w_k, x \\rangle + w{0k}\\right) $$\n\nДавайте посмотрим, что даст этот подход применительно к нашему датасету. Обучим три линейных модели, отличающих один класс от остальных:\n\n{: .left}\n\nТеперь сравним значения линейных функций\n\n{: .left}\n\nи для каждой точки выберем тот класс, которому соответствует большее значение, то есть самый <<уверенный>> классификатор:\n\n{: .left}\n\nХочется сказать, что самый маленький класс <<обидели>>.\n\nПроблема данного подхода заключается в том, что каждый из классификаторов $b_1(x), \\dots, b_K(x)$ обучается на своей выборке, и значения линейных функций $\\langle w_k, x \\rangle + w_{0k}$ или, проще говоря, "выходы" классификаторов могут иметь разные масштабы. Из-за этого сравнивать их будет неправильно. Нормировать вектора весов, чтобы они выдавали ответы в одной и той же шкале, не всегда может быть разумным решением: так, в случае с SVM веса перестанут являться решением задачи, поскольку нормировка изменит норму весов.\n\nВсе против всех (all-versus-all)\n\nОбучим $C_K^2$ классификаторов $a_{ij}(x)$, $i, j = 1, \\dots, K$, $i \\neq j$. Например, в случае с линейными моделями эти модели будут иметь вид\n\n$$b_{ij}(x) = \\text{sgn}\\left( \\langle w_{ij}, x \\rangle + w_{0,ij} \\right)$$\n\nКлассификатор $a_{ij}(x)$ будем настраивать по подвыборке $X_{ij} \\subset X$, содержащей только объекты классов $i$ и $j$. Соответственно, классификатор $a_{ij}(x)$ будет выдавать для любого объекта либо класс $i$, либо класс $j$. Проиллюстрируем это для нашей выборки:\n\n{: .left}\n\nЧтобы классифицировать новый объект, подадим его на вход каждого из построенных бинарных классификаторов. Каждый из них проголосует за своей класс; в качестве ответа выберем тот класс, за который наберется больше всего голосов:\n\n$$a(x) = \\text{argmax}k\\sum{i = 1}^{K} \\sum_{j \\neq i}\\mathbb{I}[a_{ij}(x) = k]$$\n\nДля нашего датасета получается следующая картинка:\n\n{: .left}\n\nОбратите внимание на серый треугольник на стыке областей. Это точки, для которых голоса разделились (в данном случае каждый классификатор выдал какой-то свой класс, то есть у каждого класса было по одному голосу). Для этих точек нет явного способа выдать обоснованное предсказание.\n\nМногоклассовая логистическая регрессия\n\nНекоторые методы бинарной классификации можно напрямую обобщить на случай многих классов. Выясним, как это можно проделать с логистической регрессией.\n\nВ логистической регрессии для двух классов мы строили линейную модель\n\n$$b(x) = \\langle w, x \\rangle + w_0$$\n\nа затем переводили её прогноз в вероятность с помощью сигмоидной функции $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$. Допустим, что мы теперь решаем многоклассовую задачу и построили $K$ линейных моделей\n\n$$b_k(x) = \\langle w_k, x \\rangle + w_{0k},$$\n\nкаждая из которых даёт оценку принадлежности объекта одному из классов. Как преобразовать вектор оценок $(b_1(x), \\ldots, b_K(x))$ в вероятности? Для этого можно воспользоваться оператором $\\text{softmax}(z_1, \\ldots, z_K)$, который производит <<нормировку>> вектора:\n\n$$\\text{softmax}(z_1, \\ldots, z_K) = \\left(\\frac{\\exp(z_1)}{\\sum_{k = 1}^{K} \\exp(z_k)}, \\dots, \\frac{\\exp(z_K)}{\\sum_{k = 1}^{K} \\exp(z_k)}\\right).$$\n\nВ этом случае вероятность $k$-го класса будет выражаться как\n\n$$P(y = k \\vert x, w) = \\frac{ \\exp{(\\langle w_k, x \\rangle + w_{0k})}}{ \\sum_{j = 1}^{K} \\exp{(\\langle w_j, x \\rangle + w_{0j})}}.$$\n\nОбучать эти веса предлагается с помощью метода максимального правдоподобия: так же, как и в случае с двухклассовой логистической регрессией:\n\n$$\\sum_{i = 1}^{N} \\log P(y = y_i \\vert x_i, w) \\to \\max_{w_1, \\dots, w_K}$$\n\nМасштабируемость линейных моделей\n\nМы уже обсуждали, что SGD позволяет обучению хорошо масштабироваться по числу объектов, так как мы можем не загружать их целиком в оперативную память. А что делать, если признаков очень много, или мы не знаем заранее, сколько их будет? Такое может быть актуально, например, в следующих ситуациях:\n\nКлассификация текстов: мы можем представить текст в формате <<мешка слов>>, то есть неупорядоченного набора слов, встретившихся в данном тексте, и обучить на нём, например, определение тональности отзыва в интернете. Наличие каждого слова из языка в тексте у нас будет кодироваться отдельной фичой. Тогда размерность каждого элемента обучающей выборки будет порядка нескольких сотен тысяч.\n\nВ задаче предсказания кликов по рекламе можно получить выборку любой размерности, например, так: в качестве фичи закодируем индикатор того, что пользователь X побывал на веб-странице Y. Суммарная размерность тогда будет порядка $$10^9 \\cdot 10^7 = 10^{16}$$. Кроме того, всё время появляются новые пользователи и веб-страницы, так что на этапе применения нас ждут сюрпризы.\n\nЕсть несколько хаков, которые позволяют бороться с такими проблемами: * Несмотря на то, что полная размерность объекта в выборке огромна, количество ненулевых элементов в нём невелико. Значит, можно использовать разреженное кодирование, то есть вместо плотного вектора хранить словарь, в котором будут перечислены индексы и значения ненулевых элементов вектора. * Даже хранить все веса не обязательно! Можно хранить их в хэш-таблице и вычислять индекс по формуле hash(feature) % tablesize. Хэш может вычисляться прямо от слова или id пользователя. Таким образом, несколько фичей будут иметь общий вес, который тем не менее обучится оптимальным образом. Такой подход называется hashing trick. Ясно, что сжатие вектора весов приводит к потерям в качестве, но, как правило, ценой совсем небольших потерь можно сжать этот вектор на много порядков.\n\nПримером открытой библиотеки, в которой реализованы эти возможности, является vowpal wabbit.\n\nParameter server\n\nЕсли при решении задачи ставки столь высоки, что мы не можем разменивать качество на сжатие вектора весов, а признаков всё-таки очень много, то задачу можно решать распределённо, храня все признаки в шардированной хеш-таблице\n\n{: .left}\n\nКружки здесь означают отдельные сервера. Жёлтые загружают данные, а серые хранят части модели. Для обучения жёлтый кружок запрашивает у серого нужные ему для предсказания веса, считает градиент и отправляет его обратно, где тот потом применяется. Схема обладает бесконечной масштабируемостью, но задач, где это оправдано, не очень много.\n\nПодытожим\n\nНа линейную модель можно смотреть как на однослойную нейросеть, поэтому многие методы, которые были изначально разработаны для них, сейчас переиспользуются в задачах глубокого обучения, а базовые подходы к регрессии, классификации и оптимизации вообще выглядят абсолютно так же. Так что несмотря на то, что в целом линейные модели на сегодня применяются редко, то, из чего они состоят и как строятся, знать очень и очень полезно.\n\nНадеемся также, что главным итогом прочтения этой главы для вас будет осознание того, что решение любой ML-задачи состоит из выбора функции потерь, параметризованного класса моделей и способа оптимизации. В следующих главах мы познакомимся с другими моделями и оптимизаторами, но эти базовые принципы не изменятся.')]